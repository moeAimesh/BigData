{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebbfe77",
   "metadata": {},
   "source": [
    "# Image Feature Loader & DB-Saver\n",
    "\n",
    "Dieses Notebook lädt Bilder, extrahiert Features und speichert sie in einer SQLite-Datenbank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8193981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, time, traceback, sqlite3, cProfile, pstats, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import umap\n",
    "\n",
    "# ===== Deine bestehenden Funktionen (NICHT hier implementieren) =====\n",
    "from loader import image_generator\n",
    "from features.hash import calc_hash\n",
    "from features.color_vec import calc_histogram\n",
    "from features.embedding_vec import extract_embeddings\n",
    "from image_load import fast_load\n",
    "\n",
    "# Optional: Header-only Pixel-Check + Resize\n",
    "try:\n",
    "    from PIL import Image\n",
    "    PIL_AVAILABLE = True\n",
    "except Exception:\n",
    "    PIL_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61df86",
   "metadata": {},
   "source": [
    "# FIRST RUN TO EXTRACT SMALL IMAGES AND MARK BIG IMAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1549cfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verarbeitung: 100%|██████████| 371202/371202 [3:58:39<00:00, 60.04Bild/s]   z:\\CODING\\UNI\\BIG_DATA\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "Verarbeitung: 100%|██████████| 371202/371202 [4:29:28<00:00, 22.96Bild/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "PHOTO_FOLDER = r\"D:\\data\\image_data\"\n",
    "EMBEDDING_DIR = r\"C:\\BIG_DATA\\embeddings\"\n",
    "EMBEDDING_PCA_DIR = r\"C:\\BIG_DATA\\embeddings_pca\"\n",
    "\n",
    "# Logging / Outputs\n",
    "LOG_FILE = \"verarbeitung_log.txt\"\n",
    "SKIPPED_LOG = \"skipped_and_errors.txt\"    # fehlerhafte / nicht verarbeitete Bilder\n",
    "DEFERRED_CSV = \"Large_images.csv\"         # deferred wegen Größe/Pixeln\n",
    "\n",
    "# Sharding\n",
    "IMAGES_PER_TABLE = 50000\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "\n",
    "# Preprocess/Embedding\n",
    "TARGET_SIZE = (224, 224)      # (W,H) für PIL\n",
    "EMBED_INPUT_DTYPE = \"float32\" # \"float32\" (0..1) oder \"uint8\"\n",
    "MICRO_BATCH = 32              # Embedding-Microbatch\n",
    "MAX_MP = 8                    # >8 Megapixel => defer (nur Header lesen)\n",
    "MAX_IMAGE_SIZE_BEFORE_REDUCE = 4 * 1024 * 1024  # 4 MB Datei-Threshold\n",
    "\n",
    "# Pixel-Budget (statisch ODER dynamisch aktivieren)\n",
    "INITIAL_BATCH_SIZE = 500\n",
    "PIXEL_BUDGET = INITIAL_BATCH_SIZE * TARGET_SIZE[0] * TARGET_SIZE[1]  # statisch\n",
    "AUTO_PIXEL_BUDGET_FRACTION = None  # z.B. 0.30 für 30% des freien RAMs; None = aus\n",
    "\n",
    "# Dirs\n",
    "Path(EMBEDDING_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(EMBEDDING_PCA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ========== DB ==========\n",
    "conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA journal_mode=OFF;\")\n",
    "cursor.execute(\"PRAGMA synchronous=OFF;\")\n",
    "cursor.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "cursor.execute(\"PRAGMA mmap_size=0;\")  # vermeidet aufgeblähten \"sichtbaren\" Speicher\n",
    "\n",
    "\n",
    "# ========== Logging/Helfer ==========\n",
    "def log_debug(logfile, msg):\n",
    "    logfile.write(msg + \"\\n\")\n",
    "\n",
    "def log_skip_or_error(filename, path, reason):\n",
    "    with open(SKIPPED_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{datetime.now()} | {filename} | {path} | {reason}\\n\")\n",
    "\n",
    "def mark_deferred_large(filename, path, reason=\"LARGE_IMAGE_DEFERRED\"):\n",
    "    with open(DEFERRED_CSV, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{datetime.now()},{filename},{path},{reason}\\n\")\n",
    "\n",
    "def print_resource_usage(stage, logfile):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / (1024 ** 2)\n",
    "    cpu = process.cpu_percent(interval=0.1)\n",
    "    log_debug(logfile, f\"[RESOURCE] {stage} | RAM: {mem:.2f} MB | CPU: {cpu:.2f}%\")\n",
    "\n",
    "def is_large_by_pixels(path, max_mp=MAX_MP):\n",
    "    if not PIL_AVAILABLE:\n",
    "        return False\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            w, h = im.size\n",
    "        return (w * h) > max_mp * 1_000_000\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def should_defer(path):\n",
    "    try:\n",
    "        if os.path.getsize(path) > MAX_IMAGE_SIZE_BEFORE_REDUCE:\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return is_large_by_pixels(path)\n",
    "\n",
    "def compute_pixel_budget_from_ram():\n",
    "    # Dynamisch: z.B. 30% des freien RAMs in Pixel umrechnen\n",
    "    bytes_per_val = 4 if EMBED_INPUT_DTYPE == \"float32\" else 1\n",
    "    channels = 3\n",
    "    avail_bytes = psutil.virtual_memory().available\n",
    "    target_frac = AUTO_PIXEL_BUDGET_FRACTION\n",
    "    target_bytes = int(avail_bytes * target_frac)\n",
    "    return target_bytes // (channels * bytes_per_val)\n",
    "\n",
    "if AUTO_PIXEL_BUDGET_FRACTION:\n",
    "    PIXEL_BUDGET = compute_pixel_budget_from_ram()\n",
    "\n",
    "def create_table_if_not_exists(table_name):\n",
    "    cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            filename TEXT NOT NULL,\n",
    "            path TEXT NOT NULL,\n",
    "            color_hist TEXT,\n",
    "            embedding_path TEXT,\n",
    "            image_hash TEXT,\n",
    "            resolution TEXT,\n",
    "            file_size INTEGER,\n",
    "            pca_embedding TEXT,\n",
    "            umap_x REAL,\n",
    "            umap_y REAL\n",
    "        )\n",
    "    ''')\n",
    "    cursor.execute(f'CREATE INDEX IF NOT EXISTS idx_{table_name}_filename_path ON {table_name}(filename, path);')\n",
    "\n",
    "def save_batch_to_db(entries, logfile, table_name):\n",
    "    if not entries:\n",
    "        return\n",
    "    log_debug(logfile, f\"[DEBUG] Speichere {len(entries)} Einträge in {table_name}...\")\n",
    "    start = time.time()\n",
    "    cursor.executemany(f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        (filename, path, color_hist, embedding_path, image_hash, resolution, file_size,\n",
    "         pca_embedding, umap_x, umap_y)\n",
    "        VALUES ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", entries)\n",
    "    dur = time.time() - start\n",
    "    log_debug(logfile, f\"[DEBUG] DB-Speicherung: {dur:.2f}s\")\n",
    "\n",
    "def preprocess_for_model(img_uint8, target_size=TARGET_SIZE):\n",
    "    # Resize → PIL erwartet (width,height)\n",
    "    if PIL_AVAILABLE:\n",
    "        im = Image.fromarray(img_uint8)\n",
    "        im = im.resize(target_size, Image.BILINEAR)\n",
    "        arr = np.asarray(im)\n",
    "    else:\n",
    "        th, tw = target_size[1], target_size[0]\n",
    "        y_idx = (np.linspace(0, img_uint8.shape[0]-1, th)).astype(np.int32)\n",
    "        x_idx = (np.linspace(0, img_uint8.shape[1]-1, tw)).astype(np.int32)\n",
    "        arr = img_uint8[np.ix_(y_idx, x_idx)]\n",
    "    if EMBED_INPUT_DTYPE == \"float32\":\n",
    "        return (arr.astype(np.float32) / 255.0)\n",
    "    else:\n",
    "        return arr.astype(np.uint8)\n",
    "\n",
    "def to_embed_dtype(arr):\n",
    "    if EMBED_INPUT_DTYPE == \"float32\":\n",
    "        return arr.astype(np.float32) if arr.dtype != np.float32 else arr\n",
    "    else:\n",
    "        if arr.dtype == np.float32:\n",
    "            return (np.clip(arr, 0.0, 1.0) * 255.0).round().astype(np.uint8)\n",
    "        return arr.astype(np.uint8)\n",
    "\n",
    "def extract_embeddings_micro(bimgs, chunk=MICRO_BATCH):\n",
    "    outs = []\n",
    "    for i in range(0, len(bimgs), chunk):\n",
    "        outs.append(extract_embeddings(bimgs[i:i+chunk]))\n",
    "    return np.vstack(outs)\n",
    "\n",
    "def _unique_npy_path(base_path):\n",
    "    base, ext = os.path.splitext(base_path)\n",
    "    if ext.lower() != \".npy\":\n",
    "        base_path = base + \".npy\"\n",
    "        base, ext = os.path.splitext(base_path)\n",
    "    out = base_path\n",
    "    c = 1\n",
    "    while os.path.exists(out):\n",
    "        out = f\"{base}_{c}.npy\"\n",
    "        c += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "# ========== Feature-Vorbereitung ==========\n",
    "def prepare_image_features(filename, path, logfile):\n",
    "    \"\"\"\n",
    "    Defer (Datei/Pixel groß) -> CSV + skip (len==8 Rückgabe).\n",
    "    Sonst: fast_load (uint8), Hash auf Original, Preprocess (klein) für Histogramm+Embedding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if should_defer(path):\n",
    "            log_debug(logfile, f\"[INFO] Deferred (gross/pixelreich): {filename}\")\n",
    "            mark_deferred_large(filename, path, \"LARGE_BY_FILE_OR_PIXELS\")\n",
    "            return (filename, path, None, None, None, None, None, \"LARGE_IMAGE_DEFERRED\")\n",
    "\n",
    "        img = fast_load(path)  # deine Loader-Funktion; Erwartung: uint8 ndarray\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            raise TypeError(\"fast_load() must return a NumPy uint8 array.\")\n",
    "        if img.dtype != np.uint8:\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "        resolution = f\"{img.shape[1]}x{img.shape[0]}\"\n",
    "        file_size  = os.path.getsize(path)\n",
    "\n",
    "        # Hash auf Original (robust gg. dtype)\n",
    "        img_hash = calc_hash(img)\n",
    "\n",
    "        # RAM-schonend: nur verkleinertes Bild im Batch halten\n",
    "        img_small = preprocess_for_model(img)     # (224,224,3)\n",
    "        embed_input = to_embed_dtype(img_small)   # zu erwartetem dtype\n",
    "        \n",
    "        color_hist = calc_histogram(img_small, bins=32)\n",
    "        # Früh stringifizieren (klein halten)\n",
    "        hist_str = \",\".join(str(round(float(v), 6)) for v in np.ravel(color_hist))\n",
    "\n",
    "        del img\n",
    "        return (filename, path, embed_input, hist_str, img_hash, resolution, file_size)\n",
    "\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        reason = f\"{e} | Traceback:\\n{tb}\"\n",
    "        log_debug(logfile, f\"[ERROR] Fehler bei {filename}: {e}\")\n",
    "        log_skip_or_error(filename, path, reason)\n",
    "        return (filename, path, None, None, None, None, None, reason)\n",
    "\n",
    "\n",
    "# ========== Globales Finalisieren über alle Shards ==========\n",
    "def finalize_all_shards_global(\n",
    "    logfile,\n",
    "    sample_size: int = 100_000,\n",
    "    pca_components: int = 100,\n",
    "    chunk: int = 2048,\n",
    "):\n",
    "    \"\"\"\n",
    "    Globales Finalisieren über ALLE Tabellen (einmalig am Ende):\n",
    "    1) IPCA PASS 1: partial_fit auf allen Embeddings (gestreamt)\n",
    "    2) IPCA PASS 2: transform, PCA-Vektoren speichern, pca_embedding aktualisieren\n",
    "       (+ Reservoir-Sampling der PCA-Vektoren für UMAP)\n",
    "    3) UMAP global fit (auf Sample) + PASS 3: transform aller PCA-Vektoren, umap_x/y updaten\n",
    "    \"\"\"\n",
    "    log_debug(logfile, \"[DEBUG] === Globales Finalisieren (IPCA+UMAP) startet ===\")\n",
    "\n",
    "    # 0) Alle Shard-Tabellen finden\n",
    "    cursor.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    tables = [r[0] for r in cursor.fetchall()]\n",
    "    if not tables:\n",
    "        log_debug(logfile, \"[WARN] Keine Shard-Tabellen gefunden – Abbruch.\")\n",
    "        return\n",
    "\n",
    "    # Helper\n",
    "    def load_embeddings(paths):\n",
    "        mats, keep_paths = [], []\n",
    "        for p in paths:\n",
    "            try:\n",
    "                mats.append(np.load(p))\n",
    "                keep_paths.append(p)\n",
    "            except Exception as e:\n",
    "                log_skip_or_error(\"UNKNOWN\", p, f\"PCA-Load-Fehler: {e}\")\n",
    "        if not mats:\n",
    "            return None, []\n",
    "        X = np.stack(mats, axis=0).astype(np.float32)\n",
    "        return X, keep_paths\n",
    "\n",
    "    # PASS 1: IncrementalPCA partial_fit\n",
    "    ipca = None\n",
    "    total_vectors = 0\n",
    "    for tbl in tables:\n",
    "        cursor.execute(f\"SELECT id, embedding_path FROM {tbl}\")\n",
    "        batch_ids, batch_paths = [], []\n",
    "        while True:\n",
    "            row = cursor.fetchone()\n",
    "            if row is None:\n",
    "                if batch_paths:\n",
    "                    X, kept = load_embeddings(batch_paths)\n",
    "                    if X is not None:\n",
    "                        if ipca is None:\n",
    "                            nfeat = X.shape[1]\n",
    "                            ncomp = min(pca_components, nfeat)\n",
    "                            ipca = IncrementalPCA(n_components=ncomp)\n",
    "                            log_debug(logfile, f\"[DEBUG] IPCA init mit n_components={ncomp} (feat={nfeat})\")\n",
    "                        ipca.partial_fit(X)\n",
    "                        total_vectors += X.shape[0]\n",
    "                        del X\n",
    "                    batch_ids.clear(); batch_paths.clear()\n",
    "                    gc.collect()\n",
    "                break\n",
    "\n",
    "            _id, epath = row\n",
    "            if epath:\n",
    "                batch_ids.append(_id)\n",
    "                batch_paths.append(epath)\n",
    "                if len(batch_paths) >= chunk:\n",
    "                    X, kept = load_embeddings(batch_paths)\n",
    "                    if X is not None:\n",
    "                        if ipca is None:\n",
    "                            nfeat = X.shape[1]\n",
    "                            ncomp = min(pca_components, nfeat)\n",
    "                            ipca = IncrementalPCA(n_components=ncomp)\n",
    "                            log_debug(logfile, f\"[DEBUG] IPCA init mit n_components={ncomp} (feat={nfeat})\")\n",
    "                        ipca.partial_fit(X)\n",
    "                        total_vectors += X.shape[0]\n",
    "                        del X\n",
    "                    batch_ids.clear(); batch_paths.clear()\n",
    "                    gc.collect()\n",
    "\n",
    "    if ipca is None or total_vectors == 0:\n",
    "        log_debug(logfile, \"[WARN] Keine Embeddings gefunden – IPCA konnte nicht trainiert werden.\")\n",
    "        return\n",
    "\n",
    "    log_debug(logfile, f\"[DEBUG] PASS 1 fertig – partial_fit auf {total_vectors} Vektoren.\")\n",
    "    print_resource_usage(\"Nach IPCA PASS 1\", logfile)\n",
    "\n",
    "    # PASS 2: transform & PCA speichern + Sample sammeln\n",
    "    rng = random.Random(42)\n",
    "    sample_Xp = []\n",
    "    sample_cnt = 0\n",
    "    total_pca_saved = 0\n",
    "\n",
    "    for tbl in tables:\n",
    "        cursor.execute(f\"SELECT id, embedding_path FROM {tbl}\")\n",
    "        batch_ids, batch_paths = [], []\n",
    "        while True:\n",
    "            row = cursor.fetchone()\n",
    "            if row is None:\n",
    "                if batch_paths:\n",
    "                    X, kept_paths = load_embeddings(batch_paths)\n",
    "                    if X is not None:\n",
    "                        Xp = ipca.transform(X)\n",
    "                        updates = []\n",
    "                        for (_id, _path), vec in zip(batch_ids, Xp):\n",
    "                            pca_path = _unique_npy_path(os.path.join(EMBEDDING_PCA_DIR, f\"{_id}_pca.npy\"))\n",
    "                            try:\n",
    "                                np.save(pca_path, vec.astype(np.float32))\n",
    "                            except Exception as e:\n",
    "                                log_skip_or_error(\"UNKNOWN\", f\"DB_ID={_id}\", f\"PCA-Save-Fehler: {e}\")\n",
    "                                pca_path = \"\"\n",
    "                            updates.append((pca_path, _id))\n",
    "                            # Reservoir-Sampling für UMAP\n",
    "                            sample_cnt += 1\n",
    "                            if len(sample_Xp) < sample_size:\n",
    "                                sample_Xp.append(vec.copy())\n",
    "                            else:\n",
    "                                j = rng.randrange(sample_cnt)\n",
    "                                if j < sample_size:\n",
    "                                    sample_Xp[j] = vec.copy()\n",
    "                        cursor.executemany(f\"UPDATE {tbl} SET pca_embedding=? WHERE id=?\", updates)\n",
    "                        conn.commit()\n",
    "                        total_pca_saved += len(updates)\n",
    "                        del X, Xp, updates\n",
    "                    batch_ids.clear(); batch_paths.clear()\n",
    "                    gc.collect()\n",
    "                break\n",
    "\n",
    "            _id, epath = row\n",
    "            if epath:\n",
    "                batch_ids.append((_id, epath))\n",
    "                batch_paths.append(epath)\n",
    "                if len(batch_paths) >= chunk:\n",
    "                    X, kept_paths = load_embeddings(batch_paths)\n",
    "                    if X is not None:\n",
    "                        Xp = ipca.transform(X)\n",
    "                        updates = []\n",
    "                        for (_id2, _path2), vec in zip(batch_ids, Xp):\n",
    "                            pca_path = _unique_npy_path(os.path.join(EMBEDDING_PCA_DIR, f\"{_id2}_pca.npy\"))\n",
    "                            try:\n",
    "                                np.save(pca_path, vec.astype(np.float32))\n",
    "                            except Exception as e:\n",
    "                                log_skip_or_error(\"UNKNOWN\", f\"DB_ID={_id2}\", f\"PCA-Save-Fehler: {e}\")\n",
    "                                pca_path = \"\"\n",
    "                            updates.append((pca_path, _id2))\n",
    "                            sample_cnt += 1\n",
    "                            if len(sample_Xp) < sample_size:\n",
    "                                sample_Xp.append(vec.copy())\n",
    "                            else:\n",
    "                                j = rng.randrange(sample_cnt)\n",
    "                                if j < sample_size:\n",
    "                                    sample_Xp[j] = vec.copy()\n",
    "                        cursor.executemany(f\"UPDATE {tbl} SET pca_embedding=? WHERE id=?\", updates)\n",
    "                        conn.commit()\n",
    "                        total_pca_saved += len(updates)\n",
    "                        del X, Xp, updates\n",
    "                    batch_ids.clear(); batch_paths.clear()\n",
    "                    gc.collect()\n",
    "\n",
    "    log_debug(logfile, f\"[DEBUG] PASS 2 fertig – {total_pca_saved} PCA-Vektoren gespeichert.\")\n",
    "    print_resource_usage(\"Nach IPCA PASS 2\", logfile)\n",
    "\n",
    "    if not sample_Xp:\n",
    "        log_debug(logfile, \"[WARN] Kein PCA-Sample vorhanden – UMAP wird übersprungen.\")\n",
    "        return\n",
    "\n",
    "    # UMAP fit (auf Sample)\n",
    "    sample_mat = np.vstack(sample_Xp).astype(np.float32)\n",
    "    reducer = umap.UMAP(n_components=2, metric=\"euclidean\", random_state=42, low_memory=True)\n",
    "    log_debug(logfile, f\"[DEBUG] UMAP Fit auf Sample mit {sample_mat.shape[0]} Vektoren …\")\n",
    "    reducer.fit(sample_mat)\n",
    "    del sample_mat, sample_Xp\n",
    "    gc.collect()\n",
    "    print_resource_usage(\"Nach UMAP Fit (Sample)\", logfile)\n",
    "\n",
    "    # PASS 3: UMAP transform aller PCA-Vektoren & DB-Update\n",
    "    total_umap_updated = 0\n",
    "    for tbl in tables:\n",
    "        cursor.execute(f\"SELECT id, pca_embedding FROM {tbl}\")\n",
    "        batch_ids, pca_paths = [], []\n",
    "        while True:\n",
    "            row = cursor.fetchone()\n",
    "            if row is None:\n",
    "                if pca_paths:\n",
    "                    mats, ids_keep = [], []\n",
    "                    for _id, ppath in batch_ids:\n",
    "                        try:\n",
    "                            mats.append(np.load(ppath).astype(np.float32))\n",
    "                            ids_keep.append(_id)\n",
    "                        except Exception as e:\n",
    "                            log_skip_or_error(\"UNKNOWN\", ppath, f\"UMAP-PCA-Load-Fehler: {e}\")\n",
    "                    if mats:\n",
    "                        Xp = np.stack(mats, axis=0)\n",
    "                        coords = reducer.transform(Xp)\n",
    "                        updates = [(float(x), float(y), _id) for _id, (x, y) in zip(ids_keep, coords)]\n",
    "                        cursor.executemany(f\"UPDATE {tbl} SET umap_x=?, umap_y=? WHERE id=?\", updates)\n",
    "                        conn.commit()\n",
    "                        total_umap_updated += len(updates)\n",
    "                        del Xp, coords, updates, mats\n",
    "                    batch_ids.clear(); pca_paths.clear()\n",
    "                    gc.collect()\n",
    "                break\n",
    "\n",
    "            _id, ppath = row\n",
    "            if ppath:\n",
    "                batch_ids.append((_id, ppath))\n",
    "                pca_paths.append(ppath)\n",
    "                if len(pca_paths) >= chunk:\n",
    "                    mats, ids_keep = [], []\n",
    "                    for _id2, ppath2 in batch_ids:\n",
    "                        try:\n",
    "                            mats.append(np.load(ppath2).astype(np.float32))\n",
    "                            ids_keep.append(_id2)\n",
    "                        except Exception as e:\n",
    "                            log_skip_or_error(\"UNKNOWN\", ppath2, f\"UMAP-PCA-Load-Fehler: {e}\")\n",
    "                    if mats:\n",
    "                        Xp = np.stack(mats, axis=0)\n",
    "                        coords = reducer.transform(Xp)\n",
    "                        updates = [(float(x), float(y), _id2) for _id2, (x, y) in zip(ids_keep, coords)]\n",
    "                        cursor.executemany(f\"UPDATE {tbl} SET umap_x=?, umap_y=? WHERE id=?\", updates)\n",
    "                        conn.commit()\n",
    "                        total_umap_updated += len(updates)\n",
    "                        del Xp, coords, updates, mats\n",
    "                    batch_ids.clear(); pca_paths.clear()\n",
    "                    gc.collect()\n",
    "\n",
    "    log_debug(logfile, f\"[DEBUG] PASS 3 fertig – {total_umap_updated} UMAP-Koordinaten aktualisiert.\")\n",
    "    print_resource_usage(\"Nach UMAP PASS 3\", logfile)\n",
    "    log_debug(logfile, \"[DEBUG] === Globales Finalisieren (IPCA+UMAP) abgeschlossen ===\")\n",
    "\n",
    "\n",
    "# ========== Hauptpipeline (Streaming + PIXEL_BUDGET) ==========\n",
    "def main():\n",
    "    logfile = open(LOG_FILE, \"a\", encoding=\"utf-8\")\n",
    "    # Logs frisch beginnen (optional anpassen)\n",
    "    for f in (SKIPPED_LOG, DEFERRED_CSV):\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "\n",
    "    print_resource_usage(\"Start\", logfile)\n",
    "    logfile.write(f\"[{datetime.now()}] [DEBUG] Start Hauptfunktion\\n\")\n",
    "\n",
    "    # Warm-up\n",
    "    dummy_img = np.zeros((10, 10, 3), dtype=np.uint8)\n",
    "    _ = calc_hash(dummy_img)\n",
    "    _ = calc_histogram(dummy_img)\n",
    "    log_debug(logfile, \"[DEBUG] Warm-up abgeschlossen.\")\n",
    "\n",
    "    # Zählen ohne Materialisierung\n",
    "    total_images = sum(1 for _ in image_generator(PHOTO_FOLDER))\n",
    "    log_debug(logfile, f\"[DEBUG] {total_images} Bilder gefunden\")\n",
    "\n",
    "    # Initiale Tabelle\n",
    "    table_id = 1\n",
    "    current_table_name = f\"{TABLE_PREFIX}{table_id}\"\n",
    "    create_table_if_not_exists(current_table_name)\n",
    "\n",
    "    pbar = tqdm(total=total_images, desc=\"Verarbeitung\", unit=\"Bild\")\n",
    "\n",
    "    gen = image_generator(PHOTO_FOLDER)\n",
    "    total_inserted = 0\n",
    "\n",
    "    # Laufender Batch (nur vorverarbeitete Bilder)\n",
    "    batch_meta, batch_imgs = [], []\n",
    "    cur_pixels = 0  # Summe der (H*W) der preprocessed Bilder im RAM\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            filename, path = next(gen)\n",
    "        except StopIteration:\n",
    "            # Ende: evtl. Rest-Flush\n",
    "            if batch_imgs:\n",
    "                kept = flush_by_embeddings_and_insert(batch_meta, batch_imgs, logfile, current_table_name)\n",
    "                total_inserted += kept\n",
    "                batch_meta.clear(); batch_imgs.clear(); cur_pixels = 0\n",
    "                gc.collect()\n",
    "            break\n",
    "\n",
    "        # Existenz-Check (index-gestützt) – nach (filename,path)\n",
    "        cursor.execute(f\"SELECT 1 FROM {current_table_name} WHERE filename=? AND path=? LIMIT 1\", (filename, path))\n",
    "        if cursor.fetchone():\n",
    "            log_debug(logfile, f\"[DEBUG] Übersprungen (bereits in DB): {filename}\")\n",
    "            log_skip_or_error(filename, path, \"Bereits in DB\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        # Features vorbereiten\n",
    "        result = prepare_image_features(filename, path, logfile)\n",
    "        if len(result) == 8:\n",
    "            # deferred oder Fehler\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        fname, p, emb_input, hist_str, img_hash, resolution, size = result\n",
    "        batch_meta.append((fname, p, hist_str, img_hash, resolution, size))\n",
    "        batch_imgs.append(emb_input)\n",
    "        cur_pixels += emb_input.shape[0] * emb_input.shape[1]\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Pixel-Budget erreicht? → Flush\n",
    "        if cur_pixels >= PIXEL_BUDGET:\n",
    "            kept = flush_by_embeddings_and_insert(batch_meta, batch_imgs, logfile, current_table_name)\n",
    "            total_inserted += kept\n",
    "\n",
    "            # Cleanup für nächsten Mini-Batch\n",
    "            batch_meta.clear(); batch_imgs.clear(); cur_pixels = 0\n",
    "            gc.collect()\n",
    "            try:\n",
    "                import torch\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    log_debug(logfile, \"[DEBUG] CUDA Cache geleert.\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            print_resource_usage(\"Nach PixelBudget-Flush\", logfile)\n",
    "\n",
    "            # Sharding: neue Tabelle, falls Grenze erreicht\n",
    "            if total_inserted >= table_id * IMAGES_PER_TABLE:\n",
    "                table_id += 1\n",
    "                current_table_name = f\"{TABLE_PREFIX}{table_id}\"\n",
    "                create_table_if_not_exists(current_table_name)\n",
    "                log_debug(logfile, f\"[DEBUG] Neue Tabelle angelegt: {current_table_name}\")\n",
    "\n",
    "    # === Globales Finalisieren über ALLE Tabellen ===\n",
    "    finalize_all_shards_global(\n",
    "        logfile,\n",
    "        sample_size=100_000,   # ggf. anpassen (RAM/Tempo)\n",
    "        pca_components=100,\n",
    "        chunk=2048\n",
    "    )\n",
    "\n",
    "    pbar.close()\n",
    "    logfile.write(f\"[{datetime.now()}] ✓ Verarbeitung abgeschlossen. Insgesamt eingefügt: {total_inserted}\\n\")\n",
    "    print_resource_usage(\"Ende\", logfile)\n",
    "    logfile.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def flush_by_embeddings_and_insert(batch_meta, batch_imgs, logfile, table_name):\n",
    "    \"\"\"Embeddings (micro-batched) berechnen, speichern, DB-Insert; Rückgabe: #eingefügt.\"\"\"\n",
    "    if not batch_imgs:\n",
    "        return 0\n",
    "\n",
    "    # Embeddings\n",
    "    try:\n",
    "        embs = extract_embeddings_micro(batch_imgs, chunk=MICRO_BATCH)\n",
    "    except Exception as e:\n",
    "        log_debug(logfile, f\"[ERROR] Embedding-Batch fehlgeschlagen: {e}\")\n",
    "        embs = []\n",
    "        for (filename, path, hist_str, img_hash, resolution, size), img_small in zip(batch_meta, batch_imgs):\n",
    "            try:\n",
    "                embs.append(extract_embeddings([img_small])[0])\n",
    "            except Exception as ee:\n",
    "                tb = traceback.format_exc()\n",
    "                log_skip_or_error(filename, path, f\"Embedding-Fehler (single): {ee}\\n{tb}\")\n",
    "        if not embs:\n",
    "            return 0\n",
    "        embs = np.stack(embs, axis=0)\n",
    "\n",
    "    # Speichern + DB-Insert\n",
    "    entries = []\n",
    "    kept = 0\n",
    "    for (filename, path, hist_str, img_hash, resolution, size), emb in zip(batch_meta, embs):\n",
    "        try:\n",
    "            emb_path = _unique_npy_path(os.path.join(EMBEDDING_DIR, f\"{filename}.npy\"))\n",
    "            np.save(emb_path, emb.astype(np.float32))\n",
    "            entries.append((filename, path, hist_str, emb_path, img_hash, resolution, size, \"\", 0.0, 0.0))\n",
    "            kept += 1\n",
    "        except Exception as e_save:\n",
    "            tb = traceback.format_exc()\n",
    "            log_skip_or_error(filename, path, f\"Embedding-Save-Fehler: {e_save}\\n{tb}\")\n",
    "\n",
    "    save_batch_to_db(entries, logfile, table_name)\n",
    "    del embs, entries\n",
    "    return kept\n",
    "\n",
    "\n",
    "# ========== ENTRYPOINT ==========\n",
    "if __name__ == \"__main__\":\n",
    "    with cProfile.Profile() as pr:\n",
    "        main()\n",
    "\n",
    "    with open(\"profiling_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        stats = pstats.Stats(pr, stream=f)\n",
    "        stats.sort_stats(\"cumtime\").print_stats()\n",
    "    stats.dump_stats(\"profiling_results.prof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9ad70",
   "metadata": {},
   "source": [
    "durch die bilder neu iterierne und mehrer hash funktionen berechnen weil nur eins (average hash) nicht informativ genug ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72004d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_1: 100%|██████████| 100000/100000 [49:14<00:00, 33.85img/s]\n",
      "image_features_part_2: 100%|██████████| 51500/51500 [32:02<00:00, 26.79img/s]\n",
      "image_features_part_3: 100%|██████████| 50000/50000 [43:29<00:00, 19.16img/s]\n",
      "image_features_part_4: 100%|██████████| 50000/50000 [47:35<00:00, 17.51img/s]\n",
      "image_features_part_5: 100%|██████████| 50004/50004 [47:41<00:00, 17.48img/s]\n",
      "image_features_part_6: 100%|██████████| 50000/50000 [4:07:22<00:00,  3.37img/s]  \n",
      "image_features_part_7: 100%|██████████| 8250/8250 [40:01<00:00,  3.43img/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sqlite3\n",
    "import traceback\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========= CONFIG =========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "BATCH_SIZE = 2000               # DB-IDs pro Batch\n",
    "RESIZE_DHASH = 8                # (8+1)x8 -> 64 Bit\n",
    "RESIZE_PHASH = 32               # 32x32 -> DCT -> 8x8 -> 64 Bit\n",
    "LOG_FILE = \"hash_backfill_log.txt\"\n",
    "\n",
    "# ========= Logging =========\n",
    "def log(msg: str) -> None:\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "# ========= Utils =========\n",
    "def _bits_to_hex(bits_bool: np.ndarray) -> str:\n",
    "    \"\"\"Bool-Array -> Hex (64 Bit -> 16 Hex-Zeichen).\"\"\"\n",
    "    packed = np.packbits(bits_bool.astype(np.uint8), bitorder=\"big\")\n",
    "    return packed.tobytes().hex()\n",
    "\n",
    "def _imread_rgb(path: str) -> np.ndarray:\n",
    "    \"\"\"Robustes Laden (auch bei Sonderzeichen in Pfaden).\"\"\"\n",
    "    data = np.fromfile(path, dtype=np.uint8)\n",
    "    img = cv.imdecode(data, cv.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(\"cv.imdecode returned None\")\n",
    "    return cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "# ========= Hash-Funktionen =========\n",
    "def dhash_hex(img_rgb: np.ndarray, size: int = RESIZE_DHASH) -> str:\n",
    "    \"\"\"Difference Hash (dHash), 64 Bit -> 16 Hex.\"\"\"\n",
    "    g = cv.cvtColor(img_rgb, cv.COLOR_RGB2GRAY)\n",
    "    g = cv.resize(g, (size + 1, size), interpolation=cv.INTER_AREA)\n",
    "    diff = g[:, 1:] > g[:, :-1]\n",
    "    return _bits_to_hex(diff.reshape(-1))\n",
    "\n",
    "def phash_hex(img_rgb: np.ndarray, full: int = RESIZE_PHASH, keep: int = 8) -> str:\n",
    "    \"\"\"Perceptual Hash (pHash via DCT), 64 Bit -> 16 Hex.\"\"\"\n",
    "    g = cv.cvtColor(img_rgb, cv.COLOR_RGB2GRAY)\n",
    "    g = cv.resize(g, (full, full), interpolation=cv.INTER_AREA).astype(np.float32)\n",
    "    dct = cv.dct(g)\n",
    "    dct_low = dct[:keep, :keep].copy()\n",
    "    dct_low[0, 0] = 0.0\n",
    "    med = np.median(dct_low)\n",
    "    bits = dct_low >= med\n",
    "    return _bits_to_hex(bits.reshape(-1))\n",
    "\n",
    "# ========= DB-Helfer =========\n",
    "def get_shard_tables(cursor):\n",
    "    cursor.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' \"\n",
    "        \"AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cursor.fetchall()]\n",
    "\n",
    "def ensure_columns(conn: sqlite3.Connection, table: str) -> None:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"PRAGMA table_info({table});\")\n",
    "    cols = {row[1] for row in cur.fetchall()}\n",
    "    wanted = {\n",
    "        \"dhash\": \"TEXT\",\n",
    "        \"phash\": \"TEXT\",\n",
    "    }\n",
    "    for col, ctype in wanted.items():\n",
    "        if col not in cols:\n",
    "            cur.execute(f\"ALTER TABLE {table} ADD COLUMN {col} {ctype};\")\n",
    "            log(f\"[INFO] Added column {col} to {table}\")\n",
    "    conn.commit()\n",
    "\n",
    "def count_pending(cur: sqlite3.Cursor, table: str) -> int:\n",
    "    \"\"\"Wie viele Zeilen fehlen (mind. einer der Hashes leer)?\"\"\"\n",
    "    cur.execute(\n",
    "        f\"\"\"\n",
    "        SELECT COUNT(*) FROM {table}\n",
    "        WHERE (dhash IS NULL OR dhash = '')\n",
    "           OR (phash IS NULL OR phash = '')\n",
    "        \"\"\"\n",
    "    )\n",
    "    return int(cur.fetchone()[0])\n",
    "\n",
    "def fetch_ids_and_paths_needing_hashes(cur: sqlite3.Cursor, table: str, limit: int):\n",
    "    cur.execute(\n",
    "        f\"\"\"\n",
    "        SELECT id, path FROM {table}\n",
    "        WHERE (dhash IS NULL OR dhash = '')\n",
    "           OR (phash IS NULL OR phash = '')\n",
    "        LIMIT ?\n",
    "        \"\"\",\n",
    "        (limit,),\n",
    "    )\n",
    "    return cur.fetchall()\n",
    "\n",
    "def update_hashes(conn: sqlite3.Connection, table: str, rows) -> int:\n",
    "    \"\"\"Berechnet Hashes für rows und schreibt sie. Rückgabe: #erfolgreich aktualisiert.\"\"\"\n",
    "    cur = conn.cursor()\n",
    "    updates = []\n",
    "    for _id, path in rows:\n",
    "        try:\n",
    "            img = _imread_rgb(path)\n",
    "            d = dhash_hex(img)\n",
    "            p = phash_hex(img)\n",
    "            updates.append((d, p, _id))\n",
    "        except Exception as e:\n",
    "            log(f\"[ERROR] {table} id={_id} path='{path}': {e}\\n{traceback.format_exc()}\")\n",
    "    if updates:\n",
    "        cur.executemany(\n",
    "            f\"UPDATE {table} SET dhash=?, phash=? WHERE id=?;\",\n",
    "            updates,\n",
    "        )\n",
    "        conn.commit()\n",
    "    return len(updates)\n",
    "\n",
    "# (Optional) Indizes\n",
    "def ensure_indexes(conn: sqlite3.Connection, table: str) -> None:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table}_dhash ON {table}(dhash);\")\n",
    "    cur.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table}_phash ON {table}(phash);\")\n",
    "    conn.commit()\n",
    "\n",
    "# ========= Main =========\n",
    "def backfill_all_tables() -> None:\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)  # autocommit\n",
    "    cur = conn.cursor()\n",
    "    tables = get_shard_tables(cur)\n",
    "    if not tables:\n",
    "        log(\"[WARN] Keine Shard-Tabellen gefunden.\")\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    total_updated = 0\n",
    "    for tbl in tables:\n",
    "        ensure_columns(conn, tbl)\n",
    "        pending = count_pending(cur, tbl)\n",
    "\n",
    "        if pending == 0:\n",
    "            log(f\"[INFO] Nichts zu tun für {tbl}\")\n",
    "            continue\n",
    "\n",
    "        with tqdm(total=pending, desc=f\"{tbl}\", unit=\"img\") as pbar:\n",
    "            # ensure_indexes(conn, tbl)  # optional\n",
    "            while True:\n",
    "                rows = fetch_ids_and_paths_needing_hashes(cur, tbl, BATCH_SIZE)\n",
    "                if not rows:\n",
    "                    break\n",
    "                n_ok = update_hashes(conn, tbl, rows)\n",
    "                total_updated += n_ok\n",
    "                pbar.update(n_ok)  # Fortschritt = erfolgreich befüllte Zeilen\n",
    "                gc.collect()\n",
    "\n",
    "        log(f\"[INFO] Fertig für {tbl}\")\n",
    "\n",
    "    log(f\"[OK] Backfill abgeschlossen. Insgesamt aktualisiert: {total_updated}\")\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    backfill_all_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87eed083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-12 22:10:43.133602] Cleanup startet …\n",
      "Tabellen: ['image_features_part_1', 'image_features_part_2', 'image_features_part_3', 'image_features_part_4', 'image_features_part_5', 'image_features_part_6', 'image_features_part_7']\n",
      "DRY_RUN=False, DELETE_PCA_FILES=True\n",
      "\n",
      "=== image_features_part_1 ===\n",
      "rows_total : 100000\n",
      "pca_set    : 0\n",
      "umap_set   : 0\n",
      "PCA-Dateien gefunden : 0\n",
      "PCA-Dateien gelöscht : 0\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "database is locked",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m    126\u001b[39m     conn.close()\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDRY_RUN: DB wird NICHT geändert.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# NULL setzen ist sauberer als 0.0/'' (klarer „kein Wert“)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUPDATE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtbl\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m SET pca_embedding=NULL WHERE pca_embedding IS NOT NULL AND pca_embedding!=\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     cur.execute(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUPDATE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtbl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m SET umap_x=NULL, umap_y=NULL WHERE (umap_x IS NOT NULL) OR (umap_y IS NOT NULL)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m     conn.commit()\n",
      "\u001b[31mOperationalError\u001b[39m: database is locked"
     ]
    }
   ],
   "source": [
    "#pca und umap spalten leeren \n",
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ======= CONFIG =======\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "\n",
    "# Wo liegen deine PCA-Dateien (zur optionalen Löschung)?\n",
    "EMBEDDING_PCA_DIR = Path(r\"C:\\BIG_DATA\\embeddings_pca\")\n",
    "\n",
    "# Verhalten\n",
    "DRY_RUN = False           # True = nur zählen/anzeigen, nichts ändern/löschen\n",
    "DELETE_PCA_FILES = True  # True = referenzierte PCA-Dateien löschen (nur wenn DRY_RUN=False)\n",
    "\n",
    "CHUNK = 10000  # Streaminggröße für SELECTs\n",
    "\n",
    "# ======================\n",
    "\n",
    "def get_tables(cur, prefix):\n",
    "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "                (f\"{prefix}%\",))\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "def exists_safe(p: str) -> bool:\n",
    "    try:\n",
    "        return os.path.exists(p)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cur = conn.cursor()\n",
    "    cur_read = conn.cursor()  # separater Lese-Cursor (sicherer bei Updates)\n",
    "\n",
    "    tables = get_tables(cur, TABLE_PREFIX)\n",
    "    if not tables:\n",
    "        print(\"Keine Shard-Tabellen gefunden.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[{datetime.now()}] Cleanup startet …\")\n",
    "    print(f\"Tabellen: {tables}\")\n",
    "    print(f\"DRY_RUN={DRY_RUN}, DELETE_PCA_FILES={DELETE_PCA_FILES}\")\n",
    "\n",
    "    total_rows = total_pca_set = total_umap_set = 0\n",
    "    total_files_found = total_files_deleted = 0\n",
    "\n",
    "    for tbl in tables:\n",
    "        print(f\"\\n=== {tbl} ===\")\n",
    "\n",
    "        # Basiscounter\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "        rows_total = cur.fetchone()[0]\n",
    "        total_rows += rows_total\n",
    "\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding!=''\")\n",
    "        pca_set = cur.fetchone()[0]\n",
    "        total_pca_set += pca_set\n",
    "\n",
    "        cur.execute(f\"\"\"SELECT COUNT(*) FROM {tbl}\n",
    "                        WHERE (umap_x IS NOT NULL AND umap_y IS NOT NULL)\n",
    "                          AND (umap_x!=0.0 OR umap_y!=0.0)\"\"\")\n",
    "        umap_set = cur.fetchone()[0]\n",
    "        total_umap_set += umap_set\n",
    "\n",
    "        print(f\"rows_total : {rows_total}\")\n",
    "        print(f\"pca_set    : {pca_set}\")\n",
    "        print(f\"umap_set   : {umap_set}\")\n",
    "\n",
    "        # PCA-Dateien einsammeln (optional löschen)\n",
    "        files_found = files_deleted = 0\n",
    "        if pca_set > 0 and (DELETE_PCA_FILES or DRY_RUN):\n",
    "            cur_read.execute(f\"SELECT pca_embedding FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding!=''\")\n",
    "            while True:\n",
    "                rows = cur_read.fetchmany(CHUNK)\n",
    "                if not rows:\n",
    "                    break\n",
    "                for (ppath,) in rows:\n",
    "                    if not ppath:\n",
    "                        continue\n",
    "                    files_found += 1\n",
    "                    # Aus Sicherheitsgründen nur löschen, wenn Datei im erwarteten Root liegt\n",
    "                    if (not DRY_RUN) and DELETE_PCA_FILES:\n",
    "                        try:\n",
    "                            p = Path(ppath)\n",
    "                            if p.is_file():\n",
    "                                # Safety: innerhalb EMBEDDING_PCA_DIR?\n",
    "                                try:\n",
    "                                    p.relative_to(EMBEDDING_PCA_DIR)\n",
    "                                    os.remove(p)\n",
    "                                    files_deleted += 1\n",
    "                                except ValueError:\n",
    "                                    # liegt außerhalb; zur Sicherheit überspringen\n",
    "                                    print(f\"⚠️  Übersprungen (außerhalb Root): {ppath}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️  Löschen fehlgeschlagen: {ppath} | {e}\")\n",
    "\n",
    "        total_files_found += files_found\n",
    "        total_files_deleted += files_deleted\n",
    "        print(f\"PCA-Dateien gefunden : {files_found}\")\n",
    "        if not DRY_RUN and DELETE_PCA_FILES:\n",
    "            print(f\"PCA-Dateien gelöscht : {files_deleted}\")\n",
    "\n",
    "        # DB-Felder leeren\n",
    "        if DRY_RUN:\n",
    "            print(\"DRY_RUN: DB wird NICHT geändert.\")\n",
    "        else:\n",
    "            # NULL setzen ist sauberer als 0.0/'' (klarer „kein Wert“)\n",
    "            cur.execute(f\"UPDATE {tbl} SET pca_embedding=NULL WHERE pca_embedding IS NOT NULL AND pca_embedding!=''\")\n",
    "            cur.execute(f\"UPDATE {tbl} SET umap_x=NULL, umap_y=NULL WHERE (umap_x IS NOT NULL) OR (umap_y IS NOT NULL)\")\n",
    "            conn.commit()\n",
    "            print(\"DB-Felder geleert: pca_embedding=NULL, umap_x=NULL, umap_y=NULL\")\n",
    "\n",
    "    print(\"\\n===== Zusammenfassung =====\")\n",
    "    print(f\"Tabellen insgesamt      : {len(tables)}\")\n",
    "    print(f\"Zeilen gesamt           : {total_rows}\")\n",
    "    print(f\"PCA gesetzt (DB) gesamt : {total_pca_set}\")\n",
    "    print(f\"UMAP gesetzt (DB) gesamt: {total_umap_set}\")\n",
    "    print(f\"PCA-Dateien gefunden    : {total_files_found}\")\n",
    "    if not DRY_RUN and DELETE_PCA_FILES:\n",
    "        print(f\"PCA-Dateien gelöscht    : {total_files_deleted}\")\n",
    "\n",
    "    print(f\"\\n[{datetime.now()}] Cleanup fertig.\")\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae574a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 10 21:49:22 2025    profiling_results.prof\n",
      "\n",
      "         351036394 function calls (344465332 primitive calls) in 16175.470 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 5633 to 30 due to restriction <30>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    4.103    4.103 16175.799 16175.799 621606085.py:442(main)\n",
      "   321202  103.743    0.000 10380.608    0.032 621606085.py:166(prepare_image_features)\n",
      "   321202    2.113    0.000 5892.825    0.018 621606085.py:70(should_defer)\n",
      "  1017866 3975.147    0.004 3975.309    0.004 {built-in method io.open}\n",
      "   930620 3649.127    0.004 3649.127    0.004 {built-in method nt.stat}\n",
      "   567406    1.545    0.000 3634.637    0.006 <frozen genericpath>:48(getsize)\n",
      "      493    2.629    0.005 3576.141    7.254 621606085.py:548(flush_by_embeddings_and_insert)\n",
      "      493    0.060    0.000 3397.526    6.892 621606085.py:146(extract_embeddings_micro)\n",
      "     7879   13.108    0.002 3397.328    0.431 embedding_vec.py:34(extract_embeddings)\n",
      "590925/7879    2.137    0.000 3174.716    0.403 module.py:1747(_wrapped_call_impl)\n",
      "590925/7879    3.413    0.000 3174.675    0.403 module.py:1755(_call_impl)\n",
      "63032/7879    1.027    0.000 3174.574    0.403 container.py:238(forward)\n",
      "   259152    1.459    0.000 2486.212    0.010 image_load.py:3(fast_load)\n",
      "    63032   64.531    0.001 2471.238    0.039 resnet.py:89(forward)\n",
      "   157580    0.882    0.000 2467.006    0.016 conv.py:553(forward)\n",
      "   157580    0.761    0.000 2465.900    0.016 conv.py:536(_conv_forward)\n",
      "   157580 2465.140    0.016 2465.140    0.016 {built-in method torch.conv2d}\n",
      "   259152 2399.724    0.009 2399.724    0.009 {imread}\n",
      "   318208    2.224    0.000 2321.027    0.007 621606085.py:60(is_large_by_pixels)\n",
      "   318208    4.864    0.000 2308.276    0.007 Image.py:3459(open)\n",
      "        1    0.538    0.538 1857.567 1857.567 621606085.py:209(finalize_all_shards_global)\n",
      "   318184    5.324    0.000 1744.032    0.005 _npyio_impl.py:308(load)\n",
      "      153    1.760    0.012 1694.741   11.077 621606085.py:235(load_embeddings)\n",
      "   246204   47.967    0.000  969.472    0.004 621606085.py:122(preprocess_for_model)\n",
      "   492409    3.648    0.000  802.494    0.002 Image.py:2215(resize)\n",
      "   492408  794.305    0.002  794.305    0.002 {method 'resize' of 'ImagingCore' objects}\n",
      "   246205    2.196    0.000  695.991    0.003 hash.py:6(calc_hash)\n",
      "   492409    7.319    0.000  497.026    0.001 Image.py:3250(fromarray)\n",
      "   492409    1.590    0.000  489.294    0.001 Image.py:3166(frombuffer)\n",
      "   492409    2.839    0.000  486.547    0.001 Image.py:3120(frombytes)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x25d49d3bb50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats\n",
    "\n",
    "stats = pstats.Stats(\"profiling_results.prof\")\n",
    "stats.strip_dirs().sort_stats(\"cumulative\").print_stats(30)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788e8c6",
   "metadata": {},
   "source": [
    "Vergleichen wir zunächst Anzahl bearbietete bilder mit Anzahl bilder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8015dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dateien im Ordner: 359474\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "photo_dir = Path(r\"C:\\BIG_DATA\\embeddings\")\n",
    "n_files = sum(1 for p in photo_dir.iterdir() if p.is_file())\n",
    "print(f\"Dateien im Ordner: {n_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afabb50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_1: 100000 Einträge\n",
      "image_features_part_2: 51500 Einträge\n",
      "image_features_part_3: 50000 Einträge\n",
      "image_features_part_4: 50000 Einträge\n",
      "image_features_part_5: 50004 Einträge\n",
      "image_features_part_6: 50000 Einträge\n",
      "image_features_part_7: 8250 Einträge\n",
      "\n",
      "Gesamtanzahl Einträge: 359754\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "\n",
    "# 1) Verbindung öffnen\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 2) Alle relevanten Tabellen ermitteln\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT name\n",
    "      FROM sqlite_master\n",
    "     WHERE type='table'\n",
    "       AND name LIKE 'image_features_part_%'\n",
    "\"\"\")\n",
    "tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# 3) Zeilen in jeder Tabelle zählen und aufsummieren\n",
    "total_entries = 0\n",
    "for table in tables:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"{table}: {count} Einträge\")\n",
    "    total_entries += count\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\nGesamtanzahl Einträge: {total_entries}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b145a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Bilder: 371202\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PHOTO_FOLDER = r\"D:\\data\\image_data\"\n",
    "IMAGE_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'}  \n",
    "\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(PHOTO_FOLDER):\n",
    "    for fname in files:\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "        if ext in IMAGE_EXTS:\n",
    "            count += 1\n",
    "\n",
    "print(f\"Gefundene Bilder: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86c2b7",
   "metadata": {},
   "source": [
    "überprüfen wie viele bilder erfolgereich PCA embeddings und UMAP koordinaten bekommen haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6f79e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-12 14:58:49.049598] Audit startet …\n",
      "Gefundene Tabellen: 7 → ['image_features_part_1', 'image_features_part_2', 'image_features_part_3', 'image_features_part_4', 'image_features_part_5', 'image_features_part_6', 'image_features_part_7']\n",
      "[FS] Dateien im EMBEDDING_DIR:   359474\n",
      "[FS] Dateien im EMBEDDING_PCA_DIR: 100000\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_1 ===\n",
      "rows_total            : 100000\n",
      "emb_db_set            : 100000\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 100000\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_2 ===\n",
      "rows_total            : 51500\n",
      "emb_db_set            : 51500\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 51500\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_3 ===\n",
      "rows_total            : 50000\n",
      "emb_db_set            : 50000\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 50000\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_4 ===\n",
      "rows_total            : 50000\n",
      "emb_db_set            : 50000\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 50000\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_5 ===\n",
      "rows_total            : 50004\n",
      "emb_db_set            : 50004\n",
      "emb_fs_missing        : 3800\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 50004\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "  Beispiele fehlender EMB-Dateien: ['id=46205|C:\\\\BIG_DATA\\\\embeddings\\\\geometric-modern-house-19541342.jpeg', 'id=46206|C:\\\\BIG_DATA\\\\embeddings\\\\girl-at-garden-party-10032928.jpeg', 'id=46207|C:\\\\BIG_DATA\\\\embeddings\\\\giraffes-on-savannah-in-africa-13255266.jpeg', 'id=46208|C:\\\\BIG_DATA\\\\embeddings\\\\galata-tower-in-istanbul-14643727.jpeg', 'id=46209|C:\\\\BIG_DATA\\\\embeddings\\\\girl-wearing-orange-floral-jumpsuit-2032569.jpeg']\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_6 ===\n",
      "rows_total            : 50000\n",
      "emb_db_set            : 50000\n",
      "emb_fs_missing        : 50000\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 50000\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "  Beispiele fehlender EMB-Dateien: ['id=1|C:\\\\BIG_DATA\\\\embeddings\\\\hand-touching-glass-3944752.jpeg', 'id=2|C:\\\\BIG_DATA\\\\embeddings\\\\group-of-person-watching-concert-outdoors-1540343.jpeg', 'id=3|C:\\\\BIG_DATA\\\\embeddings\\\\group-of-people-playing-on-the-beach-707185.jpeg', 'id=4|C:\\\\BIG_DATA\\\\embeddings\\\\hands-forming-a-heart-shape-4672712.jpeg', 'id=5|C:\\\\BIG_DATA\\\\embeddings\\\\group-of-people-holding-placards-6257831.jpeg']\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_7 ===\n",
      "rows_total            : 8250\n",
      "emb_db_set            : 8250\n",
      "emb_fs_missing        : 8250\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 8250\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "  Beispiele fehlender EMB-Dateien: ['id=1|C:\\\\BIG_DATA\\\\embeddings\\\\woman-in-white-long-sleeve-shirt-standing-beside-woman-in-black-blazer-8761307.jpeg', 'id=2|C:\\\\BIG_DATA\\\\embeddings\\\\woman-in-yellow-shirt-holding-bread-4004469.jpeg', 'id=3|C:\\\\BIG_DATA\\\\embeddings\\\\woman-in-yellow-blazer-holding-blue-and-yellow-shoes-8386668.jpeg', 'id=4|C:\\\\BIG_DATA\\\\embeddings\\\\woman-in-yellow-dress-lying-on-bed-3992193.jpeg', 'id=5|C:\\\\BIG_DATA\\\\embeddings\\\\woman-in-white-protective-suit-wearing-white-face-mask-looking-through-the-microscope-8442097.jpeg']\n",
      "\n",
      "===== GESAMT =====\n",
      "rows_total            : 359754\n",
      "emb_db_set            : 359754\n",
      "emb_fs_missing        : 62050\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 359754\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "CSV-Report geschrieben nach: Z:\\CODING\\UNI\\BIG_DATA\\src\\report_embeddings_pca_umap.csv\n",
      "[2025-08-12 14:59:39.845909] Audit fertig.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "EMBEDDING_DIR = Path(r\"C:\\BIG_DATA\\embeddings\")\n",
    "EMBEDDING_PCA_DIR = Path(r\"C:\\BIG_DATA\\embeddings_pca\")\n",
    "\n",
    "WRITE_CSV = True\n",
    "CSV_PATH = Path(\"report_embeddings_pca_umap.csv\")\n",
    "\n",
    "CHUNK = 10000   # DB-Streaminggröße\n",
    "\n",
    "# ========== HELPERS ==========\n",
    "def count_files_in_dir(d: Path) -> int:\n",
    "    try:\n",
    "        return sum(1 for p in d.iterdir() if p.is_file())\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def exists_safe(p: str) -> bool:\n",
    "    try:\n",
    "        return os.path.exists(p)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_tables(cur, prefix):\n",
    "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "                (f\"{prefix}%\",))\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "# ========== MAIN AUDIT ==========\n",
    "def main():\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    tables = get_tables(cur, TABLE_PREFIX)\n",
    "    if not tables:\n",
    "        print(\"Keine Shard-Tabellen gefunden.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Gefundene Tabellen: {len(tables)} → {tables}\")\n",
    "\n",
    "    # FS Grob-Check\n",
    "    emb_fs_count = count_files_in_dir(EMBEDDING_DIR)\n",
    "    pca_fs_count = count_files_in_dir(EMBEDDING_PCA_DIR)\n",
    "    print(f\"[FS] Dateien im EMBEDDING_DIR:   {emb_fs_count}\")\n",
    "    print(f\"[FS] Dateien im EMBEDDING_PCA_DIR: {pca_fs_count}\")\n",
    "\n",
    "    # CSV vorbereiten\n",
    "    if WRITE_CSV:\n",
    "        import csv\n",
    "        csvfile = open(CSV_PATH, \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "        writer = csv.writer(csvfile, delimiter=\";\")\n",
    "        writer.writerow([\n",
    "            \"table\", \"rows_total\",\n",
    "            \"emb_db_set\", \"emb_fs_missing\",\n",
    "            \"pca_db_set\", \"pca_fs_missing\",\n",
    "            \"pca_db_not_set\",\n",
    "            \"umap_set\", \"umap_missing_for_pca\",\n",
    "            \"examples_missing_emb_paths\", \"examples_missing_pca_paths\",\n",
    "            \"examples_pca_set_umap_missing_ids\"\n",
    "        ])\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    # Gesamtsummen\n",
    "    G_rows = G_emb_set = G_pca_set = G_umap_set = 0\n",
    "    G_emb_fs_missing = G_pca_fs_missing = 0\n",
    "    G_pca_not_set = G_umap_missing_for_pca = 0\n",
    "\n",
    "    for tbl in tables:\n",
    "        print(f\"\\n=== Prüfe Tabelle: {tbl} ===\")\n",
    "\n",
    "        # Basiscounter aus DB\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "        rows_total = cur.fetchone()[0]\n",
    "\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\")\n",
    "        emb_db_set = cur.fetchone()[0]\n",
    "\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\")\n",
    "        pca_db_set = cur.fetchone()[0]\n",
    "\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT COUNT(*) FROM {tbl}\n",
    "            WHERE (umap_x IS NOT NULL AND umap_y IS NOT NULL) AND (umap_x != 0.0 OR umap_y != 0.0)\n",
    "        \"\"\")\n",
    "        umap_set = cur.fetchone()[0]\n",
    "\n",
    "        # Streaming-Check: fehlende Embedding-Dateien\n",
    "        emb_fs_missing = 0\n",
    "        miss_emb_examples = []\n",
    "\n",
    "        cur.execute(f\"SELECT id, embedding_path FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(CHUNK)\n",
    "            if not rows: break\n",
    "            for _id, epath in rows:\n",
    "                if not exists_safe(epath):\n",
    "                    emb_fs_missing += 1\n",
    "                    if len(miss_emb_examples) < 5:\n",
    "                        miss_emb_examples.append(f\"id={_id}|{epath}\")\n",
    "\n",
    "        # Streaming-Check: fehlende PCA-Dateien\n",
    "        pca_fs_missing = 0\n",
    "        miss_pca_examples = []\n",
    "\n",
    "        cur.execute(f\"SELECT id, pca_embedding FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(CHUNK)\n",
    "            if not rows: break\n",
    "            for _id, ppath in rows:\n",
    "                if not exists_safe(ppath):\n",
    "                    pca_fs_missing += 1\n",
    "                    if len(miss_pca_examples) < 5:\n",
    "                        miss_pca_examples.append(f\"id={_id}|{ppath}\")\n",
    "\n",
    "        # PCA gesetzt? nein:\n",
    "        pca_db_not_set = rows_total - pca_db_set\n",
    "\n",
    "        # Fälle: PCA gesetzt, aber UMAP fehlt/placeholder\n",
    "        umap_missing_for_pca = 0\n",
    "        pca_umap_missing_examples = []\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT id, umap_x, umap_y\n",
    "            FROM {tbl}\n",
    "            WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\n",
    "              AND (umap_x IS NULL OR umap_y IS NULL OR (umap_x = 0.0 AND umap_y = 0.0))\n",
    "        \"\"\")\n",
    "        while True:\n",
    "            rows = cur.fetchmany(CHUNK)\n",
    "            if not rows: break\n",
    "            for _id, x, y in rows:\n",
    "                umap_missing_for_pca += 1\n",
    "                if len(pca_umap_missing_examples) < 5:\n",
    "                    pca_umap_missing_examples.append(f\"id={_id}|umap=({x},{y})\")\n",
    "\n",
    "        # Ausgabe pro Tabelle\n",
    "        print(f\"rows_total            : {rows_total}\")\n",
    "        print(f\"emb_db_set            : {emb_db_set}\")\n",
    "        print(f\"emb_fs_missing        : {emb_fs_missing}\")\n",
    "        print(f\"pca_db_set            : {pca_db_set}\")\n",
    "        print(f\"pca_db_not_set        : {pca_db_not_set}\")\n",
    "        print(f\"pca_fs_missing        : {pca_fs_missing}\")\n",
    "        print(f\"umap_set              : {umap_set}\")\n",
    "        print(f\"umap_missing_for_pca  : {umap_missing_for_pca}\")\n",
    "\n",
    "        if miss_emb_examples:\n",
    "            print(f\"  Beispiele fehlender EMB-Dateien: {miss_emb_examples}\")\n",
    "        if miss_pca_examples:\n",
    "            print(f\"  Beispiele fehlender PCA-Dateien: {miss_pca_examples}\")\n",
    "        if pca_umap_missing_examples:\n",
    "            print(f\"  Beispiele: PCA gesetzt, UMAP fehlt: {pca_umap_missing_examples}\")\n",
    "\n",
    "        # CSV-Zeile\n",
    "        if writer:\n",
    "            writer.writerow([\n",
    "                tbl, rows_total,\n",
    "                emb_db_set, emb_fs_missing,\n",
    "                pca_db_set, pca_fs_missing,\n",
    "                pca_db_not_set,\n",
    "                umap_set, umap_missing_for_pca,\n",
    "                \" | \".join(miss_emb_examples),\n",
    "                \" | \".join(miss_pca_examples),\n",
    "                \" | \".join(pca_umap_missing_examples),\n",
    "            ])\n",
    "\n",
    "        # Globalsumme\n",
    "        G_rows += rows_total\n",
    "        G_emb_set += emb_db_set\n",
    "        G_pca_set += pca_db_set\n",
    "        G_umap_set += umap_set\n",
    "        G_emb_fs_missing += emb_fs_missing\n",
    "        G_pca_fs_missing += pca_fs_missing\n",
    "        G_pca_not_set += pca_db_not_set\n",
    "        G_umap_missing_for_pca += umap_missing_for_pca\n",
    "\n",
    "    # Gesamt-Zusammenfassung\n",
    "    print(\"\\n===== GESAMT =====\")\n",
    "    print(f\"rows_total            : {G_rows}\")\n",
    "    print(f\"emb_db_set            : {G_emb_set}\")\n",
    "    print(f\"emb_fs_missing        : {G_emb_fs_missing}\")\n",
    "    print(f\"pca_db_set            : {G_pca_set}\")\n",
    "    print(f\"pca_db_not_set        : {G_pca_not_set}\")\n",
    "    print(f\"pca_fs_missing        : {G_pca_fs_missing}\")\n",
    "    print(f\"umap_set              : {G_umap_set}\")\n",
    "    print(f\"umap_missing_for_pca  : {G_umap_missing_for_pca}\")\n",
    "\n",
    "    if writer:\n",
    "        csvfile.close()\n",
    "        print(f\"\\nCSV-Report geschrieben nach: {CSV_PATH.resolve()}\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"[{datetime.now()}] Audit startet …\")\n",
    "    main()\n",
    "    print(f\"[{datetime.now()}] Audit fertig.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5ad0b",
   "metadata": {},
   "source": [
    "# 2. RUN THROUGH SAVED LARGE FILES  (samller batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b34ee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deferred-Verarbeitung: 100%|██████████| 62050/62050 [6:08:16<00:00,  2.81Bild/s]    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, gc, csv, time, sqlite3, traceback, cProfile, pstats\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "# Optional: PIL nur fürs Resize\n",
    "try:\n",
    "    from PIL import Image\n",
    "    PIL_AVAILABLE = True\n",
    "except Exception:\n",
    "    PIL_AVAILABLE = False\n",
    "\n",
    "# ===== Deine bestehenden Funktionen (NICHT hier implementieren) =====\n",
    "from features.hash import calc_hash\n",
    "from features.color_vec import calc_histogram\n",
    "from features.embedding_vec import extract_embeddings\n",
    "from image_load import fast_load\n",
    "\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "EMBEDDING_DIR = r\"C:\\BIG_DATA\\embeddings\"\n",
    "DEFERRED_CSV = r\"Z:\\CODING\\UNI\\BIG_DATA\\src\\Large_images.csv\"\n",
    "\n",
    "LOG_FILE = r\"Z:\\CODING\\UNI\\BIG_DATA\\src\\verarbeitung_log_deferred.txt\"\n",
    "SKIPPED_LOG = r\"Z:\\CODING\\UNI\\BIG_DATA\\src\\skipped_and_errors.txt\"\n",
    "\n",
    "# Sharding\n",
    "IMAGES_PER_TABLE = 50_000\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "\n",
    "# Preprocess/Embedding\n",
    "TARGET_SIZE = (224, 224)      # (W,H) für PIL\n",
    "EMBED_INPUT_DTYPE = \"float32\" # \"float32\" (0..1) oder \"uint8\"\n",
    "MICRO_BATCH = 32              # Embedding-Microbatch\n",
    "\n",
    "# Pixel-Budget: max. Summe der (H*W) der KLEINEN Bilder im RAM\n",
    "BATCH_TARGET = 100            # grobe Ziel-Batchgröße in kleinen Bildern\n",
    "PIXEL_BUDGET = BATCH_TARGET * TARGET_SIZE[0] * TARGET_SIZE[1]\n",
    "\n",
    "# Dirs\n",
    "Path(EMBEDDING_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ========== DB ==========\n",
    "conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA journal_mode=OFF;\")\n",
    "cursor.execute(\"PRAGMA synchronous=OFF;\")\n",
    "cursor.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "cursor.execute(\"PRAGMA mmap_size=0;\")\n",
    "\n",
    "def create_table_if_not_exists(table_name):\n",
    "    cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            filename TEXT NOT NULL,\n",
    "            path TEXT NOT NULL,\n",
    "            color_hist TEXT,\n",
    "            embedding_path TEXT,\n",
    "            image_hash TEXT,\n",
    "            resolution TEXT,\n",
    "            file_size INTEGER,\n",
    "            pca_embedding TEXT,\n",
    "            umap_x REAL,\n",
    "            umap_y REAL\n",
    "        )\n",
    "    ''')\n",
    "    cursor.execute(f'CREATE INDEX IF NOT EXISTS idx_{table_name}_filename_path ON {table_name}(filename, path);')\n",
    "\n",
    "def get_existing_shards():\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "                   (f\"{TABLE_PREFIX}%\",))\n",
    "    return [r[0] for r in cursor.fetchall()]\n",
    "\n",
    "def rows_in_table(table_name):\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "def exists_in_any_shard(filename, path, shard_tables):\n",
    "    for tbl in shard_tables:\n",
    "        cursor.execute(f\"SELECT 1 FROM {tbl} WHERE filename=? AND path=? LIMIT 1\", (filename, path))\n",
    "        if cursor.fetchone():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def save_batch_to_db(entries, table_name, logfile):\n",
    "    if not entries:\n",
    "        return\n",
    "    log_debug(logfile, f\"[DEBUG] Speichere {len(entries)} Einträge in {table_name} …\")\n",
    "    start = time.time()\n",
    "    cursor.executemany(f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        (filename, path, color_hist, embedding_path, image_hash, resolution, file_size,\n",
    "         pca_embedding, umap_x, umap_y)\n",
    "        VALUES ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", entries)\n",
    "    conn.commit()\n",
    "    log_debug(logfile, f\"[DEBUG] DB-Speicherung: {time.time()-start:.2f}s\")\n",
    "\n",
    "\n",
    "# ========== Logging ==========\n",
    "def log_debug(logfile, msg):\n",
    "    logfile.write(msg + \"\\n\"); logfile.flush()\n",
    "\n",
    "def log_skip_or_error(filename, path, reason):\n",
    "    with open(SKIPPED_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{datetime.now()} | {filename} | {path} | {reason}\\n\")\n",
    "\n",
    "\n",
    "# ========== Utils ==========\n",
    "def _unique_npy_path(base_path):\n",
    "    base, ext = os.path.splitext(base_path)\n",
    "    if ext.lower() != \".npy\":\n",
    "        base = base + \".npy\"\n",
    "    out = base\n",
    "    c = 1\n",
    "    while os.path.exists(out):\n",
    "        out = f\"{base[:-4]}_{c}.npy\"\n",
    "        c += 1\n",
    "    return out\n",
    "\n",
    "def preprocess_for_model(img_uint8, target_size=TARGET_SIZE):\n",
    "    if PIL_AVAILABLE:\n",
    "        im = Image.fromarray(img_uint8)\n",
    "        im = im.resize(target_size, Image.BILINEAR)\n",
    "        arr = np.asarray(im)\n",
    "    else:\n",
    "        th, tw = target_size[1], target_size[0]\n",
    "        y_idx = (np.linspace(0, img_uint8.shape[0]-1, th)).astype(np.int32)\n",
    "        x_idx = (np.linspace(0, img_uint8.shape[1]-1, tw)).astype(np.int32)\n",
    "        arr = img_uint8[np.ix_(y_idx, x_idx)]\n",
    "    if EMBED_INPUT_DTYPE == \"float32\":\n",
    "        return (arr.astype(np.float32) / 255.0)\n",
    "    else:\n",
    "        return arr.astype(np.uint8)\n",
    "\n",
    "def to_embed_dtype(arr):\n",
    "    if EMBED_INPUT_DTYPE == \"float32\":\n",
    "        return arr.astype(np.float32) if arr.dtype != np.float32 else arr\n",
    "    else:\n",
    "        if arr.dtype == np.float32:\n",
    "            return (np.clip(arr, 0.0, 1.0) * 255.0).round().astype(np.uint8)\n",
    "        return arr.astype(np.uint8)\n",
    "\n",
    "def extract_embeddings_micro(bimgs, chunk=MICRO_BATCH):\n",
    "    outs = []\n",
    "    for i in range(0, len(bimgs), chunk):\n",
    "        outs.append(extract_embeddings(bimgs[i:i+chunk]))\n",
    "    return np.vstack(outs)\n",
    "\n",
    "\n",
    "# ========== CSV-Quelle (Large_images.csv) ==========\n",
    "def iter_deferred_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Erwartetes Format je Zeile: timestamp,filename,path,reason\n",
    "    (wir ignorieren timestamp/reason und yielden (filename, path))\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        r = csv.reader(f)\n",
    "        for row in r:\n",
    "            if not row or len(row) < 3:\n",
    "                continue\n",
    "            # robust gg. evtl. Header\n",
    "            if row[0].lower().startswith(\"timestamp\"):\n",
    "                continue\n",
    "            ts, filename, path = row[0], row[1], row[2]\n",
    "            yield filename, path\n",
    "\n",
    "\n",
    "# ========== Feature-Vorbereitung ==========\n",
    "def prepare_image_features(filename, path, logfile):\n",
    "    \"\"\"\n",
    "    Lädt großes Bild, verkleinert sofort, berechnet Hash/Histogramm.\n",
    "    Nur die KLEINE Version wird im Batch gehalten.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = fast_load(path)  # -> uint8 RGB\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            raise TypeError(\"fast_load() must return a NumPy uint8 array.\")\n",
    "        if img.dtype != np.uint8:\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "        resolution = f\"{img.shape[1]}x{img.shape[0]}\"\n",
    "        file_size  = os.path.getsize(path)\n",
    "\n",
    "        img_hash = calc_hash(img)\n",
    "\n",
    "        img_small = preprocess_for_model(img)\n",
    "        emb_input = to_embed_dtype(img_small)\n",
    "        color_hist = calc_histogram(img_small)  # bins=default deiner Funktion\n",
    "        hist_str = \",\".join(str(round(float(v), 6)) for v in np.ravel(color_hist))\n",
    "\n",
    "        del img\n",
    "        return (filename, path, emb_input, hist_str, img_hash, resolution, file_size)\n",
    "\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        reason = f\"{e} | Traceback:\\n{tb}\"\n",
    "        log_debug(logfile, f\"[ERROR] Fehler bei {filename}: {e}\")\n",
    "        log_skip_or_error(filename, path, reason)\n",
    "        return (filename, path, None, None, None, None, None, reason)\n",
    "\n",
    "\n",
    "# ========== Flush: Embeddings + Insert ==========\n",
    "def flush_by_embeddings_and_insert(batch_meta, batch_imgs, logfile, table_name):\n",
    "    if not batch_imgs:\n",
    "        return 0\n",
    "    try:\n",
    "        embs = extract_embeddings_micro(batch_imgs, chunk=MICRO_BATCH)\n",
    "    except Exception as e:\n",
    "        log_debug(logfile, f\"[ERROR] Embedding-Chunk fehlgeschlagen: {e}\")\n",
    "        embs = []\n",
    "        for (filename, path, hist_str, img_hash, resolution, size), img_small in zip(batch_meta, batch_imgs):\n",
    "            try:\n",
    "                embs.append(extract_embeddings([img_small])[0])\n",
    "            except Exception as ee:\n",
    "                tb = traceback.format_exc()\n",
    "                log_skip_or_error(filename, path, f\"Embedding-Fehler (single): {ee}\\n{tb}\")\n",
    "        if not embs:\n",
    "            return 0\n",
    "        embs = np.stack(embs, axis=0)\n",
    "\n",
    "    entries = []\n",
    "    kept = 0\n",
    "    for (filename, path, hist_str, img_hash, resolution, size), emb in zip(batch_meta, embs):\n",
    "        try:\n",
    "            emb_path = _unique_npy_path(os.path.join(EMBEDDING_DIR, f\"{filename}.npy\"))\n",
    "            np.save(emb_path, emb.astype(np.float32))\n",
    "            entries.append((filename, path, hist_str, emb_path, img_hash, resolution, size, \"\", 0.0, 0.0))\n",
    "            kept += 1\n",
    "        except Exception as e_save:\n",
    "            tb = traceback.format_exc()\n",
    "            log_skip_or_error(filename, path, f\"Embedding-Save-Fehler: {e_save}\\n{tb}\")\n",
    "\n",
    "    save_batch_to_db(entries, table_name, logfile)\n",
    "    del embs, entries\n",
    "    return kept\n",
    "\n",
    "\n",
    "# ========== MAIN ==========\n",
    "def main():\n",
    "    logfile = open(LOG_FILE, \"a\", encoding=\"utf-8\", buffering=1)\n",
    "    log_debug(logfile, f\"[{datetime.now()}] [DEBUG] Start Deferred-Run (kein PCA/UMAP)\")\n",
    "    # Liste existierender Shards ermitteln\n",
    "    shard_tables = get_existing_shards()\n",
    "    if not shard_tables:\n",
    "        shard_tables = [f\"{TABLE_PREFIX}1\"]\n",
    "        create_table_if_not_exists(shard_tables[0])\n",
    "\n",
    "    # In die letzte Tabelle weiter einfügen\n",
    "    current_table_name = shard_tables[-1]\n",
    "    current_count = rows_in_table(current_table_name)\n",
    "\n",
    "    paths_iter = list(iter_deferred_csv(DEFERRED_CSV))\n",
    "    total_items = len(paths_iter)\n",
    "    pbar = tqdm(total=total_items, desc=\"Deferred-Verarbeitung\", unit=\"Bild\")\n",
    "\n",
    "    batch_meta, batch_imgs = [], []\n",
    "    cur_pixels = 0\n",
    "    total_inserted = 0\n",
    "\n",
    "    for filename, path in paths_iter:\n",
    "        # Dedupe/Existenzcheck über alle Shards\n",
    "        if exists_in_any_shard(filename, path, shard_tables):\n",
    "            log_skip_or_error(filename, path, \"Bereits in DB (deferred pass)\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        result = prepare_image_features(filename, path, logfile)\n",
    "        if len(result) == 8:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        _, p, emb_input, hist_str, img_hash, resolution, size = result\n",
    "        batch_meta.append((filename, p, hist_str, img_hash, resolution, size))\n",
    "        batch_imgs.append(emb_input)\n",
    "        cur_pixels += emb_input.shape[0] * emb_input.shape[1]\n",
    "        pbar.update(1)\n",
    "\n",
    "        if cur_pixels >= PIXEL_BUDGET:\n",
    "            kept = flush_by_embeddings_and_insert(batch_meta, batch_imgs, logfile, current_table_name)\n",
    "            total_inserted += kept\n",
    "            current_count += kept\n",
    "\n",
    "            batch_meta.clear(); batch_imgs.clear(); cur_pixels = 0\n",
    "            gc.collect()\n",
    "            try:\n",
    "                import torch\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    log_debug(logfile, \"[DEBUG] CUDA Cache geleert.\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Sharding: neue Tabelle nötig?\n",
    "            if current_count >= IMAGES_PER_TABLE:\n",
    "                # neue Tabelle anlegen\n",
    "                next_id = len(shard_tables) + 1\n",
    "                current_table_name = f\"{TABLE_PREFIX}{next_id}\"\n",
    "                create_table_if_not_exists(current_table_name)\n",
    "                shard_tables.append(current_table_name)\n",
    "                current_count = 0\n",
    "                log_debug(logfile, f\"[DEBUG] Neue Tabelle angelegt: {current_table_name}\")\n",
    "\n",
    "    # Finaler Flush\n",
    "    if batch_imgs:\n",
    "        kept = flush_by_embeddings_and_insert(batch_meta, batch_imgs, logfile, current_table_name)\n",
    "        total_inserted += kept\n",
    "        current_count += kept\n",
    "        batch_meta.clear(); batch_imgs.clear(); cur_pixels = 0\n",
    "        gc.collect()\n",
    "\n",
    "    pbar.close()\n",
    "    log_debug(logfile, f\"[{datetime.now()}] ✓ Deferred-Run fertig. Inserts: {total_inserted}\")\n",
    "    logfile.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# ========= ENTRYPOINT =========\n",
    "if __name__ == \"__main__\":\n",
    "    with cProfile.Profile() as pr:\n",
    "        main()\n",
    "    with open(\"profiling_results_deferred.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        stats = pstats.Stats(pr, stream=f)\n",
    "        stats.sort_stats(\"cumtime\").print_stats()\n",
    "    stats.dump_stats(\"profiling_results_deferred.prof\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1d494",
   "metadata": {},
   "source": [
    "# FIT AND TRANSFORM PCA AND CALCULATE UMAP KOORDINATES GLOBALLY ON ALL THE IMAGES AT THE SAME TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71298220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PASS1 IPCA partial_fit:   2%|▏         | 8192/359754 [07:31<5:23:06, 18.13vec/s]\n",
      "PASS1 IPCA partial_fit: 100%|██████████| 359754/359754 [30:45<00:00, 194.95vec/s]\n",
      "PASS2 IPCA transform+save: 100%|██████████| 359754/359754 [34:14<00:00, 175.08vec/s]\n",
      "z:\\CODING\\UNI\\BIG_DATA\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "PASS3 UMAP transform+update: 100%|██████████| 359754/359754 [27:53<00:00, 214.94vec/s]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Globales Finalisieren (IPCA + UMAP) über alle Shards.\n",
    "Fixes:\n",
    "- PASS 2: korrektes Mapping (id <-> embedding_path), defekte Pfade werden pro-Item geloggt/übersprungen\n",
    "- _unique_npy_path: Endung .npy bleibt korrekt erhalten (keine \"pfad ohne endung\" mehr)\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, sqlite3, random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import umap\n",
    "\n",
    "\n",
    "# ========= CONFIG =========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "EMBEDDING_PCA_DIR = Path(r\"C:\\BIG_DATA\\embeddings_pca\")\n",
    "LOG_FILE = \"verarbeitung_log.txt\"\n",
    "SKIPPED_LOG = \"skipped_and_errors.txt\"\n",
    "\n",
    "# Tuning\n",
    "PCA_COMPONENTS = 64        # 64 oder 100 – kleiner = schneller/leichter\n",
    "UMAP_SAMPLE_SIZE = 25_000  # Sample fürs UMAP-Fit (Rest wird transformiert)\n",
    "CHUNK = 4096               # I/O-Chunkgröße (RAM vs. Speed)\n",
    "\n",
    "\n",
    "# ========= Logging / Helpers =========\n",
    "def log_debug(f, msg: str):\n",
    "    f.write(msg + \"\\n\")\n",
    "    f.flush()\n",
    "\n",
    "def log_skip_or_error(filename_or_id, path, reason):\n",
    "    with open(SKIPPED_LOG, \"a\", encoding=\"utf-8\") as ff:\n",
    "        ff.write(f\"{datetime.now()} | {filename_or_id} | {path} | {reason}\\n\")\n",
    "\n",
    "def _unique_npy_path(base_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Gibt einen nicht-belegten .npy-Pfad zurück. Falls base_path bereits .npy hat,\n",
    "    bleibt die Endung erhalten; sonst wird .npy angehängt.\n",
    "    \"\"\"\n",
    "    base, ext = os.path.splitext(base_path)\n",
    "    if ext.lower() != \".npy\":\n",
    "        out = base_path + \".npy\"\n",
    "    else:\n",
    "        out = base_path\n",
    "    c = 1\n",
    "    while os.path.exists(out):\n",
    "        # base ist ohne Endung\n",
    "        out = f\"{base}_{c}.npy\"\n",
    "        c += 1\n",
    "    return out\n",
    "\n",
    "def get_tables(cur, prefix):\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{prefix}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "def load_embeddings(paths):\n",
    "    \"\"\"Lädt eine Liste von .npy-Pfaden -> (N,D) float32, überspringt defekte. (Nur für PASS 1.)\"\"\"\n",
    "    mats, keep = [], []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            mats.append(np.load(p))\n",
    "            keep.append(p)\n",
    "        except Exception as e:\n",
    "            log_skip_or_error(\"UNKNOWN\", p, f\"PCA-Load-Fehler: {e}\")\n",
    "    if not mats:\n",
    "        return None, []\n",
    "    X = np.stack(mats, axis=0).astype(np.float32)\n",
    "    return X, keep\n",
    "\n",
    "\n",
    "# ========= Finalizer =========\n",
    "def finalize_all_shards_global(conn, cursor, logfile,\n",
    "                               sample_size=UMAP_SAMPLE_SIZE,\n",
    "                               pca_components=PCA_COMPONENTS,\n",
    "                               chunk=CHUNK):\n",
    "    log_debug(logfile, \"[DEBUG] === Globales Finalisieren (IPCA+UMAP) startet ===\")\n",
    "\n",
    "    # Tabellen finden\n",
    "    tables = get_tables(cursor, TABLE_PREFIX)\n",
    "    if not tables:\n",
    "        log_debug(logfile, \"[WARN] Keine Shard-Tabellen gefunden – Abbruch.\")\n",
    "        return\n",
    "    os.makedirs(EMBEDDING_PCA_DIR, exist_ok=True)\n",
    "\n",
    "    # eigener Lese-Cursor (SELECT vom UPDATE trennen!)\n",
    "    cur_read = conn.cursor()\n",
    "\n",
    "    # -------- totals für Progressbars --------\n",
    "    total_embed_rows = 0\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(f\"SELECT COUNT(*) FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\")\n",
    "        total_embed_rows += int(cur_read.fetchone()[0])\n",
    "\n",
    "    # -------- PASS 1: IPCA partial_fit (global, gestreamt) --------\n",
    "    ipca = None\n",
    "    seen = 0\n",
    "    pbar1 = tqdm(total=total_embed_rows, desc=\"PASS1 IPCA partial_fit\", unit=\"vec\")\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(f\"SELECT embedding_path FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\")\n",
    "        while True:\n",
    "            rows = cur_read.fetchmany(chunk)\n",
    "            if not rows:\n",
    "                break\n",
    "            paths = [r[0] for r in rows]\n",
    "            X, _ = load_embeddings(paths)  # für partial_fit reicht das\n",
    "            if X is not None:\n",
    "                if ipca is None:\n",
    "                    nfeat = X.shape[1]\n",
    "                    ncomp = min(pca_components, nfeat)\n",
    "                    ipca = IncrementalPCA(n_components=ncomp)\n",
    "                    log_debug(logfile, f\"[DEBUG] IPCA init mit n_components={ncomp} (feat={nfeat})\")\n",
    "                ipca.partial_fit(X)\n",
    "                seen += X.shape[0]\n",
    "                del X\n",
    "            # Fortschritt nach geplanter Menge im Chunk (auch wenn einzelne fehlschlugen)\n",
    "            pbar1.update(len(paths))\n",
    "            gc.collect()\n",
    "    pbar1.close()\n",
    "\n",
    "    if ipca is None or seen == 0:\n",
    "        log_debug(logfile, \"[WARN] Keine Embeddings gefunden – IPCA konnte nicht trainiert werden.\")\n",
    "        return\n",
    "\n",
    "    log_debug(logfile, f\"[DEBUG] PASS 1 fertig – partial_fit auf {seen} Vektoren.\")\n",
    "    _print_res(\"Nach IPCA PASS 1\", logfile)\n",
    "\n",
    "    # -------- PASS 2: transform, PCA speichern, pca_embedding updaten, Sample sammeln --------\n",
    "    rng = random.Random(42)\n",
    "    sample_Xp, sample_cnt = [], 0\n",
    "    saved = 0\n",
    "\n",
    "    pbar2 = tqdm(total=total_embed_rows, desc=\"PASS2 IPCA transform+save\", unit=\"vec\")\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(f\"SELECT id, embedding_path FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\")\n",
    "        while True:\n",
    "            rows = cur_read.fetchmany(chunk)\n",
    "            if not rows:\n",
    "                break\n",
    "\n",
    "            ids   = [r[0] for r in rows]\n",
    "            paths = [r[1] for r in rows]\n",
    "\n",
    "            # WICHTIG: pro (id, path) laden -> nur erfolgreiche Paare übernehmen\n",
    "            mats, keep_ids = [], []\n",
    "            for _id, p in zip(ids, paths):\n",
    "                try:\n",
    "                    mats.append(np.load(p).astype(np.float32))\n",
    "                    keep_ids.append(_id)\n",
    "                except Exception as e:\n",
    "                    log_skip_or_error(f\"DB_ID={_id}\", p, f\"PCA-Load-Fehler: {e}\")\n",
    "\n",
    "            if mats:\n",
    "                X = np.stack(mats, axis=0)\n",
    "                Xp = ipca.transform(X)\n",
    "\n",
    "                updates = []\n",
    "                for _id_ok, vec in zip(keep_ids, Xp):\n",
    "                    pca_path = _unique_npy_path(str(EMBEDDING_PCA_DIR / f\"{_id_ok}_pca.npy\"))\n",
    "                    try:\n",
    "                        np.save(pca_path, vec.astype(np.float32))\n",
    "                    except Exception as e:\n",
    "                        log_skip_or_error(f\"DB_ID={_id_ok}\", pca_path, f\"PCA-Save-Fehler: {e}\")\n",
    "                        pca_path = \"\"\n",
    "                    updates.append((pca_path, _id_ok))\n",
    "\n",
    "                    # Reservoir-Sampling für UMAP-Fit\n",
    "                    sample_cnt += 1\n",
    "                    if len(sample_Xp) < sample_size:\n",
    "                        sample_Xp.append(vec.copy())\n",
    "                    else:\n",
    "                        j = rng.randrange(sample_cnt)\n",
    "                        if j < sample_size:\n",
    "                            sample_Xp[j] = vec.copy()\n",
    "\n",
    "                cursor.executemany(f\"UPDATE {tbl} SET pca_embedding=? WHERE id=?\", updates)\n",
    "                conn.commit()\n",
    "                saved += len(updates)\n",
    "\n",
    "                del X, Xp, updates, mats\n",
    "                gc.collect()\n",
    "\n",
    "            # Fortschritt: geplanter Chunk (auch wenn einzelne fehlschlugen)\n",
    "            pbar2.update(len(paths))\n",
    "    pbar2.close()\n",
    "\n",
    "    log_debug(logfile, f\"[DEBUG] PASS 2 fertig – {saved} PCA-Vektoren gespeichert.\")\n",
    "    _print_res(\"Nach IPCA PASS 2\", logfile)\n",
    "\n",
    "    if not sample_Xp:\n",
    "        log_debug(logfile, \"[WARN] Kein PCA-Sample – UMAP wird übersprungen.\")\n",
    "        return\n",
    "\n",
    "    # -------- UMAP Fit (auf Sample) --------\n",
    "    sample_mat = np.vstack(sample_Xp).astype(np.float32)\n",
    "    reducer = umap.UMAP(n_components=2, metric=\"euclidean\", random_state=42, low_memory=True)\n",
    "    log_debug(logfile, f\"[DEBUG] UMAP Fit auf Sample mit {sample_mat.shape[0]} Vektoren …\")\n",
    "    reducer.fit(sample_mat)\n",
    "    del sample_mat, sample_Xp\n",
    "    gc.collect()\n",
    "    _print_res(\"Nach UMAP Fit (Sample)\", logfile)\n",
    "\n",
    "    # -------- PASS 3: UMAP transform aller PCA-Vektoren & DB-Update --------\n",
    "    # wie viele haben jetzt pca_embedding?\n",
    "    total_pca_rows = 0\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(f\"SELECT COUNT(*) FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\")\n",
    "        total_pca_rows += int(cur_read.fetchone()[0])\n",
    "\n",
    "    updated = 0\n",
    "    pbar3 = tqdm(total=total_pca_rows, desc=\"PASS3 UMAP transform+update\", unit=\"vec\")\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(f\"SELECT id, pca_embedding FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\")\n",
    "        while True:\n",
    "            rows = cur_read.fetchmany(chunk)\n",
    "            if not rows:\n",
    "                break\n",
    "\n",
    "            ids = [r[0] for r in rows]\n",
    "            ppaths = [r[1] for r in rows]\n",
    "\n",
    "            mats, keep_ids = [], []\n",
    "            for _id, p in zip(ids, ppaths):\n",
    "                try:\n",
    "                    mats.append(np.load(p).astype(np.float32))\n",
    "                    keep_ids.append(_id)\n",
    "                except Exception as e:\n",
    "                    log_skip_or_error(f\"DB_ID={_id}\", p, f\"UMAP-PCA-Load-Fehler: {e}\")\n",
    "\n",
    "            if mats:\n",
    "                Xp = np.stack(mats, axis=0)\n",
    "                coords = reducer.transform(Xp)\n",
    "                updates = [(float(x), float(y), i) for i, (x, y) in zip(keep_ids, coords)]\n",
    "                cursor.executemany(f\"UPDATE {tbl} SET umap_x=?, umap_y=? WHERE id=?\", updates)\n",
    "                conn.commit()\n",
    "                updated += len(updates)\n",
    "\n",
    "                del Xp, coords, updates, mats\n",
    "                gc.collect()\n",
    "\n",
    "            pbar3.update(len(rows))\n",
    "    pbar3.close()\n",
    "\n",
    "    log_debug(logfile, f\"[DEBUG] PASS 3 fertig – {updated} UMAP-Koordinaten aktualisiert.\")\n",
    "    _print_res(\"Nach UMAP PASS 3\", logfile)\n",
    "    log_debug(logfile, \"[DEBUG] === Globales Finalisieren (IPCA+UMAP) abgeschlossen ===\")\n",
    "\n",
    "\n",
    "def _print_res(stage, logfile):\n",
    "    # kurze Ressourcen-Info\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / (1024 ** 2)\n",
    "    cpu = process.cpu_percent(interval=0.1)\n",
    "    log_debug(logfile, f\"[RESOURCE] {stage} | RAM: {mem:.2f} MB | CPU: {cpu:.2f}%\")\n",
    "\n",
    "\n",
    "# ========= main =========\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(EMBEDDING_PCA_DIR, exist_ok=True)\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # (Optionale) PRAGMAs\n",
    "    cursor.execute(\"PRAGMA journal_mode=OFF;\")\n",
    "    cursor.execute(\"PRAGMA synchronous=OFF;\")\n",
    "    cursor.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "    cursor.execute(\"PRAGMA mmap_size=0;\")\n",
    "\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\", buffering=1) as logfile:\n",
    "        log_debug(logfile, f\"[{datetime.now()}] [DEBUG] finalize_only gestartet\")\n",
    "        try:\n",
    "            finalize_all_shards_global(conn, cursor, logfile,\n",
    "                                       sample_size=UMAP_SAMPLE_SIZE,\n",
    "                                       pca_components=PCA_COMPONENTS,\n",
    "                                       chunk=CHUNK)\n",
    "        finally:\n",
    "            log_debug(logfile, f\"[{datetime.now()}] [DEBUG] finalize_only beendet\")\n",
    "\n",
    "    conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd54901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_1: embeddings ok=100000, missing_or_wrong=0, total_nonempty=100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_2: embeddings ok=51500, missing_or_wrong=0, total_nonempty=51500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_3: embeddings ok=50000, missing_or_wrong=0, total_nonempty=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_4: embeddings ok=50000, missing_or_wrong=0, total_nonempty=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_5: embeddings ok=50004, missing_or_wrong=0, total_nonempty=50004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_6: embeddings ok=50000, missing_or_wrong=0, total_nonempty=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_7: embeddings ok=8250, missing_or_wrong=0, total_nonempty=8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os, sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "\n",
    "def get_tables(cur):\n",
    "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "                (f\"{TABLE_PREFIX}%\",))\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "def main():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "    tables = get_tables(cur)\n",
    "    for tbl in tables:\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {tbl} WHERE embedding_path IS NOT NULL AND TRIM(embedding_path)<>''\")\n",
    "        total = int(cur.fetchone()[0])\n",
    "        if total == 0:\n",
    "            print(f\"{tbl}: 0 embedding_path – übersprungen\")\n",
    "            continue\n",
    "        ok = missing = 0\n",
    "        cur.execute(f\"SELECT embedding_path FROM {tbl} WHERE embedding_path IS NOT NULL AND TRIM(embedding_path)<>''\")\n",
    "        for (p,) in tqdm(cur.fetchall(), desc=tbl, unit=\"path\", leave=False):\n",
    "            p = p.strip()\n",
    "            if os.path.exists(p):\n",
    "                ok += 1\n",
    "            elif os.path.exists(p + \".npy\"):\n",
    "                # Falls hier viele Treffer: vorherigen Repair-Job laufen lassen, um in der DB auf '.npy' zu korrigieren\n",
    "                missing += 1\n",
    "            else:\n",
    "                missing += 1\n",
    "        print(f\"{tbl}: embeddings ok={ok}, missing_or_wrong={missing}, total_nonempty={total}\")\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a42ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tabellen: 100%|██████████| 7/7 [00:13<00:00,  1.94s/tbl]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Summary: hash_reports_all_columns\\all_columns_summary.csv\n",
      "[OK] Missing paths (alle Spalten): hash_reports_all_columns\\missing_by_column_paths.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Geht über alle image_features_part_% Tabellen,\n",
    "zählt pro Spalte (alle!) filled/missing\n",
    "und exportiert fehlende Pfade je Spalte in eine CSV.\n",
    "Erzeugt:\n",
    "- all_columns_summary.csv  (long format: table, column, col_type, total, filled, missing)\n",
    "- missing_by_column_paths.csv (table, column, id, filename, path)\n",
    "\"\"\"\n",
    "\n",
    "import os, csv, sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== CONFIG ======\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "OUTPUT_DIR = \"hash_reports_all_columns\"\n",
    "USE_TQDM = True\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "SUMMARY_CSV = os.path.join(OUTPUT_DIR, \"all_columns_summary.csv\")\n",
    "MISSING_CSV = os.path.join(OUTPUT_DIR, \"missing_by_column_paths.csv\")\n",
    "\n",
    "# ====== DB helpers ======\n",
    "def get_shard_tables(cur):\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "def get_columns(cur, table):\n",
    "    \"\"\"\n",
    "    Liefert Liste von (name, decl_type) aus PRAGMA table_info.\n",
    "    decl_type kann z.B. 'TEXT', 'INTEGER', 'REAL', 'NUMERIC', 'BLOB' etc. sein.\n",
    "    \"\"\"\n",
    "    cur.execute(f\"PRAGMA table_info({table});\")\n",
    "    cols = [(row[1], (row[2] or \"\").upper()) for row in cur.fetchall()]\n",
    "    # 'id' rauswerfen\n",
    "    return [(n, t) for (n, t) in cols if n.lower() != \"id\"]\n",
    "\n",
    "def count_total(cur, table):\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    return int(cur.fetchone()[0])\n",
    "\n",
    "def build_missing_condition(col_name, col_type_upper):\n",
    "    # TEXT -> Null oder leere/Whitespace-Strings; sonst nur NULL\n",
    "    if \"TEXT\" in col_type_upper:\n",
    "        return f\"({col_name} IS NULL OR TRIM({col_name}) = '')\"\n",
    "    else:\n",
    "        return f\"({col_name} IS NULL)\"\n",
    "\n",
    "def column_exists(cur, table, col_name):\n",
    "    cur.execute(f\"PRAGMA table_info({table});\")\n",
    "    return any(r[1] == col_name for r in cur.fetchall())\n",
    "\n",
    "# ====== Export missing paths (streaming) ======\n",
    "def export_missing_for_column(cur, table, col_name, cond, writer, has_filename, has_path):\n",
    "    # Baue SELECT dynamisch je nach vorhandenen Spalten\n",
    "    select_cols = [\"id\"]\n",
    "    if has_filename: select_cols.append(\"filename\")\n",
    "    if has_path:     select_cols.append(\"path\")\n",
    "    sel = \", \".join(select_cols)\n",
    "\n",
    "    cur.execute(f\"SELECT {sel} FROM {table} WHERE {cond}\")\n",
    "    while True:\n",
    "        rows = cur.fetchmany(5000)\n",
    "        if not rows:\n",
    "            break\n",
    "        for row in rows:\n",
    "            # row ist Tupel mit 1..3 Elementen je nach vorhandenen Spalten\n",
    "            rid = row[0]\n",
    "            fname = row[1] if has_filename else \"\"\n",
    "            pth = row[2] if (has_filename and has_path and len(row) > 2) else (row[1] if (has_path and not has_filename and len(row) > 1) else \"\")\n",
    "            writer.writerow({\"table\": table, \"column\": col_name, \"id\": rid, \"filename\": fname, \"path\": pth})\n",
    "\n",
    "def main():\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    tables = get_shard_tables(cur)\n",
    "    if not tables:\n",
    "        print(\"Keine Shard-Tabellen gefunden.\")\n",
    "        return\n",
    "\n",
    "    # Prepare CSVs\n",
    "    with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fsum, \\\n",
    "         open(MISSING_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fmiss:\n",
    "\n",
    "        sum_writer = csv.DictWriter(fsum, fieldnames=[\"table\", \"column\", \"col_type\", \"total\", \"filled\", \"missing\"])\n",
    "        sum_writer.writeheader()\n",
    "\n",
    "        miss_writer = csv.DictWriter(fmiss, fieldnames=[\"table\", \"column\", \"id\", \"filename\", \"path\"])\n",
    "        miss_writer.writeheader()\n",
    "\n",
    "        iterator = tqdm(tables, desc=\"Tabellen\", unit=\"tbl\") if USE_TQDM else tables\n",
    "        for tbl in iterator:\n",
    "            # Spalten & Basisinfos\n",
    "            cols = get_columns(cur, tbl)\n",
    "            total = count_total(cur, tbl)\n",
    "\n",
    "            # Prüfen, ob filename/path vorhanden sind (für Export)\n",
    "            present_colnames = {name for (name, _) in cols}\n",
    "            has_filename = \"filename\" in present_colnames\n",
    "            has_path = \"path\" in present_colnames\n",
    "\n",
    "            # Fortschritt pro Tabelle (über Spalten)\n",
    "            col_iter = tqdm(cols, desc=f\"{tbl}\", unit=\"col\", leave=False) if USE_TQDM else cols\n",
    "            for col_name, col_type in col_iter:\n",
    "                cond = build_missing_condition(col_name, col_type)\n",
    "\n",
    "                # Missing zählen\n",
    "                cur.execute(f\"SELECT COUNT(*) FROM {tbl} WHERE {cond}\")\n",
    "                missing = int(cur.fetchone()[0])\n",
    "                filled = total - missing\n",
    "\n",
    "                sum_writer.writerow({\n",
    "                    \"table\": tbl,\n",
    "                    \"column\": col_name,\n",
    "                    \"col_type\": col_type,\n",
    "                    \"total\": total,\n",
    "                    \"filled\": filled,\n",
    "                    \"missing\": missing\n",
    "                })\n",
    "\n",
    "                # Fehlende Pfade exportieren\n",
    "                if missing > 0:\n",
    "                    export_missing_for_column(cur, tbl, col_name, cond, miss_writer, has_filename, has_path)\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"[OK] Summary: {SUMMARY_CSV}\")\n",
    "    print(f\"[OK] Missing paths (alle Spalten): {MISSING_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140d7c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexiere Embeddings im Ordner …\n",
      "Gefundene Embeddings: 359,474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_1: 100%|██████████| 100000/100000 [00:07<00:00, 13850.67row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_1: ok=100,000, fixed=0, missing=0, total=100,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_2: 100%|██████████| 51500/51500 [00:03<00:00, 14453.34row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_2: ok=51,500, fixed=0, missing=0, total=51,500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_3: 100%|██████████| 50000/50000 [00:03<00:00, 14801.86row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_3: ok=50,000, fixed=0, missing=0, total=50,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_4: 100%|██████████| 50000/50000 [00:03<00:00, 15238.08row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_4: ok=50,000, fixed=0, missing=0, total=50,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_5: 100%|██████████| 50004/50004 [00:03<00:00, 15031.16row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_5: ok=50,004, fixed=0, missing=0, total=50,004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_6: 100%|██████████| 50000/50000 [02:34<00:00, 324.63row/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_6: ok=5,000, fixed=45,000, missing=0, total=50,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_7: 100%|██████████| 8250/8250 [00:11<00:00, 718.09row/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_7: ok=5,000, fixed=3,250, missing=0, total=8,250\n",
      "\n",
      "=== Zusammenfassung ===\n",
      "OK:       311,504\n",
      "Fixed:    48,250\n",
      "Missing:  0\n",
      "Fehlende Bildpfade in: missing_embedding_images.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\"\"\"\n",
    "Repariert embedding_path in allen image_features_part_% Tabellen – ID-weise paged,\n",
    "damit Updates den Scan nicht abbrechen. Loggt fehlende als Bildpfade.\n",
    "\"\"\"\n",
    "# ===== CONFIG =====\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "EMBEDDING_DIR = Path(r\"C:\\BIG_DATA\\embeddings\")\n",
    "MISSING_TXT = \"missing_embedding_images.txt\"\n",
    "PAGE = 5000  # Seitengröße nach id\n",
    "\n",
    "# ===== Helpers =====\n",
    "def get_tables(cur):\n",
    "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "                (f\"{TABLE_PREFIX}%\",))\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "def col_exists(cur, table, col):\n",
    "    cur.execute(f\"PRAGMA table_info({table});\")\n",
    "    return any(r[1] == col for r in cur.fetchall())\n",
    "\n",
    "def index_embeddings(root: Path):\n",
    "    idx = {}\n",
    "    for base, _, files in os.walk(root):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith(\".npy\"):\n",
    "                full = os.path.join(base, fn)\n",
    "                idx.setdefault(fn, full)  # erster Treffer gewinnt\n",
    "    return idx\n",
    "\n",
    "def expected_basenames(filename: str):\n",
    "    fn = (filename or \"\").strip()\n",
    "    stem = Path(fn).stem\n",
    "    return [f\"{fn}.npy\", f\"{stem}.npy\"]\n",
    "\n",
    "def is_ok(embedding_path: str, filename: str) -> bool:\n",
    "    if not embedding_path:\n",
    "        return False\n",
    "    p = embedding_path.strip()\n",
    "    if not p or not p.lower().endswith(\".npy\"):\n",
    "        return False\n",
    "    if not os.path.exists(p):\n",
    "        return False\n",
    "    base = os.path.basename(p)\n",
    "    return base in set(expected_basenames(filename))\n",
    "\n",
    "def try_fix_path(p_current: str, filename: str, idx: dict, embedding_dir: Path):\n",
    "    fn = (filename or \"\").strip()\n",
    "    stem = Path(fn).stem\n",
    "    cand_names = [f\"{fn}.npy\", f\"{stem}.npy\"]\n",
    "\n",
    "    # 1) alter Pfad + .npy?\n",
    "    if p_current:\n",
    "        p_strip = p_current.strip()\n",
    "        if p_strip and not p_strip.lower().endswith(\".npy\"):\n",
    "            cand = p_strip + \".npy\"\n",
    "            if os.path.exists(cand):\n",
    "                return cand\n",
    "\n",
    "    # 2) direkt im Zielordner\n",
    "    for name in cand_names:\n",
    "        p = embedding_dir / name\n",
    "        if os.path.exists(p):\n",
    "            return str(p)\n",
    "\n",
    "    # 3) Index\n",
    "    for name in cand_names:\n",
    "        if name in idx:\n",
    "            return idx[name]\n",
    "\n",
    "    # 4) eindeutiger Prefix-Treffer (stem_)\n",
    "    stem_prefix = f\"{stem}_\"\n",
    "    matches = [path for base, path in idx.items() if base.startswith(stem_prefix)]\n",
    "    if len(matches) == 1:\n",
    "        return matches[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "# ===== Main per Tabelle (paged) =====\n",
    "def process_table(conn, tbl, idx, missing_file):\n",
    "    cur = conn.cursor()\n",
    "    cur_write = conn.cursor()\n",
    "\n",
    "    # Zählung für Progressbar\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "    total = int(cur.fetchone()[0])\n",
    "    pbar = tqdm(total=total, desc=tbl, unit=\"row\")\n",
    "\n",
    "    ok = fixed = missing = 0\n",
    "    last_id = 0\n",
    "\n",
    "    while True:\n",
    "        # Page nach Primärschlüssel\n",
    "        cur.execute(\n",
    "            f\"\"\"SELECT id, filename, path, embedding_path\n",
    "                FROM {tbl}\n",
    "                WHERE id > ?\n",
    "                ORDER BY id ASC\n",
    "                LIMIT ?\"\"\",\n",
    "            (last_id, PAGE)\n",
    "        )\n",
    "        rows = cur.fetchall()\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        updates = []  # (new_path, id)\n",
    "        for _id, filename, img_path, emb_path in rows:\n",
    "            if is_ok(emb_path, filename):\n",
    "                ok += 1\n",
    "                pbar.update(1)\n",
    "                last_id = _id\n",
    "                continue\n",
    "\n",
    "            newp = try_fix_path(emb_path or \"\", filename or \"\", idx, EMBEDDING_DIR)\n",
    "            if newp:\n",
    "                updates.append((newp, _id))\n",
    "                fixed += 1\n",
    "            else:\n",
    "                # fehlend -> Bildpfad loggen\n",
    "                missing += 1\n",
    "                missing_file.write(f\"{_id}; {tbl}; {filename or ''}; {img_path or ''}\\n\")\n",
    "\n",
    "            pbar.update(1)\n",
    "            last_id = _id\n",
    "\n",
    "        if updates:\n",
    "            cur_write.executemany(f\"UPDATE {tbl} SET embedding_path=? WHERE id=?\", updates)\n",
    "            conn.commit()\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"{tbl}: ok={ok:,}, fixed={fixed:,}, missing={missing:,}, total={total:,}\")\n",
    "    return ok, fixed, missing\n",
    "\n",
    "def main():\n",
    "    print(\"Indexiere Embeddings im Ordner …\")\n",
    "    idx = index_embeddings(EMBEDDING_DIR)\n",
    "    print(f\"Gefundene Embeddings: {len(idx):,}\")\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cur = conn.cursor()\n",
    "    tables = get_tables(cur)\n",
    "    if not tables:\n",
    "        print(\"Keine Tabellen gefunden.\")\n",
    "        return\n",
    "\n",
    "    # Spalten-Check\n",
    "    for t in tables:\n",
    "        for c in (\"filename\", \"path\", \"embedding_path\"):\n",
    "            if not col_exists(cur, t, c):\n",
    "                print(f\"[WARN] {t}: Spalte {c} fehlt – übersprungen.\")\n",
    "                tables.remove(t)\n",
    "                break\n",
    "\n",
    "    grand_ok = grand_fixed = grand_missing = 0\n",
    "    with open(MISSING_TXT, \"w\", encoding=\"utf-8\") as mf:\n",
    "        mf.write(\"# id; table; filename; image_path\\n\")\n",
    "        for tbl in tables:\n",
    "            ok, fixed, missing = process_table(conn, tbl, idx, mf)\n",
    "            grand_ok += ok\n",
    "            grand_fixed += fixed\n",
    "            grand_missing += missing\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\n=== Zusammenfassung ===\")\n",
    "    print(f\"OK:       {grand_ok:,}\")\n",
    "    print(f\"Fixed:    {grand_fixed:,}\")\n",
    "    print(f\"Missing:  {grand_missing:,}\")\n",
    "    print(f\"Fehlende Bildpfade in: {MISSING_TXT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
