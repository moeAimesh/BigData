{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebbfe77",
   "metadata": {},
   "source": [
    "# Image Feature Loader & DB-Saver\n",
    "\n",
    "Dieses Notebook lädt Bilder, extrahiert Features und speichert sie in einer SQLite-Datenbank & nötige DS Steps Ausgeführt um zu analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8193981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, time, traceback, sqlite3, cProfile, pstats, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import umap\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "\n",
    "    PIL_AVAILABLE = True\n",
    "except Exception:\n",
    "    PIL_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61df86",
   "metadata": {},
   "source": [
    "# FIRST RUN TO EXTRACT SMALL IMAGES AND MARK BIG IMAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549cfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verarbeitung: 100%|██████████| 371202/371202 [3:58:39<00:00, 60.04Bild/s]   z:\\CODING\\UNI\\BIG_DATA\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "Verarbeitung: 100%|██████████| 371202/371202 [4:29:28<00:00, 22.96Bild/s]\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIG ==========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "PHOTO_FOLDER = r\"D:\\data\\image_data\"\n",
    "EMBEDDING_DIR = r\"C:\\BIG_DATA\\embeddings\"\n",
    "EMBEDDING_PCA_DIR = r\"C:\\BIG_DATA\\embeddings_pca\"\n",
    "\n",
    "# Logging / Outputs\n",
    "LOG_FILE = \"verarbeitung_log.txt\"\n",
    "SKIPPED_LOG = \"skipped_and_errors.txt\"  # fehlerhafte / nicht verarbeitete Bilder\n",
    "DEFERRED_CSV = \"Large_images.csv\"  # deferred wegen Größe/Pixeln\n",
    "\n",
    "# Sharding\n",
    "IMAGES_PER_TABLE = 50000\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "\n",
    "# Preprocess/Embedding\n",
    "TARGET_SIZE = (224, 224)  # (W,H) für PIL\n",
    "EMBED_INPUT_DTYPE = \"float32\"  # \"float32\" (0..1) oder \"uint8\"\n",
    "MICRO_BATCH = 32  # Embedding-Microbatch\n",
    "MAX_MP = 8  # >8 Megapixel => defer (nur Header lesen)\n",
    "MAX_IMAGE_SIZE_BEFORE_REDUCE = 4 * 1024 * 1024  # 4 MB Datei-Threshold\n",
    "\n",
    "# Pixel-Budget (statisch ODER dynamisch aktivieren)\n",
    "INITIAL_BATCH_SIZE = 500\n",
    "PIXEL_BUDGET = INITIAL_BATCH_SIZE * TARGET_SIZE[0] * TARGET_SIZE[1]  # statisch\n",
    "AUTO_PIXEL_BUDGET_FRACTION = None  # z.B. 0.30 für 30% des freien RAMs; None = aus\n",
    "\n",
    "# Dirs\n",
    "Path(EMBEDDING_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(EMBEDDING_PCA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ========== DB ==========\n",
    "conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA journal_mode=OFF;\")\n",
    "cursor.execute(\"PRAGMA synchronous=OFF;\")\n",
    "cursor.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "cursor.execute(\"PRAGMA mmap_size=0;\")  # vermeidet aufgeblähten \"sichtbaren\" Speicher\n",
    "\n",
    "\n",
    "# ========== Logging/Helfer ==========\n",
    "def log_debug(logfile, msg):\n",
    "    logfile.write(msg + \"\\n\")\n",
    "\n",
    "\n",
    "def log_skip_or_error(filename, path, reason):\n",
    "    with open(SKIPPED_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{datetime.now()} | {filename} | {path} | {reason}\\n\")\n",
    "\n",
    "\n",
    "def mark_deferred_large(filename, path, reason=\"LARGE_IMAGE_DEFERRED\"):\n",
    "    with open(DEFERRED_CSV, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{datetime.now()},{filename},{path},{reason}\\n\")\n",
    "\n",
    "\n",
    "def print_resource_usage(stage, logfile):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / (1024**2)\n",
    "    cpu = process.cpu_percent(interval=0.1)\n",
    "    log_debug(logfile, f\"[RESOURCE] {stage} | RAM: {mem:.2f} MB | CPU: {cpu:.2f}%\")\n",
    "\n",
    "\n",
    "def is_large_by_pixels(path, max_mp=MAX_MP):\n",
    "    if not PIL_AVAILABLE:\n",
    "        return False\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            w, h = im.size\n",
    "        return (w * h) > max_mp * 1_000_000\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def should_defer(path):\n",
    "    try:\n",
    "        if os.path.getsize(path) > MAX_IMAGE_SIZE_BEFORE_REDUCE:\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return is_large_by_pixels(path)\n",
    "\n",
    "\n",
    "def compute_pixel_budget_from_ram():\n",
    "    # Dynamisch: z.B. 30% des freien RAMs in Pixel umrechnen\n",
    "    bytes_per_val = 4 if EMBED_INPUT_DTYPE == \"float32\" else 1\n",
    "    channels = 3\n",
    "    avail_bytes = psutil.virtual_memory().available\n",
    "    target_frac = AUTO_PIXEL_BUDGET_FRACTION\n",
    "    target_bytes = int(avail_bytes * target_frac)\n",
    "    return target_bytes // (channels * bytes_per_val)\n",
    "\n",
    "\n",
    "if AUTO_PIXEL_BUDGET_FRACTION:\n",
    "    PIXEL_BUDGET = compute_pixel_budget_from_ram()\n",
    "\n",
    "\n",
    "def create_table_if_not_exists(table_name):\n",
    "    cursor.execute(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            filename TEXT NOT NULL,\n",
    "            path TEXT NOT NULL,\n",
    "            color_hist TEXT,\n",
    "            embedding_path TEXT,\n",
    "            image_hash TEXT,\n",
    "            resolution TEXT,\n",
    "            file_size INTEGER,\n",
    "            pca_embedding TEXT,\n",
    "            umap_x REAL,\n",
    "            umap_y REAL\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    cursor.execute(\n",
    "        f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_filename_path ON {table_name}(filename, path);\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_batch_to_db(entries, logfile, table_name):\n",
    "    if not entries:\n",
    "        return\n",
    "    log_debug(logfile, f\"[DEBUG] Speichere {len(entries)} Einträge in {table_name}...\")\n",
    "    start = time.time()\n",
    "    cursor.executemany(\n",
    "        f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        (filename, path, color_hist, embedding_path, image_hash, resolution, file_size,\n",
    "         pca_embedding, umap_x, umap_y)\n",
    "        VALUES ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\",\n",
    "        entries,\n",
    "    )\n",
    "    dur = time.time() - start\n",
    "    log_debug(logfile, f\"[DEBUG] DB-Speicherung: {dur:.2f}s\")\n",
    "\n",
    "\n",
    "def preprocess_for_model(img_uint8, target_size=TARGET_SIZE):\n",
    "    # Resize → PIL erwartet (width,height)\n",
    "    if PIL_AVAILABLE:\n",
    "        im = Image.fromarray(img_uint8)\n",
    "        im = im.resize(target_size, Image.BILINEAR)\n",
    "        arr = np.asarray(im)\n",
    "    else:\n",
    "        th, tw = target_size[1], target_size[0]\n",
    "        y_idx = (np.linspace(0, img_uint8.shape[0] - 1, th)).astype(np.int32)\n",
    "        x_idx = (np.linspace(0, img_uint8.shape[1] - 1, tw)).astype(np.int32)\n",
    "        arr = img_uint8[np.ix_(y_idx, x_idx)]\n",
    "    if EMBED_INPUT_DTYPE == \"float32\":\n",
    "        return arr.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        return arr.astype(np.uint8)\n",
    "\n",
    "\n",
    "def to_embed_dtype(arr):\n",
    "    if EMBED_INPUT_DTYPE == \"float32\":\n",
    "        return arr.astype(np.float32) if arr.dtype != np.float32 else arr\n",
    "    else:\n",
    "        if arr.dtype == np.float32:\n",
    "            return (np.clip(arr, 0.0, 1.0) * 255.0).round().astype(np.uint8)\n",
    "        return arr.astype(np.uint8)\n",
    "\n",
    "\n",
    "def extract_embeddings_micro(bimgs, chunk=MICRO_BATCH):\n",
    "    outs = []\n",
    "    for i in range(0, len(bimgs), chunk):\n",
    "        outs.append(extract_embeddings(bimgs[i : i + chunk]))\n",
    "    return np.vstack(outs)\n",
    "\n",
    "\n",
    "def _unique_npy_path(base_path):\n",
    "    base, ext = os.path.splitext(base_path)\n",
    "    if ext.lower() != \".npy\":\n",
    "        base_path = base + \".npy\"\n",
    "        base, ext = os.path.splitext(base_path)\n",
    "    out = base_path\n",
    "    c = 1\n",
    "    while os.path.exists(out):\n",
    "        out = f\"{base}_{c}.npy\"\n",
    "        c += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "# ========== Feature-Vorbereitung ==========\n",
    "def prepare_image_features(filename, path, logfile):\n",
    "    \"\"\"\n",
    "    Defer (Datei/Pixel groß) -> CSV + skip (len==8 Rückgabe).\n",
    "    Sonst: fast_load (uint8), Hash auf Original, Preprocess (klein) für Histogramm+Embedding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if should_defer(path):\n",
    "            log_debug(logfile, f\"[INFO] Deferred (gross/pixelreich): {filename}\")\n",
    "            mark_deferred_large(filename, path, \"LARGE_BY_FILE_OR_PIXELS\")\n",
    "            return (\n",
    "                filename,\n",
    "                path,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                \"LARGE_IMAGE_DEFERRED\",\n",
    "            )\n",
    "\n",
    "        img = fast_load(path)  # deine Loader-Funktion; Erwartung: uint8 ndarray\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            raise TypeError(\"fast_load() must return a NumPy uint8 array.\")\n",
    "        if img.dtype != np.uint8:\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "        resolution = f\"{img.shape[1]}x{img.shape[0]}\"\n",
    "        file_size = os.path.getsize(path)\n",
    "\n",
    "        # Hash auf Original (robust gg. dtype)\n",
    "        img_hash = calc_hash(img)\n",
    "\n",
    "        # RAM-schonend: nur verkleinertes Bild im Batch halten\n",
    "        img_small = preprocess_for_model(img)  # (224,224,3)\n",
    "        embed_input = to_embed_dtype(img_small)  # zu erwartetem dtype\n",
    "\n",
    "        color_hist = calc_histogram(img_small, bins=32)\n",
    "        # Früh stringifizieren (klein halten)\n",
    "        hist_str = \",\".join(str(round(float(v), 6)) for v in np.ravel(color_hist))\n",
    "\n",
    "        del img\n",
    "        return (filename, path, embed_input, hist_str, img_hash, resolution, file_size)\n",
    "\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        reason = f\"{e} | Traceback:\\n{tb}\"\n",
    "        log_debug(logfile, f\"[ERROR] Fehler bei {filename}: {e}\")\n",
    "        log_skip_or_error(filename, path, reason)\n",
    "        return (filename, path, None, None, None, None, None, reason)\n",
    "\n",
    "\n",
    "# ========== Globales Finalisieren über alle Shards ==========\n",
    "def finalize_all_shards_global(\n",
    "    logfile,\n",
    "    sample_size: int = 100_000,\n",
    "    pca_components: int = 100,\n",
    "    chunk: int = 2048,\n",
    "):\n",
    "    \"\"\"\n",
    "    Globales Finalisieren über ALLE Tabellen (einmalig am Ende):\n",
    "    1) IPCA PASS 1: partial_fit auf allen Embeddings (gestreamt)\n",
    "    2) IPCA PASS 2: transform, PCA-Vektoren speichern, pca_embedding aktualisieren\n",
    "       (+ Reservoir-Sampling der PCA-Vektoren für UMAP)\n",
    "    3) UMAP global fit (auf Sample) + PASS 3: transform aller PCA-Vektoren, umap_x/y updaten\n",
    "    \"\"\"\n",
    "    log_debug(logfile, \"[DEBUG] === Globales Finalisieren (IPCA+UMAP) startet ===\")\n",
    "\n",
    "    # 0) Alle Shard-Tabellen finden\n",
    "    cursor.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    tables = [r[0] for r in cursor.fetchall()]\n",
    "    if not tables:\n",
    "        log_debug(logfile, \"[WARN] Keine Shard-Tabellen gefunden – Abbruch.\")\n",
    "        return\n",
    "\n",
    "    # Helper\n",
    "    def load_embeddings(paths):\n",
    "        mats, keep_paths = [], []\n",
    "        for p in paths:\n",
    "            try:\n",
    "                mats.append(np.load(p))\n",
    "                keep_paths.append(p)\n",
    "            except Exception as e:\n",
    "                log_skip_or_error(\"UNKNOWN\", p, f\"PCA-Load-Fehler: {e}\")\n",
    "        if not mats:\n",
    "            return None, []\n",
    "        X = np.stack(mats, axis=0).astype(np.float32)\n",
    "        return X, keep_paths\n",
    "\n",
    "    # PASS 1: IncrementalPCA partial_fit\n",
    "    ipca = None\n",
    "    total_vectors = 0\n",
    "    for tbl in tables:\n",
    "        cursor.execute(f\"SELECT id, embedding_path FROM {tbl}\")\n",
    "        batch_ids, batch_paths = [], []\n",
    "        while True:\n",
    "            row = cursor.fetchone()\n",
    "            if row is None:\n",
    "                if batch_paths:\n",
    "                    X, kept = load_embeddings(batch_paths)\n",
    "                    if X is not None:\n",
    "                        if ipca is None:\n",
    "                            nfeat = X.shape[1]\n",
    "                            ncomp = min(pca_components, nfeat)\n",
    "                            ipca = IncrementalPCA(n_components=ncomp)\n",
    "                            log_debug(\n",
    "                                logfile,\n",
    "                                f\"[DEBUG] IPCA init mit n_components={ncomp} (feat={nfeat})\",\n",
    "                            )\n",
    "                        ipca.partial_fit(X)\n",
    "                        total_vectors += X.shape[0]\n",
    "                        del X\n",
    "                    batch_ids.clear()\n",
    "                    batch_paths.clear()\n",
    "                    gc.collect()\n",
    "                break\n",
    "\n",
    "            _id, epath = row\n",
    "            if epath:\n",
    "                batch_ids.append(_id)\n",
    "                batch_paths.append(epath)\n",
    "                if len(batch_paths) >= chunk:\n",
    "                    X, kept = load_embeddings(batch_paths)\n",
    "                    if X is not None:\n",
    "                        if ipca is None:\n",
    "                            nfeat = X.shape[1]\n",
    "                            ncomp = min(pca_components, nfeat)\n",
    "                            ipca = IncrementalPCA(n_components=ncomp)\n",
    "                            log_debug(\n",
    "                                logfile,\n",
    "                                f\"[DEBUG] IPCA init mit n_components={ncomp} (feat={nfeat})\",\n",
    "                            )\n",
    "                        ipca.partial_fit(X)\n",
    "                        total_vectors += X.shape[0]\n",
    "                        del X\n",
    "                    batch_ids.clear()\n",
    "                    batch_paths.clear()\n",
    "                    gc.collect()\n",
    "\n",
    "    if ipca is None or total_vectors == 0:\n",
    "        log_debug(\n",
    "            logfile,\n",
    "            \"[WARN] Keine Embeddings gefunden – IPCA konnte nicht trainiert werden.\",\n",
    "        )\n",
    "        return\n",
    "\n",
    "    log_debug(\n",
    "        logfile, f\"[DEBUG] PASS 1 fertig – partial_fit auf {total_vectors} Vektoren.\"\n",
    "    )\n",
    "    print_resource_usage(\"Nach IPCA PASS 1\", logfile)\n",
    "\n",
    "    # PASS 2: transform & PCA speichern + Sample sammeln\n",
    "    rng = random.Random(42)\n",
    "    sample_Xp = []\n",
    "    sample_cnt = 0\n",
    "    total_pca_saved = 0\n",
    "\n",
    "    for tbl in tables:\n",
    "        cursor.execute(f\"SELECT id, embedding_path FROM {tbl}\")\n",
    "        batch_ids, batch_paths = [], []\n",
    "        while True:\n",
    "            row = cursor.fetchone()\n",
    "            if row is None:\n",
    "                if batch_paths:\n",
    "                    X, kept_paths = load_embeddings(batch_paths)\n",
    "                    if X is not None:\n",
    "                        Xp = ipca.transform(X)\n",
    "                        updates = []\n",
    "                        for (_id, _path), vec in zip(batch_ids, Xp):\n",
    "                            pca_path = _unique_npy_path(\n",
    "                                os.path.join(EMBEDDING_PCA_DIR, f\"{_id}_pca.npy\")\n",
    "                            )\n",
    "                            try:\n",
    "                                np.save(pca_path, vec.astype(np.float32))\n",
    "                            except Exception as e:\n",
    "                                log_skip_or_error(\n",
    "                                    \"UNKNOWN\", f\"DB_ID={_id}\", f\"PCA-Save-Fehler: {e}\"\n",
    "                                )\n",
    "                                pca_path = \"\"\n",
    "                            updates.append((pca_path, _id))\n",
    "                            # Reservoir-Sampling für UMAP\n",
    "                            sample_cnt += 1\n",
    "                            if len(sample_Xp) < sample_size:\n",
    "                                sample_Xp.append(vec.copy())\n",
    "                            else:\n",
    "                                j = rng.randrange(sample_cnt)\n",
    "                                if j < sample_size:\n",
    "                                    sample_Xp[j] = vec.copy()\n",
    "                        cursor.executemany(\n",
    "                            f\"UPDATE {tbl} SET pca_embedding=? WHERE id=?\", updates\n",
    "                        )\n",
    "                        conn.commit()\n",
    "                        total_pca_saved += len(updates)\n",
    "                        del X, Xp, updates\n",
    "                    batch_ids.clear()\n",
    "                    batch_paths.clear()\n",
    "                    gc.collect()\n",
    "                break\n",
    "\n",
    "            _id, epath = row\n",
    "            if epath:\n",
    "                batch_ids.append((_id, epath))\n",
    "                batch_paths.append(epath)\n",
    "                if len(batch_paths) >= chunk:\n",
    "                    X, kept_paths = load_embeddings(batch_paths)\n",
    "                    if X is not None:\n",
    "                        Xp = ipca.transform(X)\n",
    "                        updates = []\n",
    "                        for (_id2, _path2), vec in zip(batch_ids, Xp):\n",
    "                            pca_path = _unique_npy_path(\n",
    "                                os.path.join(EMBEDDING_PCA_DIR, f\"{_id2}_pca.npy\")\n",
    "                            )\n",
    "                            try:\n",
    "                                np.save(pca_path, vec.astype(np.float32))\n",
    "                            except Exception as e:\n",
    "                                log_skip_or_error(\n",
    "                                    \"UNKNOWN\", f\"DB_ID={_id2}\", f\"PCA-Save-Fehler: {e}\"\n",
    "                                )\n",
    "                                pca_path = \"\"\n",
    "                            updates.append((pca_path, _id2))\n",
    "                            sample_cnt += 1\n",
    "                            if len(sample_Xp) < sample_size:\n",
    "                                sample_Xp.append(vec.copy())\n",
    "                            else:\n",
    "                                j = rng.randrange(sample_cnt)\n",
    "                                if j < sample_size:\n",
    "                                    sample_Xp[j] = vec.copy()\n",
    "                        cursor.executemany(\n",
    "                            f\"UPDATE {tbl} SET pca_embedding=? WHERE id=?\", updates\n",
    "                        )\n",
    "                        conn.commit()\n",
    "                        total_pca_saved += len(updates)\n",
    "                        del X, Xp, updates\n",
    "                    batch_ids.clear()\n",
    "                    batch_paths.clear()\n",
    "                    gc.collect()\n",
    "\n",
    "    log_debug(\n",
    "        logfile, f\"[DEBUG] PASS 2 fertig – {total_pca_saved} PCA-Vektoren gespeichert.\"\n",
    "    )\n",
    "    print_resource_usage(\"Nach IPCA PASS 2\", logfile)\n",
    "\n",
    "    if not sample_Xp:\n",
    "        log_debug(logfile, \"[WARN] Kein PCA-Sample vorhanden – UMAP wird übersprungen.\")\n",
    "        return\n",
    "\n",
    "    # UMAP fit (auf Sample)\n",
    "    sample_mat = np.vstack(sample_Xp).astype(np.float32)\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=2, metric=\"euclidean\", random_state=42, low_memory=True\n",
    "    )\n",
    "    log_debug(\n",
    "        logfile, f\"[DEBUG] UMAP Fit auf Sample mit {sample_mat.shape[0]} Vektoren …\"\n",
    "    )\n",
    "    reducer.fit(sample_mat)\n",
    "    del sample_mat, sample_Xp\n",
    "    gc.collect()\n",
    "    print_resource_usage(\"Nach UMAP Fit (Sample)\", logfile)\n",
    "\n",
    "    # PASS 3: UMAP transform aller PCA-Vektoren & DB-Update\n",
    "    total_umap_updated = 0\n",
    "    for tbl in tables:\n",
    "        cursor.execute(f\"SELECT id, pca_embedding FROM {tbl}\")\n",
    "        batch_ids, pca_paths = [], []\n",
    "        while True:\n",
    "            row = cursor.fetchone()\n",
    "            if row is None:\n",
    "                if pca_paths:\n",
    "                    mats, ids_keep = [], []\n",
    "                    for _id, ppath in batch_ids:\n",
    "                        try:\n",
    "                            mats.append(np.load(ppath).astype(np.float32))\n",
    "                            ids_keep.append(_id)\n",
    "                        except Exception as e:\n",
    "                            log_skip_or_error(\n",
    "                                \"UNKNOWN\", ppath, f\"UMAP-PCA-Load-Fehler: {e}\"\n",
    "                            )\n",
    "                    if mats:\n",
    "                        Xp = np.stack(mats, axis=0)\n",
    "                        coords = reducer.transform(Xp)\n",
    "                        updates = [\n",
    "                            (float(x), float(y), _id)\n",
    "                            for _id, (x, y) in zip(ids_keep, coords)\n",
    "                        ]\n",
    "                        cursor.executemany(\n",
    "                            f\"UPDATE {tbl} SET umap_x=?, umap_y=? WHERE id=?\", updates\n",
    "                        )\n",
    "                        conn.commit()\n",
    "                        total_umap_updated += len(updates)\n",
    "                        del Xp, coords, updates, mats\n",
    "                    batch_ids.clear()\n",
    "                    pca_paths.clear()\n",
    "                    gc.collect()\n",
    "                break\n",
    "\n",
    "            _id, ppath = row\n",
    "            if ppath:\n",
    "                batch_ids.append((_id, ppath))\n",
    "                pca_paths.append(ppath)\n",
    "                if len(pca_paths) >= chunk:\n",
    "                    mats, ids_keep = [], []\n",
    "                    for _id2, ppath2 in batch_ids:\n",
    "                        try:\n",
    "                            mats.append(np.load(ppath2).astype(np.float32))\n",
    "                            ids_keep.append(_id2)\n",
    "                        except Exception as e:\n",
    "                            log_skip_or_error(\n",
    "                                \"UNKNOWN\", ppath2, f\"UMAP-PCA-Load-Fehler: {e}\"\n",
    "                            )\n",
    "                    if mats:\n",
    "                        Xp = np.stack(mats, axis=0)\n",
    "                        coords = reducer.transform(Xp)\n",
    "                        updates = [\n",
    "                            (float(x), float(y), _id2)\n",
    "                            for _id2, (x, y) in zip(ids_keep, coords)\n",
    "                        ]\n",
    "                        cursor.executemany(\n",
    "                            f\"UPDATE {tbl} SET umap_x=?, umap_y=? WHERE id=?\", updates\n",
    "                        )\n",
    "                        conn.commit()\n",
    "                        total_umap_updated += len(updates)\n",
    "                        del Xp, coords, updates, mats\n",
    "                    batch_ids.clear()\n",
    "                    pca_paths.clear()\n",
    "                    gc.collect()\n",
    "\n",
    "    log_debug(\n",
    "        logfile,\n",
    "        f\"[DEBUG] PASS 3 fertig – {total_umap_updated} UMAP-Koordinaten aktualisiert.\",\n",
    "    )\n",
    "    print_resource_usage(\"Nach UMAP PASS 3\", logfile)\n",
    "    log_debug(\n",
    "        logfile, \"[DEBUG] === Globales Finalisieren (IPCA+UMAP) abgeschlossen ===\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ========== Hauptpipeline (Streaming + PIXEL_BUDGET) ==========\n",
    "def main():\n",
    "    logfile = open(LOG_FILE, \"a\", encoding=\"utf-8\")\n",
    "    # Logs frisch beginnen\n",
    "    for f in (SKIPPED_LOG, DEFERRED_CSV):\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "\n",
    "    print_resource_usage(\"Start\", logfile)\n",
    "    logfile.write(f\"[{datetime.now()}] [DEBUG] Start Hauptfunktion\\n\")\n",
    "\n",
    "    # Warm-up\n",
    "    dummy_img = np.zeros((10, 10, 3), dtype=np.uint8)\n",
    "    _ = calc_hash(dummy_img)\n",
    "    _ = calc_histogram(dummy_img)\n",
    "    log_debug(logfile, \"[DEBUG] Warm-up abgeschlossen.\")\n",
    "\n",
    "    # Zählen ohne Materialisierung\n",
    "    total_images = sum(1 for _ in image_generator(PHOTO_FOLDER))\n",
    "    log_debug(logfile, f\"[DEBUG] {total_images} Bilder gefunden\")\n",
    "\n",
    "    # Initiale Tabelle\n",
    "    table_id = 1\n",
    "    current_table_name = f\"{TABLE_PREFIX}{table_id}\"\n",
    "    create_table_if_not_exists(current_table_name)\n",
    "\n",
    "    pbar = tqdm(total=total_images, desc=\"Verarbeitung\", unit=\"Bild\")\n",
    "\n",
    "    gen = image_generator(PHOTO_FOLDER)\n",
    "    total_inserted = 0\n",
    "\n",
    "    # Laufender Batch (nur vorverarbeitete Bilder)\n",
    "    batch_meta, batch_imgs = [], []\n",
    "    cur_pixels = 0  # Summe der (H*W) der preprocessed Bilder im RAM\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            filename, path = next(gen)\n",
    "        except StopIteration:\n",
    "            # Ende: evtl. Rest-Flush\n",
    "            if batch_imgs:\n",
    "                kept = flush_by_embeddings_and_insert(\n",
    "                    batch_meta, batch_imgs, logfile, current_table_name\n",
    "                )\n",
    "                total_inserted += kept\n",
    "                batch_meta.clear()\n",
    "                batch_imgs.clear()\n",
    "                cur_pixels = 0\n",
    "                gc.collect()\n",
    "            break\n",
    "\n",
    "        # Existenz-Check (index-gestützt) – nach (filename,path)\n",
    "        cursor.execute(\n",
    "            f\"SELECT 1 FROM {current_table_name} WHERE filename=? AND path=? LIMIT 1\",\n",
    "            (filename, path),\n",
    "        )\n",
    "        if cursor.fetchone():\n",
    "            log_debug(logfile, f\"[DEBUG] Übersprungen (bereits in DB): {filename}\")\n",
    "            log_skip_or_error(filename, path, \"Bereits in DB\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        # Features vorbereiten\n",
    "        result = prepare_image_features(filename, path, logfile)\n",
    "        if len(result) == 8:\n",
    "            # deferred oder Fehler\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        fname, p, emb_input, hist_str, img_hash, resolution, size = result\n",
    "        batch_meta.append((fname, p, hist_str, img_hash, resolution, size))\n",
    "        batch_imgs.append(emb_input)\n",
    "        cur_pixels += emb_input.shape[0] * emb_input.shape[1]\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Pixel-Budget erreicht? → Flush\n",
    "        if cur_pixels >= PIXEL_BUDGET:\n",
    "            kept = flush_by_embeddings_and_insert(\n",
    "                batch_meta, batch_imgs, logfile, current_table_name\n",
    "            )\n",
    "            total_inserted += kept\n",
    "\n",
    "            # Cleanup für nächsten Mini-Batch\n",
    "            batch_meta.clear()\n",
    "            batch_imgs.clear()\n",
    "            cur_pixels = 0\n",
    "            gc.collect()\n",
    "            try:\n",
    "                import torch\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    log_debug(logfile, \"[DEBUG] CUDA Cache geleert.\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            print_resource_usage(\"Nach PixelBudget-Flush\", logfile)\n",
    "\n",
    "            # Sharding: neue Tabelle, falls Grenze erreicht\n",
    "            if total_inserted >= table_id * IMAGES_PER_TABLE:\n",
    "                table_id += 1\n",
    "                current_table_name = f\"{TABLE_PREFIX}{table_id}\"\n",
    "                create_table_if_not_exists(current_table_name)\n",
    "                log_debug(\n",
    "                    logfile, f\"[DEBUG] Neue Tabelle angelegt: {current_table_name}\"\n",
    "                )\n",
    "\n",
    "    # === Globales Finalisieren über ALLE Tabellen ===\n",
    "    finalize_all_shards_global(\n",
    "        logfile,\n",
    "        sample_size=100_000,  # ggf. anpassen (RAM/Tempo)\n",
    "        pca_components=100,\n",
    "        chunk=2048,\n",
    "    )\n",
    "\n",
    "    pbar.close()\n",
    "    logfile.write(\n",
    "        f\"[{datetime.now()}] ✓ Verarbeitung abgeschlossen. Insgesamt eingefügt: {total_inserted}\\n\"\n",
    "    )\n",
    "    print_resource_usage(\"Ende\", logfile)\n",
    "    logfile.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def flush_by_embeddings_and_insert(batch_meta, batch_imgs, logfile, table_name):\n",
    "    \"\"\"Embeddings (micro-batched) berechnen, speichern, DB-Insert; Rückgabe: #eingefügt.\"\"\"\n",
    "    if not batch_imgs:\n",
    "        return 0\n",
    "\n",
    "    # Embeddings\n",
    "    try:\n",
    "        embs = extract_embeddings_micro(batch_imgs, chunk=MICRO_BATCH)\n",
    "    except Exception as e:\n",
    "        log_debug(logfile, f\"[ERROR] Embedding-Batch fehlgeschlagen: {e}\")\n",
    "        embs = []\n",
    "        for (filename, path, hist_str, img_hash, resolution, size), img_small in zip(\n",
    "            batch_meta, batch_imgs\n",
    "        ):\n",
    "            try:\n",
    "                embs.append(extract_embeddings([img_small])[0])\n",
    "            except Exception as ee:\n",
    "                tb = traceback.format_exc()\n",
    "                log_skip_or_error(\n",
    "                    filename, path, f\"Embedding-Fehler (single): {ee}\\n{tb}\"\n",
    "                )\n",
    "        if not embs:\n",
    "            return 0\n",
    "        embs = np.stack(embs, axis=0)\n",
    "\n",
    "    # Speichern + DB-Insert\n",
    "    entries = []\n",
    "    kept = 0\n",
    "    for (filename, path, hist_str, img_hash, resolution, size), emb in zip(\n",
    "        batch_meta, embs\n",
    "    ):\n",
    "        try:\n",
    "            emb_path = _unique_npy_path(os.path.join(EMBEDDING_DIR, f\"{filename}.npy\"))\n",
    "            np.save(emb_path, emb.astype(np.float32))\n",
    "            entries.append(\n",
    "                (\n",
    "                    filename,\n",
    "                    path,\n",
    "                    hist_str,\n",
    "                    emb_path,\n",
    "                    img_hash,\n",
    "                    resolution,\n",
    "                    size,\n",
    "                    \"\",\n",
    "                    0.0,\n",
    "                    0.0,\n",
    "                )\n",
    "            )\n",
    "            kept += 1\n",
    "        except Exception as e_save:\n",
    "            tb = traceback.format_exc()\n",
    "            log_skip_or_error(filename, path, f\"Embedding-Save-Fehler: {e_save}\\n{tb}\")\n",
    "\n",
    "    save_batch_to_db(entries, logfile, table_name)\n",
    "    del embs, entries\n",
    "    return kept\n",
    "\n",
    "\n",
    "# ========== ENTRYPOINT ==========\n",
    "if __name__ == \"__main__\":\n",
    "    with cProfile.Profile() as pr:\n",
    "        main()\n",
    "\n",
    "    with open(\"profiling_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        stats = pstats.Stats(pr, stream=f)\n",
    "        stats.sort_stats(\"cumtime\").print_stats()\n",
    "    stats.dump_stats(\"profiling_results.prof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9ad70",
   "metadata": {},
   "source": [
    "durch die bilder neu iterierne und mehrer hash funktionen berechnen weil nur eins (average hash) nicht informativ genug ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72004d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_1: 100%|██████████| 100000/100000 [49:14<00:00, 33.85img/s]\n",
      "image_features_part_2: 100%|██████████| 51500/51500 [32:02<00:00, 26.79img/s]\n",
      "image_features_part_3: 100%|██████████| 50000/50000 [43:29<00:00, 19.16img/s]\n",
      "image_features_part_4: 100%|██████████| 50000/50000 [47:35<00:00, 17.51img/s]\n",
      "image_features_part_5: 100%|██████████| 50004/50004 [47:41<00:00, 17.48img/s]\n",
      "image_features_part_6: 100%|██████████| 50000/50000 [4:07:22<00:00,  3.37img/s]  \n",
      "image_features_part_7: 100%|██████████| 8250/8250 [40:01<00:00,  3.43img/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sqlite3\n",
    "import traceback\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========= CONFIG =========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "BATCH_SIZE = 2000  # DB-IDs pro Batch\n",
    "RESIZE_DHASH = 8  # (8+1)x8 -> 64 Bit\n",
    "RESIZE_PHASH = 32  # 32x32 -> DCT -> 8x8 -> 64 Bit\n",
    "LOG_FILE = \"hash_backfill_log.txt\"\n",
    "\n",
    "\n",
    "# ========= Logging =========\n",
    "def log(msg: str) -> None:\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "\n",
    "# ========= Utils =========\n",
    "def _bits_to_hex(bits_bool: np.ndarray) -> str:\n",
    "    \"\"\"Bool-Array -> Hex (64 Bit -> 16 Hex-Zeichen).\"\"\"\n",
    "    packed = np.packbits(bits_bool.astype(np.uint8), bitorder=\"big\")\n",
    "    return packed.tobytes().hex()\n",
    "\n",
    "\n",
    "def _imread_rgb(path: str) -> np.ndarray:\n",
    "    \"\"\"Robustes Laden (auch bei Sonderzeichen in Pfaden).\"\"\"\n",
    "    data = np.fromfile(path, dtype=np.uint8)\n",
    "    img = cv.imdecode(data, cv.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(\"cv.imdecode returned None\")\n",
    "    return cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# ========= Hash-Funktionen =========\n",
    "def dhash_hex(img_rgb: np.ndarray, size: int = RESIZE_DHASH) -> str:\n",
    "    \"\"\"Difference Hash (dHash), 64 Bit -> 16 Hex.\"\"\"\n",
    "    g = cv.cvtColor(img_rgb, cv.COLOR_RGB2GRAY)\n",
    "    g = cv.resize(g, (size + 1, size), interpolation=cv.INTER_AREA)\n",
    "    diff = g[:, 1:] > g[:, :-1]\n",
    "    return _bits_to_hex(diff.reshape(-1))\n",
    "\n",
    "\n",
    "def phash_hex(img_rgb: np.ndarray, full: int = RESIZE_PHASH, keep: int = 8) -> str:\n",
    "    \"\"\"Perceptual Hash (pHash via DCT), 64 Bit -> 16 Hex.\"\"\"\n",
    "    g = cv.cvtColor(img_rgb, cv.COLOR_RGB2GRAY)\n",
    "    g = cv.resize(g, (full, full), interpolation=cv.INTER_AREA).astype(np.float32)\n",
    "    dct = cv.dct(g)\n",
    "    dct_low = dct[:keep, :keep].copy()\n",
    "    dct_low[0, 0] = 0.0\n",
    "    med = np.median(dct_low)\n",
    "    bits = dct_low >= med\n",
    "    return _bits_to_hex(bits.reshape(-1))\n",
    "\n",
    "\n",
    "# ========= DB-Helfer =========\n",
    "def get_shard_tables(cursor):\n",
    "    cursor.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' \"\n",
    "        \"AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cursor.fetchall()]\n",
    "\n",
    "\n",
    "def ensure_columns(conn: sqlite3.Connection, table: str) -> None:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"PRAGMA table_info({table});\")\n",
    "    cols = {row[1] for row in cur.fetchall()}\n",
    "    wanted = {\n",
    "        \"dhash\": \"TEXT\",\n",
    "        \"phash\": \"TEXT\",\n",
    "    }\n",
    "    for col, ctype in wanted.items():\n",
    "        if col not in cols:\n",
    "            cur.execute(f\"ALTER TABLE {table} ADD COLUMN {col} {ctype};\")\n",
    "            log(f\"[INFO] Added column {col} to {table}\")\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def count_pending(cur: sqlite3.Cursor, table: str) -> int:\n",
    "    \"\"\"Wie viele Zeilen fehlen (mind. einer der Hashes leer)?\"\"\"\n",
    "    cur.execute(\n",
    "        f\"\"\"\n",
    "        SELECT COUNT(*) FROM {table}\n",
    "        WHERE (dhash IS NULL OR dhash = '')\n",
    "           OR (phash IS NULL OR phash = '')\n",
    "        \"\"\"\n",
    "    )\n",
    "    return int(cur.fetchone()[0])\n",
    "\n",
    "\n",
    "def fetch_ids_and_paths_needing_hashes(cur: sqlite3.Cursor, table: str, limit: int):\n",
    "    cur.execute(\n",
    "        f\"\"\"\n",
    "        SELECT id, path FROM {table}\n",
    "        WHERE (dhash IS NULL OR dhash = '')\n",
    "           OR (phash IS NULL OR phash = '')\n",
    "        LIMIT ?\n",
    "        \"\"\",\n",
    "        (limit,),\n",
    "    )\n",
    "    return cur.fetchall()\n",
    "\n",
    "\n",
    "def update_hashes(conn: sqlite3.Connection, table: str, rows) -> int:\n",
    "    \"\"\"Berechnet Hashes für rows und schreibt sie. Rückgabe: #erfolgreich aktualisiert.\"\"\"\n",
    "    cur = conn.cursor()\n",
    "    updates = []\n",
    "    for _id, path in rows:\n",
    "        try:\n",
    "            img = _imread_rgb(path)\n",
    "            d = dhash_hex(img)\n",
    "            p = phash_hex(img)\n",
    "            updates.append((d, p, _id))\n",
    "        except Exception as e:\n",
    "            log(\n",
    "                f\"[ERROR] {table} id={_id} path='{path}': {e}\\n{traceback.format_exc()}\"\n",
    "            )\n",
    "    if updates:\n",
    "        cur.executemany(\n",
    "            f\"UPDATE {table} SET dhash=?, phash=? WHERE id=?;\",\n",
    "            updates,\n",
    "        )\n",
    "        conn.commit()\n",
    "    return len(updates)\n",
    "\n",
    "\n",
    "# (Optional) Indizes\n",
    "def ensure_indexes(conn: sqlite3.Connection, table: str) -> None:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table}_dhash ON {table}(dhash);\")\n",
    "    cur.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table}_phash ON {table}(phash);\")\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# ========= Main =========\n",
    "def backfill_all_tables() -> None:\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)  # autocommit\n",
    "    cur = conn.cursor()\n",
    "    tables = get_shard_tables(cur)\n",
    "    if not tables:\n",
    "        log(\"[WARN] Keine Shard-Tabellen gefunden.\")\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    total_updated = 0\n",
    "    for tbl in tables:\n",
    "        ensure_columns(conn, tbl)\n",
    "        pending = count_pending(cur, tbl)\n",
    "\n",
    "        if pending == 0:\n",
    "            log(f\"[INFO] Nichts zu tun für {tbl}\")\n",
    "            continue\n",
    "\n",
    "        with tqdm(total=pending, desc=f\"{tbl}\", unit=\"img\") as pbar:\n",
    "            # ensure_indexes(conn, tbl)  # optional\n",
    "            while True:\n",
    "                rows = fetch_ids_and_paths_needing_hashes(cur, tbl, BATCH_SIZE)\n",
    "                if not rows:\n",
    "                    break\n",
    "                n_ok = update_hashes(conn, tbl, rows)\n",
    "                total_updated += n_ok\n",
    "                pbar.update(n_ok)  # Fortschritt = erfolgreich befüllte Zeilen\n",
    "                gc.collect()\n",
    "\n",
    "        log(f\"[INFO] Fertig für {tbl}\")\n",
    "\n",
    "    log(f\"[OK] Backfill abgeschlossen. Insgesamt aktualisiert: {total_updated}\")\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    backfill_all_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472cbcc",
   "metadata": {},
   "source": [
    "pca und umap spalten leeren (wenn neu rechnen nötig ist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eed083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-14 23:44:27.563880] Cleanup startet …\n",
      "Tabellen: ['image_features_part_1', 'image_features_part_2', 'image_features_part_3', 'image_features_part_4', 'image_features_part_5', 'image_features_part_6', 'image_features_part_7']\n",
      "DRY_RUN=False, DELETE_PCA_FILES=True\n",
      "\n",
      "=== image_features_part_1 ===\n",
      "rows_total : 100000\n",
      "pca_set    : 100000\n",
      "umap_set   : 100000\n",
      "PCA-Dateien gefunden : 100000\n",
      "PCA-Dateien gelöscht : 0\n",
      "DB-Felder geleert: pca_embedding=NULL, umap_x=NULL, umap_y=NULL\n",
      "\n",
      "=== image_features_part_2 ===\n",
      "rows_total : 51500\n",
      "pca_set    : 51500\n",
      "umap_set   : 51500\n",
      "PCA-Dateien gefunden : 51500\n",
      "PCA-Dateien gelöscht : 0\n",
      "DB-Felder geleert: pca_embedding=NULL, umap_x=NULL, umap_y=NULL\n",
      "\n",
      "=== image_features_part_3 ===\n",
      "rows_total : 50000\n",
      "pca_set    : 50000\n",
      "umap_set   : 50000\n",
      "PCA-Dateien gefunden : 50000\n",
      "PCA-Dateien gelöscht : 0\n",
      "DB-Felder geleert: pca_embedding=NULL, umap_x=NULL, umap_y=NULL\n",
      "\n",
      "=== image_features_part_4 ===\n",
      "rows_total : 50000\n",
      "pca_set    : 50000\n",
      "umap_set   : 50000\n",
      "PCA-Dateien gefunden : 50000\n",
      "PCA-Dateien gelöscht : 0\n",
      "DB-Felder geleert: pca_embedding=NULL, umap_x=NULL, umap_y=NULL\n",
      "\n",
      "=== image_features_part_5 ===\n",
      "rows_total : 50004\n",
      "pca_set    : 50004\n",
      "umap_set   : 50004\n",
      "PCA-Dateien gefunden : 50004\n",
      "PCA-Dateien gelöscht : 0\n",
      "DB-Felder geleert: pca_embedding=NULL, umap_x=NULL, umap_y=NULL\n",
      "\n",
      "=== image_features_part_6 ===\n",
      "rows_total : 50000\n",
      "pca_set    : 50000\n",
      "umap_set   : 50000\n",
      "PCA-Dateien gefunden : 50000\n",
      "PCA-Dateien gelöscht : 0\n",
      "DB-Felder geleert: pca_embedding=NULL, umap_x=NULL, umap_y=NULL\n",
      "\n",
      "=== image_features_part_7 ===\n",
      "rows_total : 8250\n",
      "pca_set    : 8250\n",
      "umap_set   : 8250\n",
      "PCA-Dateien gefunden : 8250\n",
      "PCA-Dateien gelöscht : 0\n",
      "DB-Felder geleert: pca_embedding=NULL, umap_x=NULL, umap_y=NULL\n",
      "\n",
      "===== Zusammenfassung =====\n",
      "Tabellen insgesamt      : 7\n",
      "Zeilen gesamt           : 359754\n",
      "PCA gesetzt (DB) gesamt : 359754\n",
      "UMAP gesetzt (DB) gesamt: 359754\n",
      "PCA-Dateien gefunden    : 359754\n",
      "PCA-Dateien gelöscht    : 0\n",
      "\n",
      "[2025-08-14 23:44:51.745442] Cleanup fertig.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ======= CONFIG =======\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "\n",
    "# Wo liegen deine PCA-Dateien (zur optionalen Löschung)?\n",
    "EMBEDDING_PCA_DIR = Path(r\"C:\\BIG_DATA\\embeddings_pca\")\n",
    "\n",
    "# Verhalten\n",
    "DRY_RUN = False  # True = nur zählen/anzeigen, nichts ändern/löschen\n",
    "DELETE_PCA_FILES = (\n",
    "    True  # True = referenzierte PCA-Dateien löschen (nur wenn DRY_RUN=False)\n",
    ")\n",
    "\n",
    "CHUNK = 10000  # Streaminggröße für SELECTs\n",
    "\n",
    "# ======================\n",
    "\n",
    "\n",
    "def get_tables(cur, prefix):\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{prefix}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "\n",
    "def exists_safe(p: str) -> bool:\n",
    "    try:\n",
    "        return os.path.exists(p)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cur = conn.cursor()\n",
    "    cur_read = conn.cursor()  # separater Lese-Cursor (sicherer bei Updates)\n",
    "\n",
    "    tables = get_tables(cur, TABLE_PREFIX)\n",
    "    if not tables:\n",
    "        print(\"Keine Shard-Tabellen gefunden.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[{datetime.now()}] Cleanup startet …\")\n",
    "    print(f\"Tabellen: {tables}\")\n",
    "    print(f\"DRY_RUN={DRY_RUN}, DELETE_PCA_FILES={DELETE_PCA_FILES}\")\n",
    "\n",
    "    total_rows = total_pca_set = total_umap_set = 0\n",
    "    total_files_found = total_files_deleted = 0\n",
    "\n",
    "    for tbl in tables:\n",
    "        print(f\"\\n=== {tbl} ===\")\n",
    "\n",
    "        # Basiscounter\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "        rows_total = cur.fetchone()[0]\n",
    "        total_rows += rows_total\n",
    "\n",
    "        cur.execute(\n",
    "            f\"SELECT COUNT(*) FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding!=''\"\n",
    "        )\n",
    "        pca_set = cur.fetchone()[0]\n",
    "        total_pca_set += pca_set\n",
    "\n",
    "        cur.execute(\n",
    "            f\"\"\"SELECT COUNT(*) FROM {tbl}\n",
    "                        WHERE (umap_x IS NOT NULL AND umap_y IS NOT NULL)\n",
    "                          AND (umap_x!=0.0 OR umap_y!=0.0)\"\"\"\n",
    "        )\n",
    "        umap_set = cur.fetchone()[0]\n",
    "        total_umap_set += umap_set\n",
    "\n",
    "        print(f\"rows_total : {rows_total}\")\n",
    "        print(f\"pca_set    : {pca_set}\")\n",
    "        print(f\"umap_set   : {umap_set}\")\n",
    "\n",
    "        # PCA-Dateien einsammeln (optional löschen)\n",
    "        files_found = files_deleted = 0\n",
    "        if pca_set > 0 and (DELETE_PCA_FILES or DRY_RUN):\n",
    "            cur_read.execute(\n",
    "                f\"SELECT pca_embedding FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding!=''\"\n",
    "            )\n",
    "            while True:\n",
    "                rows = cur_read.fetchmany(CHUNK)\n",
    "                if not rows:\n",
    "                    break\n",
    "                for (ppath,) in rows:\n",
    "                    if not ppath:\n",
    "                        continue\n",
    "                    files_found += 1\n",
    "                    # Aus Sicherheitsgründen nur löschen, wenn Datei im erwarteten Root liegt\n",
    "                    if (not DRY_RUN) and DELETE_PCA_FILES:\n",
    "                        try:\n",
    "                            p = Path(ppath)\n",
    "                            if p.is_file():\n",
    "                                # Safety: innerhalb EMBEDDING_PCA_DIR?\n",
    "                                try:\n",
    "                                    p.relative_to(EMBEDDING_PCA_DIR)\n",
    "                                    os.remove(p)\n",
    "                                    files_deleted += 1\n",
    "                                except ValueError:\n",
    "                                    # liegt außerhalb; zur Sicherheit überspringen\n",
    "                                    print(f\"⚠️  Übersprungen (außerhalb Root): {ppath}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️  Löschen fehlgeschlagen: {ppath} | {e}\")\n",
    "\n",
    "        total_files_found += files_found\n",
    "        total_files_deleted += files_deleted\n",
    "        print(f\"PCA-Dateien gefunden : {files_found}\")\n",
    "        if not DRY_RUN and DELETE_PCA_FILES:\n",
    "            print(f\"PCA-Dateien gelöscht : {files_deleted}\")\n",
    "\n",
    "        # DB-Felder leeren\n",
    "        if DRY_RUN:\n",
    "            print(\"DRY_RUN: DB wird NICHT geändert.\")\n",
    "        else:\n",
    "            # NULL setzen ist sauberer als 0.0/'' (klarer „kein Wert“)\n",
    "            cur.execute(\n",
    "                f\"UPDATE {tbl} SET pca_embedding=NULL WHERE pca_embedding IS NOT NULL AND pca_embedding!=''\"\n",
    "            )\n",
    "            cur.execute(\n",
    "                f\"UPDATE {tbl} SET umap_x=NULL, umap_y=NULL WHERE (umap_x IS NOT NULL) OR (umap_y IS NOT NULL)\"\n",
    "            )\n",
    "            conn.commit()\n",
    "            print(\"DB-Felder geleert: pca_embedding=NULL, umap_x=NULL, umap_y=NULL\")\n",
    "\n",
    "    print(\"\\n===== Zusammenfassung =====\")\n",
    "    print(f\"Tabellen insgesamt      : {len(tables)}\")\n",
    "    print(f\"Zeilen gesamt           : {total_rows}\")\n",
    "    print(f\"PCA gesetzt (DB) gesamt : {total_pca_set}\")\n",
    "    print(f\"UMAP gesetzt (DB) gesamt: {total_umap_set}\")\n",
    "    print(f\"PCA-Dateien gefunden    : {total_files_found}\")\n",
    "    if not DRY_RUN and DELETE_PCA_FILES:\n",
    "        print(f\"PCA-Dateien gelöscht    : {total_files_deleted}\")\n",
    "\n",
    "    print(f\"\\n[{datetime.now()}] Cleanup fertig.\")\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae574a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 10 21:49:22 2025    profiling_results.prof\n",
      "\n",
      "         351036394 function calls (344465332 primitive calls) in 16175.470 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 5633 to 30 due to restriction <30>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    4.103    4.103 16175.799 16175.799 621606085.py:442(main)\n",
      "   321202  103.743    0.000 10380.608    0.032 621606085.py:166(prepare_image_features)\n",
      "   321202    2.113    0.000 5892.825    0.018 621606085.py:70(should_defer)\n",
      "  1017866 3975.147    0.004 3975.309    0.004 {built-in method io.open}\n",
      "   930620 3649.127    0.004 3649.127    0.004 {built-in method nt.stat}\n",
      "   567406    1.545    0.000 3634.637    0.006 <frozen genericpath>:48(getsize)\n",
      "      493    2.629    0.005 3576.141    7.254 621606085.py:548(flush_by_embeddings_and_insert)\n",
      "      493    0.060    0.000 3397.526    6.892 621606085.py:146(extract_embeddings_micro)\n",
      "     7879   13.108    0.002 3397.328    0.431 embedding_vec.py:34(extract_embeddings)\n",
      "590925/7879    2.137    0.000 3174.716    0.403 module.py:1747(_wrapped_call_impl)\n",
      "590925/7879    3.413    0.000 3174.675    0.403 module.py:1755(_call_impl)\n",
      "63032/7879    1.027    0.000 3174.574    0.403 container.py:238(forward)\n",
      "   259152    1.459    0.000 2486.212    0.010 image_load.py:3(fast_load)\n",
      "    63032   64.531    0.001 2471.238    0.039 resnet.py:89(forward)\n",
      "   157580    0.882    0.000 2467.006    0.016 conv.py:553(forward)\n",
      "   157580    0.761    0.000 2465.900    0.016 conv.py:536(_conv_forward)\n",
      "   157580 2465.140    0.016 2465.140    0.016 {built-in method torch.conv2d}\n",
      "   259152 2399.724    0.009 2399.724    0.009 {imread}\n",
      "   318208    2.224    0.000 2321.027    0.007 621606085.py:60(is_large_by_pixels)\n",
      "   318208    4.864    0.000 2308.276    0.007 Image.py:3459(open)\n",
      "        1    0.538    0.538 1857.567 1857.567 621606085.py:209(finalize_all_shards_global)\n",
      "   318184    5.324    0.000 1744.032    0.005 _npyio_impl.py:308(load)\n",
      "      153    1.760    0.012 1694.741   11.077 621606085.py:235(load_embeddings)\n",
      "   246204   47.967    0.000  969.472    0.004 621606085.py:122(preprocess_for_model)\n",
      "   492409    3.648    0.000  802.494    0.002 Image.py:2215(resize)\n",
      "   492408  794.305    0.002  794.305    0.002 {method 'resize' of 'ImagingCore' objects}\n",
      "   246205    2.196    0.000  695.991    0.003 hash.py:6(calc_hash)\n",
      "   492409    7.319    0.000  497.026    0.001 Image.py:3250(fromarray)\n",
      "   492409    1.590    0.000  489.294    0.001 Image.py:3166(frombuffer)\n",
      "   492409    2.839    0.000  486.547    0.001 Image.py:3120(frombytes)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x25d49d3bb50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats\n",
    "\n",
    "stats = pstats.Stats(\"profiling_results.prof\")\n",
    "stats.strip_dirs().sort_stats(\"cumulative\").print_stats(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788e8c6",
   "metadata": {},
   "source": [
    "Vergleichen wir zunächst Anzahl bearbietete bilder mit Anzahl bilder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8015dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dateien im Ordner: 359474\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "photo_dir = Path(r\"C:\\BIG_DATA\\embeddings\")\n",
    "n_files = sum(1 for p in photo_dir.iterdir() if p.is_file())\n",
    "print(f\"Dateien im Ordner: {n_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afabb50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_1: 100000 Einträge\n",
      "image_features_part_2: 51500 Einträge\n",
      "image_features_part_3: 50000 Einträge\n",
      "image_features_part_4: 50000 Einträge\n",
      "image_features_part_5: 50004 Einträge\n",
      "image_features_part_6: 50000 Einträge\n",
      "image_features_part_7: 8250 Einträge\n",
      "\n",
      "Gesamtanzahl Einträge: 359754\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "\n",
    "# 1) Verbindung öffnen\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 2) Alle relevanten Tabellen ermitteln\n",
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    SELECT name\n",
    "      FROM sqlite_master\n",
    "     WHERE type='table'\n",
    "       AND name LIKE 'image_features_part_%'\n",
    "\"\"\"\n",
    ")\n",
    "tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# 3) Zeilen in jeder Tabelle zählen und aufsummieren\n",
    "total_entries = 0\n",
    "for table in tables:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"{table}: {count} Einträge\")\n",
    "    total_entries += count\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\nGesamtanzahl Einträge: {total_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b145a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Bilder: 371202\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PHOTO_FOLDER = r\"D:\\data\\image_data\"\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\n",
    "\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(PHOTO_FOLDER):\n",
    "    for fname in files:\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "        if ext in IMAGE_EXTS:\n",
    "            count += 1\n",
    "\n",
    "print(f\"Gefundene Bilder: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86c2b7",
   "metadata": {},
   "source": [
    "überprüfen wie viele bilder erfolgereich PCA embeddings und UMAP koordinaten bekommen haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6f79e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-14 23:49:26.724628] Audit startet …\n",
      "Gefundene Tabellen: 7 → ['image_features_part_1', 'image_features_part_2', 'image_features_part_3', 'image_features_part_4', 'image_features_part_5', 'image_features_part_6', 'image_features_part_7']\n",
      "[FS] Dateien im EMBEDDING_DIR:   359474\n",
      "[FS] Dateien im EMBEDDING_PCA_DIR: -1\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_1 ===\n",
      "rows_total            : 100000\n",
      "emb_db_set            : 100000\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 100000\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_2 ===\n",
      "rows_total            : 51500\n",
      "emb_db_set            : 51500\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 51500\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_3 ===\n",
      "rows_total            : 50000\n",
      "emb_db_set            : 50000\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 50000\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_4 ===\n",
      "rows_total            : 50000\n",
      "emb_db_set            : 50000\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 50000\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_5 ===\n",
      "rows_total            : 50004\n",
      "emb_db_set            : 50004\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 50004\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_6 ===\n",
      "rows_total            : 50000\n",
      "emb_db_set            : 50000\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 50000\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "=== Prüfe Tabelle: image_features_part_7 ===\n",
      "rows_total            : 8250\n",
      "emb_db_set            : 8250\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 8250\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "===== GESAMT =====\n",
      "rows_total            : 359754\n",
      "emb_db_set            : 359754\n",
      "emb_fs_missing        : 0\n",
      "pca_db_set            : 0\n",
      "pca_db_not_set        : 359754\n",
      "pca_fs_missing        : 0\n",
      "umap_set              : 0\n",
      "umap_missing_for_pca  : 0\n",
      "\n",
      "CSV-Report geschrieben nach: Z:\\CODING\\UNI\\BIG_DATA\\src\\report_embeddings_pca_umap.csv\n",
      "[2025-08-14 23:50:21.446591] Audit fertig.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "EMBEDDING_DIR = Path(r\"C:\\BIG_DATA\\embeddings\")\n",
    "EMBEDDING_PCA_DIR = Path(r\"C:\\BIG_DATA\\embeddings_pca\")\n",
    "\n",
    "WRITE_CSV = True\n",
    "CSV_PATH = Path(\"report_embeddings_pca_umap.csv\")\n",
    "\n",
    "CHUNK = 10000  # DB-Streaminggröße\n",
    "\n",
    "\n",
    "# ========== HELPERS ==========\n",
    "def count_files_in_dir(d: Path) -> int:\n",
    "    try:\n",
    "        return sum(1 for p in d.iterdir() if p.is_file())\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def exists_safe(p: str) -> bool:\n",
    "    try:\n",
    "        return os.path.exists(p)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_tables(cur, prefix):\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{prefix}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "\n",
    "# ========== MAIN AUDIT ==========\n",
    "def main():\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    tables = get_tables(cur, TABLE_PREFIX)\n",
    "    if not tables:\n",
    "        print(\"Keine Shard-Tabellen gefunden.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Gefundene Tabellen: {len(tables)} → {tables}\")\n",
    "\n",
    "    # FS Grob-Check\n",
    "    emb_fs_count = count_files_in_dir(EMBEDDING_DIR)\n",
    "    pca_fs_count = count_files_in_dir(EMBEDDING_PCA_DIR)\n",
    "    print(f\"[FS] Dateien im EMBEDDING_DIR:   {emb_fs_count}\")\n",
    "    print(f\"[FS] Dateien im EMBEDDING_PCA_DIR: {pca_fs_count}\")\n",
    "\n",
    "    # CSV vorbereiten\n",
    "    if WRITE_CSV:\n",
    "        import csv\n",
    "\n",
    "        csvfile = open(CSV_PATH, \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "        writer = csv.writer(csvfile, delimiter=\";\")\n",
    "        writer.writerow(\n",
    "            [\n",
    "                \"table\",\n",
    "                \"rows_total\",\n",
    "                \"emb_db_set\",\n",
    "                \"emb_fs_missing\",\n",
    "                \"pca_db_set\",\n",
    "                \"pca_fs_missing\",\n",
    "                \"pca_db_not_set\",\n",
    "                \"umap_set\",\n",
    "                \"umap_missing_for_pca\",\n",
    "                \"examples_missing_emb_paths\",\n",
    "                \"examples_missing_pca_paths\",\n",
    "                \"examples_pca_set_umap_missing_ids\",\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    # Gesamtsummen\n",
    "    G_rows = G_emb_set = G_pca_set = G_umap_set = 0\n",
    "    G_emb_fs_missing = G_pca_fs_missing = 0\n",
    "    G_pca_not_set = G_umap_missing_for_pca = 0\n",
    "\n",
    "    for tbl in tables:\n",
    "        print(f\"\\n=== Prüfe Tabelle: {tbl} ===\")\n",
    "\n",
    "        # Basiscounter aus DB\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "        rows_total = cur.fetchone()[0]\n",
    "\n",
    "        cur.execute(\n",
    "            f\"SELECT COUNT(*) FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\"\n",
    "        )\n",
    "        emb_db_set = cur.fetchone()[0]\n",
    "\n",
    "        cur.execute(\n",
    "            f\"SELECT COUNT(*) FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\"\n",
    "        )\n",
    "        pca_db_set = cur.fetchone()[0]\n",
    "\n",
    "        cur.execute(\n",
    "            f\"\"\"\n",
    "            SELECT COUNT(*) FROM {tbl}\n",
    "            WHERE (umap_x IS NOT NULL AND umap_y IS NOT NULL) AND (umap_x != 0.0 OR umap_y != 0.0)\n",
    "        \"\"\"\n",
    "        )\n",
    "        umap_set = cur.fetchone()[0]\n",
    "\n",
    "        # Streaming-Check: fehlende Embedding-Dateien\n",
    "        emb_fs_missing = 0\n",
    "        miss_emb_examples = []\n",
    "\n",
    "        cur.execute(\n",
    "            f\"SELECT id, embedding_path FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\"\n",
    "        )\n",
    "        while True:\n",
    "            rows = cur.fetchmany(CHUNK)\n",
    "            if not rows:\n",
    "                break\n",
    "            for _id, epath in rows:\n",
    "                if not exists_safe(epath):\n",
    "                    emb_fs_missing += 1\n",
    "                    if len(miss_emb_examples) < 5:\n",
    "                        miss_emb_examples.append(f\"id={_id}|{epath}\")\n",
    "\n",
    "        # Streaming-Check: fehlende PCA-Dateien\n",
    "        pca_fs_missing = 0\n",
    "        miss_pca_examples = []\n",
    "\n",
    "        cur.execute(\n",
    "            f\"SELECT id, pca_embedding FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\"\n",
    "        )\n",
    "        while True:\n",
    "            rows = cur.fetchmany(CHUNK)\n",
    "            if not rows:\n",
    "                break\n",
    "            for _id, ppath in rows:\n",
    "                if not exists_safe(ppath):\n",
    "                    pca_fs_missing += 1\n",
    "                    if len(miss_pca_examples) < 5:\n",
    "                        miss_pca_examples.append(f\"id={_id}|{ppath}\")\n",
    "\n",
    "        # PCA gesetzt? nein:\n",
    "        pca_db_not_set = rows_total - pca_db_set\n",
    "\n",
    "        # Fälle: PCA gesetzt, aber UMAP fehlt/placeholder\n",
    "        umap_missing_for_pca = 0\n",
    "        pca_umap_missing_examples = []\n",
    "        cur.execute(\n",
    "            f\"\"\"\n",
    "            SELECT id, umap_x, umap_y\n",
    "            FROM {tbl}\n",
    "            WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\n",
    "              AND (umap_x IS NULL OR umap_y IS NULL OR (umap_x = 0.0 AND umap_y = 0.0))\n",
    "        \"\"\"\n",
    "        )\n",
    "        while True:\n",
    "            rows = cur.fetchmany(CHUNK)\n",
    "            if not rows:\n",
    "                break\n",
    "            for _id, x, y in rows:\n",
    "                umap_missing_for_pca += 1\n",
    "                if len(pca_umap_missing_examples) < 5:\n",
    "                    pca_umap_missing_examples.append(f\"id={_id}|umap=({x},{y})\")\n",
    "\n",
    "        # Ausgabe pro Tabelle\n",
    "        print(f\"rows_total            : {rows_total}\")\n",
    "        print(f\"emb_db_set            : {emb_db_set}\")\n",
    "        print(f\"emb_fs_missing        : {emb_fs_missing}\")\n",
    "        print(f\"pca_db_set            : {pca_db_set}\")\n",
    "        print(f\"pca_db_not_set        : {pca_db_not_set}\")\n",
    "        print(f\"pca_fs_missing        : {pca_fs_missing}\")\n",
    "        print(f\"umap_set              : {umap_set}\")\n",
    "        print(f\"umap_missing_for_pca  : {umap_missing_for_pca}\")\n",
    "\n",
    "        if miss_emb_examples:\n",
    "            print(f\"  Beispiele fehlender EMB-Dateien: {miss_emb_examples}\")\n",
    "        if miss_pca_examples:\n",
    "            print(f\"  Beispiele fehlender PCA-Dateien: {miss_pca_examples}\")\n",
    "        if pca_umap_missing_examples:\n",
    "            print(f\"  Beispiele: PCA gesetzt, UMAP fehlt: {pca_umap_missing_examples}\")\n",
    "\n",
    "        # CSV-Zeile\n",
    "        if writer:\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    tbl,\n",
    "                    rows_total,\n",
    "                    emb_db_set,\n",
    "                    emb_fs_missing,\n",
    "                    pca_db_set,\n",
    "                    pca_fs_missing,\n",
    "                    pca_db_not_set,\n",
    "                    umap_set,\n",
    "                    umap_missing_for_pca,\n",
    "                    \" | \".join(miss_emb_examples),\n",
    "                    \" | \".join(miss_pca_examples),\n",
    "                    \" | \".join(pca_umap_missing_examples),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Globalsumme\n",
    "        G_rows += rows_total\n",
    "        G_emb_set += emb_db_set\n",
    "        G_pca_set += pca_db_set\n",
    "        G_umap_set += umap_set\n",
    "        G_emb_fs_missing += emb_fs_missing\n",
    "        G_pca_fs_missing += pca_fs_missing\n",
    "        G_pca_not_set += pca_db_not_set\n",
    "        G_umap_missing_for_pca += umap_missing_for_pca\n",
    "\n",
    "    # Gesamt-Zusammenfassung\n",
    "    print(\"\\n===== GESAMT =====\")\n",
    "    print(f\"rows_total            : {G_rows}\")\n",
    "    print(f\"emb_db_set            : {G_emb_set}\")\n",
    "    print(f\"emb_fs_missing        : {G_emb_fs_missing}\")\n",
    "    print(f\"pca_db_set            : {G_pca_set}\")\n",
    "    print(f\"pca_db_not_set        : {G_pca_not_set}\")\n",
    "    print(f\"pca_fs_missing        : {G_pca_fs_missing}\")\n",
    "    print(f\"umap_set              : {G_umap_set}\")\n",
    "    print(f\"umap_missing_for_pca  : {G_umap_missing_for_pca}\")\n",
    "\n",
    "    if writer:\n",
    "        csvfile.close()\n",
    "        print(f\"\\nCSV-Report geschrieben nach: {CSV_PATH.resolve()}\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"[{datetime.now()}] Audit startet …\")\n",
    "    main()\n",
    "    print(f\"[{datetime.now()}] Audit fertig.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5ad0b",
   "metadata": {},
   "source": [
    "# 2. RUN DURCH GESPEICHERTE LARGE FILES  (KLEINEREN BATCHSIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34ee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deferred-Verarbeitung: 100%|██████████| 62050/62050 [6:08:16<00:00,  2.81Bild/s]    \n"
     ]
    }
   ],
   "source": [
    "import os, gc, csv, time, sqlite3, traceback, cProfile, pstats\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "\n",
    "    PIL_AVAILABLE = True\n",
    "except Exception:\n",
    "    PIL_AVAILABLE = False\n",
    "\n",
    "from features.hash import calc_hash\n",
    "from features.color_vec import calc_histogram\n",
    "from features.embedding_vec import extract_embeddings\n",
    "from image_load import fast_load\n",
    "\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "EMBEDDING_DIR = r\"C:\\BIG_DATA\\embeddings\"\n",
    "DEFERRED_CSV = r\"Z:\\CODING\\UNI\\BIG_DATA\\src\\Large_images.csv\"\n",
    "\n",
    "LOG_FILE = r\"Z:\\CODING\\UNI\\BIG_DATA\\src\\verarbeitung_log_deferred.txt\"\n",
    "SKIPPED_LOG = r\"Z:\\CODING\\UNI\\BIG_DATA\\src\\skipped_and_errors.txt\"\n",
    "\n",
    "# Sharding\n",
    "IMAGES_PER_TABLE = 50_000\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "\n",
    "# Preprocess/Embedding\n",
    "TARGET_SIZE = (224, 224)  # (W,H) für PIL\n",
    "EMBED_INPUT_DTYPE = \"float32\"  # \"float32\" (0..1) oder \"uint8\"\n",
    "MICRO_BATCH = 32  # Embedding-Microbatch\n",
    "\n",
    "# Pixel-Budget: max. Summe der (H*W) der KLEINEN Bilder im RAM\n",
    "BATCH_TARGET = 100  # grobe Ziel-Batchgröße in kleinen Bildern\n",
    "PIXEL_BUDGET = BATCH_TARGET * TARGET_SIZE[0] * TARGET_SIZE[1]\n",
    "\n",
    "# Dirs\n",
    "Path(EMBEDDING_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ========== DB ==========\n",
    "conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA journal_mode=OFF;\")\n",
    "cursor.execute(\"PRAGMA synchronous=OFF;\")\n",
    "cursor.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "cursor.execute(\"PRAGMA mmap_size=0;\")\n",
    "\n",
    "\n",
    "def create_table_if_not_exists(table_name):\n",
    "    cursor.execute(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            filename TEXT NOT NULL,\n",
    "            path TEXT NOT NULL,\n",
    "            color_hist TEXT,\n",
    "            embedding_path TEXT,\n",
    "            image_hash TEXT,\n",
    "            resolution TEXT,\n",
    "            file_size INTEGER,\n",
    "            pca_embedding TEXT,\n",
    "            umap_x REAL,\n",
    "            umap_y REAL\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    cursor.execute(\n",
    "        f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_filename_path ON {table_name}(filename, path);\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_existing_shards():\n",
    "    cursor.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cursor.fetchall()]\n",
    "\n",
    "\n",
    "def rows_in_table(table_name):\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "def exists_in_any_shard(filename, path, shard_tables):\n",
    "    for tbl in shard_tables:\n",
    "        cursor.execute(\n",
    "            f\"SELECT 1 FROM {tbl} WHERE filename=? AND path=? LIMIT 1\", (filename, path)\n",
    "        )\n",
    "        if cursor.fetchone():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def save_batch_to_db(entries, table_name, logfile):\n",
    "    if not entries:\n",
    "        return\n",
    "    log_debug(logfile, f\"[DEBUG] Speichere {len(entries)} Einträge in {table_name} …\")\n",
    "    start = time.time()\n",
    "    cursor.executemany(\n",
    "        f\"\"\"\n",
    "        INSERT INTO {table_name}\n",
    "        (filename, path, color_hist, embedding_path, image_hash, resolution, file_size,\n",
    "         pca_embedding, umap_x, umap_y)\n",
    "        VALUES ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\",\n",
    "        entries,\n",
    "    )\n",
    "    conn.commit()\n",
    "    log_debug(logfile, f\"[DEBUG] DB-Speicherung: {time.time()-start:.2f}s\")\n",
    "\n",
    "\n",
    "# ========== Logging ==========\n",
    "def log_debug(logfile, msg):\n",
    "    logfile.write(msg + \"\\n\")\n",
    "    logfile.flush()\n",
    "\n",
    "\n",
    "def log_skip_or_error(filename, path, reason):\n",
    "    with open(SKIPPED_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{datetime.now()} | {filename} | {path} | {reason}\\n\")\n",
    "\n",
    "\n",
    "# ========== Utils ==========\n",
    "def _unique_npy_path(base_path):\n",
    "    base, ext = os.path.splitext(base_path)\n",
    "    if ext.lower() != \".npy\":\n",
    "        base = base + \".npy\"\n",
    "    out = base\n",
    "    c = 1\n",
    "    while os.path.exists(out):\n",
    "        out = f\"{base[:-4]}_{c}.npy\"\n",
    "        c += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess_for_model(img_uint8, target_size=TARGET_SIZE):\n",
    "    if PIL_AVAILABLE:\n",
    "        im = Image.fromarray(img_uint8)\n",
    "        im = im.resize(target_size, Image.BILINEAR)\n",
    "        arr = np.asarray(im)\n",
    "    else:\n",
    "        th, tw = target_size[1], target_size[0]\n",
    "        y_idx = (np.linspace(0, img_uint8.shape[0] - 1, th)).astype(np.int32)\n",
    "        x_idx = (np.linspace(0, img_uint8.shape[1] - 1, tw)).astype(np.int32)\n",
    "        arr = img_uint8[np.ix_(y_idx, x_idx)]\n",
    "    if EMBED_INPUT_DTYPE == \"float32\":\n",
    "        return arr.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        return arr.astype(np.uint8)\n",
    "\n",
    "\n",
    "def to_embed_dtype(arr):\n",
    "    if EMBED_INPUT_DTYPE == \"float32\":\n",
    "        return arr.astype(np.float32) if arr.dtype != np.float32 else arr\n",
    "    else:\n",
    "        if arr.dtype == np.float32:\n",
    "            return (np.clip(arr, 0.0, 1.0) * 255.0).round().astype(np.uint8)\n",
    "        return arr.astype(np.uint8)\n",
    "\n",
    "\n",
    "def extract_embeddings_micro(bimgs, chunk=MICRO_BATCH):\n",
    "    outs = []\n",
    "    for i in range(0, len(bimgs), chunk):\n",
    "        outs.append(extract_embeddings(bimgs[i : i + chunk]))\n",
    "    return np.vstack(outs)\n",
    "\n",
    "\n",
    "# ========== CSV-Quelle (Large_images.csv) ==========\n",
    "def iter_deferred_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Erwartetes Format je Zeile: timestamp,filename,path,reason\n",
    "    (wir ignorieren timestamp/reason und yielden (filename, path))\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        r = csv.reader(f)\n",
    "        for row in r:\n",
    "            if not row or len(row) < 3:\n",
    "                continue\n",
    "            # robust gg. evtl. Header\n",
    "            if row[0].lower().startswith(\"timestamp\"):\n",
    "                continue\n",
    "            ts, filename, path = row[0], row[1], row[2]\n",
    "            yield filename, path\n",
    "\n",
    "\n",
    "# ========== Feature-Vorbereitung ==========\n",
    "def prepare_image_features(filename, path, logfile):\n",
    "    \"\"\"\n",
    "    Lädt großes Bild, verkleinert sofort, berechnet Hash/Histogramm.\n",
    "    Nur die KLEINE Version wird im Batch gehalten.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = fast_load(path)  # -> uint8 RGB\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            raise TypeError(\"fast_load() must return a NumPy uint8 array.\")\n",
    "        if img.dtype != np.uint8:\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "\n",
    "        resolution = f\"{img.shape[1]}x{img.shape[0]}\"\n",
    "        file_size = os.path.getsize(path)\n",
    "\n",
    "        img_hash = calc_hash(img)\n",
    "\n",
    "        img_small = preprocess_for_model(img)\n",
    "        emb_input = to_embed_dtype(img_small)\n",
    "        color_hist = calc_histogram(img_small)  # bins=default deiner Funktion\n",
    "        hist_str = \",\".join(str(round(float(v), 6)) for v in np.ravel(color_hist))\n",
    "\n",
    "        del img\n",
    "        return (filename, path, emb_input, hist_str, img_hash, resolution, file_size)\n",
    "\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        reason = f\"{e} | Traceback:\\n{tb}\"\n",
    "        log_debug(logfile, f\"[ERROR] Fehler bei {filename}: {e}\")\n",
    "        log_skip_or_error(filename, path, reason)\n",
    "        return (filename, path, None, None, None, None, None, reason)\n",
    "\n",
    "\n",
    "# ========== Flush: Embeddings + Insert ==========\n",
    "def flush_by_embeddings_and_insert(batch_meta, batch_imgs, logfile, table_name):\n",
    "    if not batch_imgs:\n",
    "        return 0\n",
    "    try:\n",
    "        embs = extract_embeddings_micro(batch_imgs, chunk=MICRO_BATCH)\n",
    "    except Exception as e:\n",
    "        log_debug(logfile, f\"[ERROR] Embedding-Chunk fehlgeschlagen: {e}\")\n",
    "        embs = []\n",
    "        for (filename, path, hist_str, img_hash, resolution, size), img_small in zip(\n",
    "            batch_meta, batch_imgs\n",
    "        ):\n",
    "            try:\n",
    "                embs.append(extract_embeddings([img_small])[0])\n",
    "            except Exception as ee:\n",
    "                tb = traceback.format_exc()\n",
    "                log_skip_or_error(\n",
    "                    filename, path, f\"Embedding-Fehler (single): {ee}\\n{tb}\"\n",
    "                )\n",
    "        if not embs:\n",
    "            return 0\n",
    "        embs = np.stack(embs, axis=0)\n",
    "\n",
    "    entries = []\n",
    "    kept = 0\n",
    "    for (filename, path, hist_str, img_hash, resolution, size), emb in zip(\n",
    "        batch_meta, embs\n",
    "    ):\n",
    "        try:\n",
    "            emb_path = _unique_npy_path(os.path.join(EMBEDDING_DIR, f\"{filename}.npy\"))\n",
    "            np.save(emb_path, emb.astype(np.float32))\n",
    "            entries.append(\n",
    "                (\n",
    "                    filename,\n",
    "                    path,\n",
    "                    hist_str,\n",
    "                    emb_path,\n",
    "                    img_hash,\n",
    "                    resolution,\n",
    "                    size,\n",
    "                    \"\",\n",
    "                    0.0,\n",
    "                    0.0,\n",
    "                )\n",
    "            )\n",
    "            kept += 1\n",
    "        except Exception as e_save:\n",
    "            tb = traceback.format_exc()\n",
    "            log_skip_or_error(filename, path, f\"Embedding-Save-Fehler: {e_save}\\n{tb}\")\n",
    "\n",
    "    save_batch_to_db(entries, table_name, logfile)\n",
    "    del embs, entries\n",
    "    return kept\n",
    "\n",
    "\n",
    "# ========== MAIN ==========\n",
    "def main():\n",
    "    logfile = open(LOG_FILE, \"a\", encoding=\"utf-8\", buffering=1)\n",
    "    log_debug(logfile, f\"[{datetime.now()}] [DEBUG] Start Deferred-Run (kein PCA/UMAP)\")\n",
    "    # Liste existierender Shards ermitteln\n",
    "    shard_tables = get_existing_shards()\n",
    "    if not shard_tables:\n",
    "        shard_tables = [f\"{TABLE_PREFIX}1\"]\n",
    "        create_table_if_not_exists(shard_tables[0])\n",
    "\n",
    "    # In die letzte Tabelle weiter einfügen\n",
    "    current_table_name = shard_tables[-1]\n",
    "    current_count = rows_in_table(current_table_name)\n",
    "\n",
    "    paths_iter = list(iter_deferred_csv(DEFERRED_CSV))\n",
    "    total_items = len(paths_iter)\n",
    "    pbar = tqdm(total=total_items, desc=\"Deferred-Verarbeitung\", unit=\"Bild\")\n",
    "\n",
    "    batch_meta, batch_imgs = [], []\n",
    "    cur_pixels = 0\n",
    "    total_inserted = 0\n",
    "\n",
    "    for filename, path in paths_iter:\n",
    "        # Dedupe/Existenzcheck über alle Shards\n",
    "        if exists_in_any_shard(filename, path, shard_tables):\n",
    "            log_skip_or_error(filename, path, \"Bereits in DB (deferred pass)\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        result = prepare_image_features(filename, path, logfile)\n",
    "        if len(result) == 8:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        _, p, emb_input, hist_str, img_hash, resolution, size = result\n",
    "        batch_meta.append((filename, p, hist_str, img_hash, resolution, size))\n",
    "        batch_imgs.append(emb_input)\n",
    "        cur_pixels += emb_input.shape[0] * emb_input.shape[1]\n",
    "        pbar.update(1)\n",
    "\n",
    "        if cur_pixels >= PIXEL_BUDGET:\n",
    "            kept = flush_by_embeddings_and_insert(\n",
    "                batch_meta, batch_imgs, logfile, current_table_name\n",
    "            )\n",
    "            total_inserted += kept\n",
    "            current_count += kept\n",
    "\n",
    "            batch_meta.clear()\n",
    "            batch_imgs.clear()\n",
    "            cur_pixels = 0\n",
    "            gc.collect()\n",
    "            try:\n",
    "                import torch\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    log_debug(logfile, \"[DEBUG] CUDA Cache geleert.\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Sharding: neue Tabelle nötig?\n",
    "            if current_count >= IMAGES_PER_TABLE:\n",
    "                # neue Tabelle anlegen\n",
    "                next_id = len(shard_tables) + 1\n",
    "                current_table_name = f\"{TABLE_PREFIX}{next_id}\"\n",
    "                create_table_if_not_exists(current_table_name)\n",
    "                shard_tables.append(current_table_name)\n",
    "                current_count = 0\n",
    "                log_debug(\n",
    "                    logfile, f\"[DEBUG] Neue Tabelle angelegt: {current_table_name}\"\n",
    "                )\n",
    "\n",
    "    # Finaler Flush\n",
    "    if batch_imgs:\n",
    "        kept = flush_by_embeddings_and_insert(\n",
    "            batch_meta, batch_imgs, logfile, current_table_name\n",
    "        )\n",
    "        total_inserted += kept\n",
    "        current_count += kept\n",
    "        batch_meta.clear()\n",
    "        batch_imgs.clear()\n",
    "        cur_pixels = 0\n",
    "        gc.collect()\n",
    "\n",
    "    pbar.close()\n",
    "    log_debug(\n",
    "        logfile, f\"[{datetime.now()}] ✓ Deferred-Run fertig. Inserts: {total_inserted}\"\n",
    "    )\n",
    "    logfile.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# ========= ENTRYPOINT =========\n",
    "if __name__ == \"__main__\":\n",
    "    with cProfile.Profile() as pr:\n",
    "        main()\n",
    "    with open(\"profiling_results_deferred.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        stats = pstats.Stats(pr, stream=f)\n",
    "        stats.sort_stats(\"cumtime\").print_stats()\n",
    "    stats.dump_stats(\"profiling_results_deferred.prof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1d494",
   "metadata": {},
   "source": [
    "# FIT UND TRANSFORM PCA UND CALCULATE UMAP KOORDINATEN GLOBALLY AN ALLE IMAGES GLEICHZEITIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71298220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PASS1 IPCA partial_fit: 100%|██████████| 359754/359754 [32:55<00:00, 182.08vec/s]\n",
      "PASS2 IPCA transform+save: 100%|██████████| 359754/359754 [27:58<00:00, 214.38vec/s]\n",
      "z:\\CODING\\UNI\\BIG_DATA\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "PASS3 UMAP transform+update: 100%|██████████| 359754/359754 [29:27<00:00, 203.56vec/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Globales Finalisieren (IPCA + UMAP) über alle Shards.\n",
    "Fixes:\n",
    "- PASS 2: korrektes Mapping (id <-> embedding_path), defekte Pfade werden pro-Item geloggt/übersprungen\n",
    "- _unique_npy_path: Endung .npy bleibt korrekt erhalten (keine \"pfad ohne endung\" mehr)\n",
    "- modell wird robust gespeichert\n",
    "-ipca = load(MODEL_DIR / \"ipca.joblib\")\n",
    "# base_vec = Roh-Embedding (wie bisher erzeugt)\n",
    "pca_vec = ipca.transform(base_vec[None, :]).astype(np.float32)[0]\n",
    "- Logging: Debug- und Fehlerausgaben in Logdatei, skipped_and_errors.txt\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, sqlite3, random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import umap\n",
    "\n",
    "import json, time\n",
    "from joblib import dump, load\n",
    "\n",
    "# ========= CONFIG =========\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "EMBEDDING_PCA_DIR = Path(r\"C:\\BIG_DATA\\embeddings_pca\")\n",
    "MODEL_DIR = Path(r\"C:\\BIG_DATA\\models\")\n",
    "LOG_FILE = \"PCA_verarbeitung_log.txt\"\n",
    "SKIPPED_LOG = \"PCA_skipped_and_errors.txt\"\n",
    "\n",
    "# Tuning\n",
    "PCA_COMPONENTS = 64  # 64 oder 100 – kleiner = schneller/leichter\n",
    "UMAP_SAMPLE_SIZE = 25_000  # Sample fürs UMAP-Fit (Rest wird transformiert)\n",
    "CHUNK = 4096  # I/O-Chunkgröße (RAM vs. Speed)\n",
    "\n",
    "\n",
    "# ========= Logging / Helpers =========\n",
    "def log_debug(f, msg: str):\n",
    "    f.write(msg + \"\\n\")\n",
    "    f.flush()\n",
    "\n",
    "\n",
    "def log_skip_or_error(filename_or_id, path, reason):\n",
    "    with open(SKIPPED_LOG, \"a\", encoding=\"utf-8\") as ff:\n",
    "        ff.write(f\"{datetime.now()} | {filename_or_id} | {path} | {reason}\\n\")\n",
    "\n",
    "\n",
    "def _unique_npy_path(base_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Gibt einen nicht-belegten .npy-Pfad zurück. Falls base_path bereits .npy hat,\n",
    "    bleibt die Endung erhalten; sonst wird .npy angehängt.\n",
    "    \"\"\"\n",
    "    base, ext = os.path.splitext(base_path)\n",
    "    if ext.lower() != \".npy\":\n",
    "        out = base_path + \".npy\"\n",
    "    else:\n",
    "        out = base_path\n",
    "    c = 1\n",
    "    while os.path.exists(out):\n",
    "        # base ist ohne Endung\n",
    "        out = f\"{base}_{c}.npy\"\n",
    "        c += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_tables(cur, prefix):\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{prefix}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "\n",
    "def load_embeddings(paths):\n",
    "    \"\"\"Lädt eine Liste von .npy-Pfaden -> (N,D) float32, überspringt defekte. (Nur für PASS 1.)\"\"\"\n",
    "    mats, keep = [], []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            mats.append(np.load(p))\n",
    "            keep.append(p)\n",
    "        except Exception as e:\n",
    "            log_skip_or_error(\"UNKNOWN\", p, f\"PCA-Load-Fehler: {e}\")\n",
    "    if not mats:\n",
    "        return None, []\n",
    "    X = np.stack(mats, axis=0).astype(np.float32)\n",
    "    return X, keep\n",
    "\n",
    "\n",
    "# ========= Finalizer =========\n",
    "def finalize_all_shards_global(\n",
    "    conn,\n",
    "    cursor,\n",
    "    logfile,\n",
    "    sample_size=UMAP_SAMPLE_SIZE,\n",
    "    pca_components=PCA_COMPONENTS,\n",
    "    chunk=CHUNK,\n",
    "):\n",
    "    log_debug(logfile, \"[DEBUG] === Globales Finalisieren (IPCA+UMAP) startet ===\")\n",
    "\n",
    "    # Tabellen finden\n",
    "    tables = get_tables(cursor, TABLE_PREFIX)\n",
    "    if not tables:\n",
    "        log_debug(logfile, \"[WARN] Keine Shard-Tabellen gefunden – Abbruch.\")\n",
    "        return\n",
    "    os.makedirs(EMBEDDING_PCA_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    log_debug(\n",
    "        logfile, f\"[DEBUG] MODEL_DIR exists={MODEL_DIR.exists()} path={MODEL_DIR}\"\n",
    "    )\n",
    "\n",
    "    # eigener Lese-Cursor (SELECT vom UPDATE trennen!)\n",
    "    cur_read = conn.cursor()\n",
    "\n",
    "    # -------- totals für Progressbars --------\n",
    "    total_embed_rows = 0\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(\n",
    "            f\"SELECT COUNT(*) FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\"\n",
    "        )\n",
    "        total_embed_rows += int(cur_read.fetchone()[0])\n",
    "\n",
    "    # -------- PASS 1: IPCA partial_fit (global, gestreamt) --------\n",
    "    ipca = None\n",
    "    seen = 0\n",
    "    pbar1 = tqdm(total=total_embed_rows, desc=\"PASS1 IPCA partial_fit\", unit=\"vec\")\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(\n",
    "            f\"SELECT embedding_path FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\"\n",
    "        )\n",
    "        while True:\n",
    "            rows = cur_read.fetchmany(chunk)\n",
    "            if not rows:\n",
    "                break\n",
    "            paths = [r[0] for r in rows]\n",
    "            X, _ = load_embeddings(paths)  # für partial_fit reicht das\n",
    "            if X is not None:\n",
    "                if ipca is None:\n",
    "                    nfeat = X.shape[1]\n",
    "                    ncomp = min(pca_components, nfeat)\n",
    "                    ipca = IncrementalPCA(n_components=ncomp)\n",
    "                    log_debug(\n",
    "                        logfile,\n",
    "                        f\"[DEBUG] IPCA init mit n_components={ncomp} (feat={nfeat})\",\n",
    "                    )\n",
    "                ipca.partial_fit(X)\n",
    "                seen += X.shape[0]\n",
    "                del X\n",
    "            # Fortschritt nach geplanter Menge im Chunk (auch wenn einzelne fehlschlugen)\n",
    "            pbar1.update(len(paths))\n",
    "            gc.collect()\n",
    "    pbar1.close()\n",
    "\n",
    "    if ipca is None or seen == 0:\n",
    "        log_debug(\n",
    "            logfile,\n",
    "            \"[WARN] Keine Embeddings gefunden – IPCA konnte nicht trainiert werden.\",\n",
    "        )\n",
    "        return\n",
    "\n",
    "    log_debug(logfile, f\"[DEBUG] PASS 1 fertig – partial_fit auf {seen} Vektoren.\")\n",
    "    _print_res(\"Nach IPCA PASS 1\", logfile)\n",
    "\n",
    "    try:\n",
    "        dump(ipca, MODEL_DIR / \"ipca.joblib\")\n",
    "        log_debug(\n",
    "            logfile, f\"[DEBUG] IPCA gespeichert unter: {MODEL_DIR / 'ipca.joblib'}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log_debug(logfile, f\"[ERROR] IPCA konnte nicht gespeichert werden: {e}\")\n",
    "\n",
    "    # -------- PASS 2: transform, PCA speichern, pca_embedding updaten, Sample sammeln --------\n",
    "    rng = random.Random(42)\n",
    "    sample_Xp, sample_cnt = [], 0\n",
    "    saved = 0\n",
    "\n",
    "    pbar2 = tqdm(total=total_embed_rows, desc=\"PASS2 IPCA transform+save\", unit=\"vec\")\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(\n",
    "            f\"SELECT id, embedding_path FROM {tbl} WHERE embedding_path IS NOT NULL AND embedding_path != ''\"\n",
    "        )\n",
    "        while True:\n",
    "            rows = cur_read.fetchmany(chunk)\n",
    "            if not rows:\n",
    "                break\n",
    "\n",
    "            ids = [r[0] for r in rows]\n",
    "            paths = [r[1] for r in rows]\n",
    "\n",
    "            # WICHTIG: pro (id, path) laden -> nur erfolgreiche Paare übernehmen\n",
    "            mats, keep_ids = [], []\n",
    "            for _id, p in zip(ids, paths):\n",
    "                try:\n",
    "                    mats.append(np.load(p).astype(np.float32))\n",
    "                    keep_ids.append(_id)\n",
    "                except Exception as e:\n",
    "                    log_skip_or_error(f\"DB_ID={_id}\", p, f\"PCA-Load-Fehler: {e}\")\n",
    "\n",
    "            if mats:\n",
    "                X = np.stack(mats, axis=0)\n",
    "                Xp = ipca.transform(X)\n",
    "\n",
    "                updates = []\n",
    "                for _id_ok, vec in zip(keep_ids, Xp):\n",
    "                    pca_path = _unique_npy_path(\n",
    "                        str(EMBEDDING_PCA_DIR / f\"{_id_ok}_pca.npy\")\n",
    "                    )\n",
    "                    try:\n",
    "                        np.save(pca_path, vec.astype(np.float32))\n",
    "                    except Exception as e:\n",
    "                        log_skip_or_error(\n",
    "                            f\"DB_ID={_id_ok}\", pca_path, f\"PCA-Save-Fehler: {e}\"\n",
    "                        )\n",
    "                        pca_path = \"\"\n",
    "                    updates.append((pca_path, _id_ok))\n",
    "\n",
    "                    # Reservoir-Sampling für UMAP-Fit\n",
    "                    sample_cnt += 1\n",
    "                    if len(sample_Xp) < sample_size:\n",
    "                        sample_Xp.append(vec.copy())\n",
    "                    else:\n",
    "                        j = rng.randrange(sample_cnt)\n",
    "                        if j < sample_size:\n",
    "                            sample_Xp[j] = vec.copy()\n",
    "\n",
    "                cursor.executemany(\n",
    "                    f\"UPDATE {tbl} SET pca_embedding=? WHERE id=?\", updates\n",
    "                )\n",
    "                conn.commit()\n",
    "                saved += len(updates)\n",
    "\n",
    "                del X, Xp, updates, mats\n",
    "                gc.collect()\n",
    "\n",
    "            # Fortschritt: geplanter Chunk (auch wenn einzelne fehlschlugen)\n",
    "            pbar2.update(len(paths))\n",
    "    pbar2.close()\n",
    "\n",
    "    log_debug(logfile, f\"[DEBUG] PASS 2 fertig – {saved} PCA-Vektoren gespeichert.\")\n",
    "    _print_res(\"Nach IPCA PASS 2\", logfile)\n",
    "\n",
    "    if not sample_Xp:\n",
    "        log_debug(logfile, \"[WARN] Kein PCA-Sample – UMAP wird übersprungen.\")\n",
    "        return\n",
    "\n",
    "    # -------- UMAP Fit (auf Sample) --------\n",
    "    sample_mat = np.vstack(sample_Xp).astype(np.float32)\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=2, metric=\"euclidean\", random_state=42, low_memory=True\n",
    "    )\n",
    "    log_debug(\n",
    "        logfile, f\"[DEBUG] UMAP Fit auf Sample mit {sample_mat.shape[0]} Vektoren …\"\n",
    "    )\n",
    "    reducer.fit(sample_mat)\n",
    "    del sample_mat, sample_Xp\n",
    "    gc.collect()\n",
    "    _print_res(\"Nach UMAP Fit (Sample)\", logfile)\n",
    "    # UMAP-Reducer speichern\n",
    "    try:\n",
    "        dump(reducer, MODEL_DIR / \"umap_reducer.joblib\")\n",
    "        log_debug(\n",
    "            logfile,\n",
    "            f\"[DEBUG] UMAP-Reducer gespeichert unter: {MODEL_DIR / 'umap_reducer.joblib'}\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log_debug(logfile, f\"[ERROR] UMAP-Reducer konnte nicht gespeichert werden: {e}\")\n",
    "\n",
    "    # --- WRITE META ---\n",
    "    try:\n",
    "        # robust: zieh Werte aus Attributen, egal ob *_ verfügbar ist\n",
    "        ipca_n_components = int(\n",
    "            getattr(\n",
    "                ipca,\n",
    "                \"n_components_\",\n",
    "                getattr(ipca, \"n_components\", ipca.components_.shape[0]),\n",
    "            )\n",
    "        )\n",
    "        ipca_n_features_in = int(\n",
    "            getattr(ipca, \"n_features_in_\", ipca.components_.shape[1])\n",
    "        )\n",
    "\n",
    "        meta = {\n",
    "            \"ipca\": {\n",
    "                \"path\": str(MODEL_DIR / \"ipca.joblib\"),\n",
    "                \"n_components\": ipca_n_components,\n",
    "                \"n_features_in_\": ipca_n_features_in,\n",
    "                \"trained_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            },\n",
    "            \"umap\": {\n",
    "                \"path\": str(MODEL_DIR / \"umap_reducer.joblib\"),\n",
    "                \"n_components\": 2,\n",
    "                \"metric\": \"euclidean\",\n",
    "            },\n",
    "        }\n",
    "        with open(MODEL_DIR / \"model_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "        log_debug(\n",
    "            logfile, f\"[DEBUG] Meta gespeichert unter: {MODEL_DIR / 'model_meta.json'}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log_debug(logfile, f\"[ERROR] Meta konnte nicht gespeichert werden: {e}\")\n",
    "\n",
    "    # -------- PASS 3: UMAP transform aller PCA-Vektoren & DB-Update --------\n",
    "    # wie viele haben jetzt pca_embedding?\n",
    "    total_pca_rows = 0\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(\n",
    "            f\"SELECT COUNT(*) FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\"\n",
    "        )\n",
    "        total_pca_rows += int(cur_read.fetchone()[0])\n",
    "\n",
    "    updated = 0\n",
    "    pbar3 = tqdm(total=total_pca_rows, desc=\"PASS3 UMAP transform+update\", unit=\"vec\")\n",
    "    for tbl in tables:\n",
    "        cur_read.execute(\n",
    "            f\"SELECT id, pca_embedding FROM {tbl} WHERE pca_embedding IS NOT NULL AND pca_embedding != ''\"\n",
    "        )\n",
    "        while True:\n",
    "            rows = cur_read.fetchmany(chunk)\n",
    "            if not rows:\n",
    "                break\n",
    "\n",
    "            ids = [r[0] for r in rows]\n",
    "            ppaths = [r[1] for r in rows]\n",
    "\n",
    "            mats, keep_ids = [], []\n",
    "            for _id, p in zip(ids, ppaths):\n",
    "                try:\n",
    "                    mats.append(np.load(p).astype(np.float32))\n",
    "                    keep_ids.append(_id)\n",
    "                except Exception as e:\n",
    "                    log_skip_or_error(f\"DB_ID={_id}\", p, f\"UMAP-PCA-Load-Fehler: {e}\")\n",
    "\n",
    "            if mats:\n",
    "                Xp = np.stack(mats, axis=0)\n",
    "                coords = reducer.transform(Xp)\n",
    "                updates = [\n",
    "                    (float(x), float(y), i) for i, (x, y) in zip(keep_ids, coords)\n",
    "                ]\n",
    "                cursor.executemany(\n",
    "                    f\"UPDATE {tbl} SET umap_x=?, umap_y=? WHERE id=?\", updates\n",
    "                )\n",
    "                conn.commit()\n",
    "                updated += len(updates)\n",
    "\n",
    "                del Xp, coords, updates, mats\n",
    "                gc.collect()\n",
    "\n",
    "            pbar3.update(len(rows))\n",
    "    pbar3.close()\n",
    "\n",
    "    log_debug(\n",
    "        logfile, f\"[DEBUG] PASS 3 fertig – {updated} UMAP-Koordinaten aktualisiert.\"\n",
    "    )\n",
    "    _print_res(\"Nach UMAP PASS 3\", logfile)\n",
    "    log_debug(\n",
    "        logfile, \"[DEBUG] === Globales Finalisieren (IPCA+UMAP) abgeschlossen ===\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _print_res(stage, logfile):\n",
    "    # kurze Ressourcen-Info\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / (1024**2)\n",
    "    cpu = process.cpu_percent(interval=0.1)\n",
    "    log_debug(logfile, f\"[RESOURCE] {stage} | RAM: {mem:.2f} MB | CPU: {cpu:.2f}%\")\n",
    "\n",
    "\n",
    "# ========= main =========\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(EMBEDDING_PCA_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # (Optionale) PRAGMAs\n",
    "    cursor.execute(\"PRAGMA journal_mode=OFF;\")\n",
    "    cursor.execute(\"PRAGMA synchronous=OFF;\")\n",
    "    cursor.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "    cursor.execute(\"PRAGMA mmap_size=0;\")\n",
    "\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\", buffering=1) as logfile:\n",
    "        log_debug(logfile, f\"[{datetime.now()}] [DEBUG] finalize_only gestartet\")\n",
    "        try:\n",
    "            finalize_all_shards_global(\n",
    "                conn,\n",
    "                cursor,\n",
    "                logfile,\n",
    "                sample_size=UMAP_SAMPLE_SIZE,\n",
    "                pca_components=PCA_COMPONENTS,\n",
    "                chunk=CHUNK,\n",
    "            )\n",
    "        finally:\n",
    "            log_debug(logfile, f\"[{datetime.now()}] [DEBUG] finalize_only beendet\")\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd54901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_1: embeddings ok=100000, missing_or_wrong=0, total_nonempty=100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_2: embeddings ok=51500, missing_or_wrong=0, total_nonempty=51500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_3: embeddings ok=50000, missing_or_wrong=0, total_nonempty=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_4: embeddings ok=50000, missing_or_wrong=0, total_nonempty=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_5: embeddings ok=50004, missing_or_wrong=0, total_nonempty=50004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_6: embeddings ok=50000, missing_or_wrong=0, total_nonempty=50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_7: embeddings ok=8250, missing_or_wrong=0, total_nonempty=8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os, sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "\n",
    "\n",
    "def get_tables(cur):\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "\n",
    "def main():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "    tables = get_tables(cur)\n",
    "    for tbl in tables:\n",
    "        cur.execute(\n",
    "            f\"SELECT COUNT(*) FROM {tbl} WHERE embedding_path IS NOT NULL AND TRIM(embedding_path)<>''\"\n",
    "        )\n",
    "        total = int(cur.fetchone()[0])\n",
    "        if total == 0:\n",
    "            print(f\"{tbl}: 0 embedding_path – übersprungen\")\n",
    "            continue\n",
    "        ok = missing = 0\n",
    "        cur.execute(\n",
    "            f\"SELECT embedding_path FROM {tbl} WHERE embedding_path IS NOT NULL AND TRIM(embedding_path)<>''\"\n",
    "        )\n",
    "        for (p,) in tqdm(cur.fetchall(), desc=tbl, unit=\"path\", leave=False):\n",
    "            p = p.strip()\n",
    "            if os.path.exists(p):\n",
    "                ok += 1\n",
    "            elif os.path.exists(p + \".npy\"):\n",
    "                # Falls hier viele Treffer: vorherigen Repair-Job laufen lassen, um in der DB auf '.npy' zu korrigieren\n",
    "                missing += 1\n",
    "            else:\n",
    "                missing += 1\n",
    "        print(\n",
    "            f\"{tbl}: embeddings ok={ok}, missing_or_wrong={missing}, total_nonempty={total}\"\n",
    "        )\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a42ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tabellen: 100%|██████████| 7/7 [00:13<00:00,  1.94s/tbl]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Summary: hash_reports_all_columns\\all_columns_summary.csv\n",
      "[OK] Missing paths (alle Spalten): hash_reports_all_columns\\missing_by_column_paths.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Geht über alle image_features_part_% Tabellen,\n",
    "zählt pro Spalte (alle!) filled/missing\n",
    "und exportiert fehlende Pfade je Spalte in eine CSV.\n",
    "Erzeugt:\n",
    "- all_columns_summary.csv  (long format: table, column, col_type, total, filled, missing)\n",
    "- missing_by_column_paths.csv (table, column, id, filename, path)\n",
    "\"\"\"\n",
    "\n",
    "import os, csv, sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== CONFIG ======\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "OUTPUT_DIR = \"hash_reports_all_columns\"\n",
    "USE_TQDM = True\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "SUMMARY_CSV = os.path.join(OUTPUT_DIR, \"all_columns_summary.csv\")\n",
    "MISSING_CSV = os.path.join(OUTPUT_DIR, \"missing_by_column_paths.csv\")\n",
    "\n",
    "\n",
    "# ====== DB helpers ======\n",
    "def get_shard_tables(cur):\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "\n",
    "def get_columns(cur, table):\n",
    "    \"\"\"\n",
    "    Liefert Liste von (name, decl_type) aus PRAGMA table_info.\n",
    "    decl_type kann z.B. 'TEXT', 'INTEGER', 'REAL', 'NUMERIC', 'BLOB' etc. sein.\n",
    "    \"\"\"\n",
    "    cur.execute(f\"PRAGMA table_info({table});\")\n",
    "    cols = [(row[1], (row[2] or \"\").upper()) for row in cur.fetchall()]\n",
    "    # 'id' rauswerfen\n",
    "    return [(n, t) for (n, t) in cols if n.lower() != \"id\"]\n",
    "\n",
    "\n",
    "def count_total(cur, table):\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    return int(cur.fetchone()[0])\n",
    "\n",
    "\n",
    "def build_missing_condition(col_name, col_type_upper):\n",
    "    # TEXT -> Null oder leere/Whitespace-Strings; sonst nur NULL\n",
    "    if \"TEXT\" in col_type_upper:\n",
    "        return f\"({col_name} IS NULL OR TRIM({col_name}) = '')\"\n",
    "    else:\n",
    "        return f\"({col_name} IS NULL)\"\n",
    "\n",
    "\n",
    "def column_exists(cur, table, col_name):\n",
    "    cur.execute(f\"PRAGMA table_info({table});\")\n",
    "    return any(r[1] == col_name for r in cur.fetchall())\n",
    "\n",
    "\n",
    "# ====== Export missing paths (streaming) ======\n",
    "def export_missing_for_column(\n",
    "    cur, table, col_name, cond, writer, has_filename, has_path\n",
    "):\n",
    "    # Baue SELECT dynamisch je nach vorhandenen Spalten\n",
    "    select_cols = [\"id\"]\n",
    "    if has_filename:\n",
    "        select_cols.append(\"filename\")\n",
    "    if has_path:\n",
    "        select_cols.append(\"path\")\n",
    "    sel = \", \".join(select_cols)\n",
    "\n",
    "    cur.execute(f\"SELECT {sel} FROM {table} WHERE {cond}\")\n",
    "    while True:\n",
    "        rows = cur.fetchmany(5000)\n",
    "        if not rows:\n",
    "            break\n",
    "        for row in rows:\n",
    "            # row ist Tupel mit 1..3 Elementen je nach vorhandenen Spalten\n",
    "            rid = row[0]\n",
    "            fname = row[1] if has_filename else \"\"\n",
    "            pth = (\n",
    "                row[2]\n",
    "                if (has_filename and has_path and len(row) > 2)\n",
    "                else (\n",
    "                    row[1] if (has_path and not has_filename and len(row) > 1) else \"\"\n",
    "                )\n",
    "            )\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"table\": table,\n",
    "                    \"column\": col_name,\n",
    "                    \"id\": rid,\n",
    "                    \"filename\": fname,\n",
    "                    \"path\": pth,\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "def main():\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    tables = get_shard_tables(cur)\n",
    "    if not tables:\n",
    "        print(\"Keine Shard-Tabellen gefunden.\")\n",
    "        return\n",
    "\n",
    "    # Prepare CSVs\n",
    "    with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fsum, open(\n",
    "        MISSING_CSV, \"w\", newline=\"\", encoding=\"utf-8\"\n",
    "    ) as fmiss:\n",
    "\n",
    "        sum_writer = csv.DictWriter(\n",
    "            fsum,\n",
    "            fieldnames=[\"table\", \"column\", \"col_type\", \"total\", \"filled\", \"missing\"],\n",
    "        )\n",
    "        sum_writer.writeheader()\n",
    "\n",
    "        miss_writer = csv.DictWriter(\n",
    "            fmiss, fieldnames=[\"table\", \"column\", \"id\", \"filename\", \"path\"]\n",
    "        )\n",
    "        miss_writer.writeheader()\n",
    "\n",
    "        iterator = tqdm(tables, desc=\"Tabellen\", unit=\"tbl\") if USE_TQDM else tables\n",
    "        for tbl in iterator:\n",
    "            # Spalten & Basisinfos\n",
    "            cols = get_columns(cur, tbl)\n",
    "            total = count_total(cur, tbl)\n",
    "\n",
    "            # Prüfen, ob filename/path vorhanden sind (für Export)\n",
    "            present_colnames = {name for (name, _) in cols}\n",
    "            has_filename = \"filename\" in present_colnames\n",
    "            has_path = \"path\" in present_colnames\n",
    "\n",
    "            # Fortschritt pro Tabelle (über Spalten)\n",
    "            col_iter = (\n",
    "                tqdm(cols, desc=f\"{tbl}\", unit=\"col\", leave=False) if USE_TQDM else cols\n",
    "            )\n",
    "            for col_name, col_type in col_iter:\n",
    "                cond = build_missing_condition(col_name, col_type)\n",
    "\n",
    "                # Missing zählen\n",
    "                cur.execute(f\"SELECT COUNT(*) FROM {tbl} WHERE {cond}\")\n",
    "                missing = int(cur.fetchone()[0])\n",
    "                filled = total - missing\n",
    "\n",
    "                sum_writer.writerow(\n",
    "                    {\n",
    "                        \"table\": tbl,\n",
    "                        \"column\": col_name,\n",
    "                        \"col_type\": col_type,\n",
    "                        \"total\": total,\n",
    "                        \"filled\": filled,\n",
    "                        \"missing\": missing,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Fehlende Pfade exportieren\n",
    "                if missing > 0:\n",
    "                    export_missing_for_column(\n",
    "                        cur, tbl, col_name, cond, miss_writer, has_filename, has_path\n",
    "                    )\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"[OK] Summary: {SUMMARY_CSV}\")\n",
    "    print(f\"[OK] Missing paths (alle Spalten): {MISSING_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140d7c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexiere Embeddings im Ordner …\n",
      "Gefundene Embeddings: 359,474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_1: 100%|██████████| 100000/100000 [00:07<00:00, 13850.67row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_1: ok=100,000, fixed=0, missing=0, total=100,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_2: 100%|██████████| 51500/51500 [00:03<00:00, 14453.34row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_2: ok=51,500, fixed=0, missing=0, total=51,500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_3: 100%|██████████| 50000/50000 [00:03<00:00, 14801.86row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_3: ok=50,000, fixed=0, missing=0, total=50,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_4: 100%|██████████| 50000/50000 [00:03<00:00, 15238.08row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_4: ok=50,000, fixed=0, missing=0, total=50,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_5: 100%|██████████| 50004/50004 [00:03<00:00, 15031.16row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_5: ok=50,004, fixed=0, missing=0, total=50,004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_6: 100%|██████████| 50000/50000 [02:34<00:00, 324.63row/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_6: ok=5,000, fixed=45,000, missing=0, total=50,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_features_part_7: 100%|██████████| 8250/8250 [00:11<00:00, 718.09row/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features_part_7: ok=5,000, fixed=3,250, missing=0, total=8,250\n",
      "\n",
      "=== Zusammenfassung ===\n",
      "OK:       311,504\n",
      "Fixed:    48,250\n",
      "Missing:  0\n",
      "Fehlende Bildpfade in: missing_embedding_images.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Repariert embedding_path in allen image_features_part_% Tabellen – ID-weise paged,\n",
    "damit Updates den Scan nicht abbrechen. Loggt fehlende als Bildpfade.\n",
    "\"\"\"\n",
    "# ===== CONFIG =====\n",
    "DB_PATH = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "EMBEDDING_DIR = Path(r\"C:\\BIG_DATA\\embeddings\")\n",
    "MISSING_TXT = \"missing_embedding_images.txt\"\n",
    "PAGE = 5000  # Seitengröße nach id\n",
    "\n",
    "\n",
    "# ===== Helpers =====\n",
    "def get_tables(cur):\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "\n",
    "def col_exists(cur, table, col):\n",
    "    cur.execute(f\"PRAGMA table_info({table});\")\n",
    "    return any(r[1] == col for r in cur.fetchall())\n",
    "\n",
    "\n",
    "def index_embeddings(root: Path):\n",
    "    idx = {}\n",
    "    for base, _, files in os.walk(root):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith(\".npy\"):\n",
    "                full = os.path.join(base, fn)\n",
    "                idx.setdefault(fn, full)  # erster Treffer gewinnt\n",
    "    return idx\n",
    "\n",
    "\n",
    "def expected_basenames(filename: str):\n",
    "    fn = (filename or \"\").strip()\n",
    "    stem = Path(fn).stem\n",
    "    return [f\"{fn}.npy\", f\"{stem}.npy\"]\n",
    "\n",
    "\n",
    "def is_ok(embedding_path: str, filename: str) -> bool:\n",
    "    if not embedding_path:\n",
    "        return False\n",
    "    p = embedding_path.strip()\n",
    "    if not p or not p.lower().endswith(\".npy\"):\n",
    "        return False\n",
    "    if not os.path.exists(p):\n",
    "        return False\n",
    "    base = os.path.basename(p)\n",
    "    return base in set(expected_basenames(filename))\n",
    "\n",
    "\n",
    "def try_fix_path(p_current: str, filename: str, idx: dict, embedding_dir: Path):\n",
    "    fn = (filename or \"\").strip()\n",
    "    stem = Path(fn).stem\n",
    "    cand_names = [f\"{fn}.npy\", f\"{stem}.npy\"]\n",
    "\n",
    "    # 1) alter Pfad + .npy?\n",
    "    if p_current:\n",
    "        p_strip = p_current.strip()\n",
    "        if p_strip and not p_strip.lower().endswith(\".npy\"):\n",
    "            cand = p_strip + \".npy\"\n",
    "            if os.path.exists(cand):\n",
    "                return cand\n",
    "\n",
    "    # 2) direkt im Zielordner\n",
    "    for name in cand_names:\n",
    "        p = embedding_dir / name\n",
    "        if os.path.exists(p):\n",
    "            return str(p)\n",
    "\n",
    "    # 3) Index\n",
    "    for name in cand_names:\n",
    "        if name in idx:\n",
    "            return idx[name]\n",
    "\n",
    "    # 4) eindeutiger Prefix-Treffer (stem_)\n",
    "    stem_prefix = f\"{stem}_\"\n",
    "    matches = [path for base, path in idx.items() if base.startswith(stem_prefix)]\n",
    "    if len(matches) == 1:\n",
    "        return matches[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ===== Main per Tabelle (paged) =====\n",
    "def process_table(conn, tbl, idx, missing_file):\n",
    "    cur = conn.cursor()\n",
    "    cur_write = conn.cursor()\n",
    "\n",
    "    # Zählung für Progressbar\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "    total = int(cur.fetchone()[0])\n",
    "    pbar = tqdm(total=total, desc=tbl, unit=\"row\")\n",
    "\n",
    "    ok = fixed = missing = 0\n",
    "    last_id = 0\n",
    "\n",
    "    while True:\n",
    "        # Page nach Primärschlüssel\n",
    "        cur.execute(\n",
    "            f\"\"\"SELECT id, filename, path, embedding_path\n",
    "                FROM {tbl}\n",
    "                WHERE id > ?\n",
    "                ORDER BY id ASC\n",
    "                LIMIT ?\"\"\",\n",
    "            (last_id, PAGE),\n",
    "        )\n",
    "        rows = cur.fetchall()\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        updates = []  # (new_path, id)\n",
    "        for _id, filename, img_path, emb_path in rows:\n",
    "            if is_ok(emb_path, filename):\n",
    "                ok += 1\n",
    "                pbar.update(1)\n",
    "                last_id = _id\n",
    "                continue\n",
    "\n",
    "            newp = try_fix_path(emb_path or \"\", filename or \"\", idx, EMBEDDING_DIR)\n",
    "            if newp:\n",
    "                updates.append((newp, _id))\n",
    "                fixed += 1\n",
    "            else:\n",
    "                # fehlend -> Bildpfad loggen\n",
    "                missing += 1\n",
    "                missing_file.write(\n",
    "                    f\"{_id}; {tbl}; {filename or ''}; {img_path or ''}\\n\"\n",
    "                )\n",
    "\n",
    "            pbar.update(1)\n",
    "            last_id = _id\n",
    "\n",
    "        if updates:\n",
    "            cur_write.executemany(\n",
    "                f\"UPDATE {tbl} SET embedding_path=? WHERE id=?\", updates\n",
    "            )\n",
    "            conn.commit()\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"{tbl}: ok={ok:,}, fixed={fixed:,}, missing={missing:,}, total={total:,}\")\n",
    "    return ok, fixed, missing\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Indexiere Embeddings im Ordner …\")\n",
    "    idx = index_embeddings(EMBEDDING_DIR)\n",
    "    print(f\"Gefundene Embeddings: {len(idx):,}\")\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH, isolation_level=None)\n",
    "    cur = conn.cursor()\n",
    "    tables = get_tables(cur)\n",
    "    if not tables:\n",
    "        print(\"Keine Tabellen gefunden.\")\n",
    "        return\n",
    "\n",
    "    # Spalten-Check\n",
    "    for t in tables:\n",
    "        for c in (\"filename\", \"path\", \"embedding_path\"):\n",
    "            if not col_exists(cur, t, c):\n",
    "                print(f\"[WARN] {t}: Spalte {c} fehlt – übersprungen.\")\n",
    "                tables.remove(t)\n",
    "                break\n",
    "\n",
    "    grand_ok = grand_fixed = grand_missing = 0\n",
    "    with open(MISSING_TXT, \"w\", encoding=\"utf-8\") as mf:\n",
    "        mf.write(\"# id; table; filename; image_path\\n\")\n",
    "        for tbl in tables:\n",
    "            ok, fixed, missing = process_table(conn, tbl, idx, mf)\n",
    "            grand_ok += ok\n",
    "            grand_fixed += fixed\n",
    "            grand_missing += missing\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\n=== Zusammenfassung ===\")\n",
    "    print(f\"OK:       {grand_ok:,}\")\n",
    "    print(f\"Fixed:    {grand_fixed:,}\")\n",
    "    print(f\"Missing:  {grand_missing:,}\")\n",
    "    print(f\"Fehlende Bildpfade in: {MISSING_TXT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b0a92",
   "metadata": {},
   "source": [
    "# UMAP FÜR 32D UND BESTEHENDE 512D EMBEDDINGSVEKTOREN BERECHNEN UND MODELLE SPEICHERN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ec3244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b85c2d5d54048b1b13d06a94fc97aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Write pca_32 image_features_part_1:   0%|          | 0/100000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca_32] image_features_part_1: neue Dateien geschrieben: 100000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07bff4441304f1dbdd26deb956f0a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Write pca_32 image_features_part_2:   0%|          | 0/51500 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca_32] image_features_part_2: neue Dateien geschrieben: 51500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db04426b37b4a2cad5fb153b58091d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Write pca_32 image_features_part_3:   0%|          | 0/50000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca_32] image_features_part_3: neue Dateien geschrieben: 50000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12013c665ad247a186994ce85e90a2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Write pca_32 image_features_part_4:   0%|          | 0/50000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca_32] image_features_part_4: neue Dateien geschrieben: 50000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161d67bf98d74a628ac70dffe937de2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Write pca_32 image_features_part_5:   0%|          | 0/50004 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca_32] image_features_part_5: neue Dateien geschrieben: 50004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5993bd44c6c340378f3fc45381a528f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Write pca_32 image_features_part_6:   0%|          | 0/50000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca_32] image_features_part_6: neue Dateien geschrieben: 50000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5766376581314997b8d51aa4bd854a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Write pca_32 image_features_part_7:   0%|          | 0/8250 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca_32] image_features_part_7: neue Dateien geschrieben: 8250\n",
      "[Base64] Params: {'metric': 'euclidean', 'n_neighbors': 15, 'min_dist': 0.1, 'init': 'spectral', 'low_memory': True, 'negative_sample_rate': 5, 'random_state': 42}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05abf1ee373145a2a3106e10a004bb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reservoir-Sampling 32:   0%|          | 0/359754 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5aaab8604e4647856df6e619f9541f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lade Vektoren 32D:   0%|          | 0/25000 [00:00<?, ?vec/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 32D-UMAP fit auf 25000 Vektoren …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\CODING\\UNI\\BIG_DATA\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204715816d0d49f591e70ad0b0e11512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reservoir-Sampling 512:   0%|          | 0/359754 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf33d6d428fc4f988a9716e2a3e650a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lade Vektoren 512D:   0%|          | 0/25000 [00:00<?, ?vec/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 512D-UMAP fit auf 25000 Vektoren …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\CODING\\UNI\\BIG_DATA\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Save] 32D -> C:\\BIG_DATA\\models\\umap32_reducer.joblib ; 512D -> C:\\BIG_DATA\\models\\umap512_reducer.joblib\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b527dd2012704bc2ba4dc966c474a5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_1 → umap32_x/umap32_y:   0%|          | 0/100000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_1] aktualisiert: 100000 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e194a6d0624a73a9914e9c1cb961d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_2 → umap32_x/umap32_y:   0%|          | 0/51500 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_2] aktualisiert: 51500 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b92a86602b84a62a47fd50a2227f216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_3 → umap32_x/umap32_y:   0%|          | 0/50000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_3] aktualisiert: 50000 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efda8b43e01a49f6af6bb17f5f6a7e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_4 → umap32_x/umap32_y:   0%|          | 0/50000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_4] aktualisiert: 50000 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c0bd2a31154770bab638f7d6e175e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_5 → umap32_x/umap32_y:   0%|          | 0/50004 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_5] aktualisiert: 50004 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e526b29775b4ef8976ee490f5360ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_6 → umap32_x/umap32_y:   0%|          | 0/50000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_6] aktualisiert: 50000 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151f6c9f307643c882a8fc6dab922c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_7 → umap32_x/umap32_y:   0%|          | 0/8250 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_7] aktualisiert: 8250 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29aa02e69f8480ca0e085b6e5a3f0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_1 → umap512_x/umap512_y:   0%|          | 0/100000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_1] aktualisiert: 100000 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23250494c3c34c63a48dead73e316bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_2 → umap512_x/umap512_y:   0%|          | 0/51500 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_2] aktualisiert: 51500 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7362d90fe8df4a948456cea0d538021e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_3 → umap512_x/umap512_y:   0%|          | 0/50000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_3] aktualisiert: 50000 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d511df79ac67411a8d88c9d01de80c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_4 → umap512_x/umap512_y:   0%|          | 0/50000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_4] aktualisiert: 50000 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af13593d489c43d0a848aa405dfe9472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_5 → umap512_x/umap512_y:   0%|          | 0/50004 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_5] aktualisiert: 50004 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ab97d61a774a3786a1602efca51f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_6 → umap512_x/umap512_y:   0%|          | 0/50000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_6] aktualisiert: 50000 Zeilen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7450b7d0106e41dda135a03b737a66ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Populate image_features_part_7 → umap512_x/umap512_y:   0%|          | 0/8250 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image_features_part_7] aktualisiert: 8250 Zeilen.\n",
      "Fertig.\n"
     ]
    }
   ],
   "source": [
    "# build_umap_32_512_from_64params_tqdm_pca32.py\n",
    "# - Liest 64D-UMAP-Params (euclidean etc.)\n",
    "# - Trainiert 32D/512D-UMAP und befüllt umap32_x/umap32_y, umap512_x/umap512_y\n",
    "# - NEU: erzeugt 32D .npy pro Datensatz und speichert Pfad in Spalte pca_32 (TEXT)\n",
    "\n",
    "import sqlite3, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import numpy as np\n",
    "from joblib import load, dump\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# =================== KONFIG (ANPASSEN) ===================\n",
    "DB_PATH = Path(r\"C:\\BIG_DATA\\data\\database.db\")\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "BATCH = 4096\n",
    "\n",
    "# 64D-UMAP (liest Params)\n",
    "UMAP64_PATH = Path(r\"C:\\BIG_DATA\\models\\umap_reducer.joblib\")\n",
    "PREFER_KEY_64 = 64\n",
    "\n",
    "# Speichermodus 32/512 UMAP-Modelle\n",
    "SAVE_MODE = \"separate\"  # \"separate\" | \"single\"\n",
    "UMAP32_PATH = Path(r\"C:\\BIG_DATA\\models\\umap32_reducer.joblib\")\n",
    "UMAP512_PATH = Path(r\"C:\\BIG_DATA\\models\\umap512_reducer.joblib\")\n",
    "UMAP_SINGLE_PATH = Path(r\"C:\\BIG_DATA\\models\\umap_reducer.joblib\")\n",
    "\n",
    "# Sampling\n",
    "SAMPLE_32 = 25_000\n",
    "SAMPLE_512 = 25_000\n",
    "\n",
    "# Preprocessing (bei metric=\"euclidean\" meist False, um 64D-Setup zu spiegeln)\n",
    "PREPROC_L2_32 = False\n",
    "PREPROC_L2_512 = False\n",
    "\n",
    "# Nur trainieren / nur befüllen\n",
    "ONLY_TRAIN = False\n",
    "ONLY_POPULATE = False\n",
    "\n",
    "# tqdm / Progressbar\n",
    "TQDM_ENABLE = True\n",
    "TQDM_MININTERVAL = 0.2\n",
    "COUNT_FOR_SAMPLING_TOTAL = True\n",
    "COUNT_FOR_POPULATE_TOTAL = True\n",
    "\n",
    "# ===== NEU: 32D .npy schreiben und Pfad in DB (Spalte pca_32) =====\n",
    "WRITE_PCA32_FILES = True\n",
    "PCA32_DIR = Path(r\"C:\\BIG_DATA\\embeddings_pca32\")  # Zielordner für 32D .npy\n",
    "PCA32_DTYPE = np.float32\n",
    "# =================== ENDE KONFIG ===================\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Bitte 'umap-learn' installieren: pip install umap-learn\") from e\n",
    "\n",
    "\n",
    "# ========= Utils =========\n",
    "def l2n(v: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    v = v.astype(np.float32, copy=False)\n",
    "    n = float(np.linalg.norm(v))\n",
    "    if n < eps:\n",
    "        return v\n",
    "    return (v / n).astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def safe_load_vec(path: str) -> np.ndarray:\n",
    "    v = np.load(path, mmap_mode=\"r\")\n",
    "    if v.ndim == 2 and v.shape[0] == 1:\n",
    "        v = v[0]\n",
    "    return v.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def list_tables(conn: sqlite3.Connection) -> List[str]:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    return [r[0] for r in cur.fetchall()]\n",
    "\n",
    "\n",
    "def ensure_cols(\n",
    "    conn: sqlite3.Connection, table: str, cols: List[Tuple[str, str]]\n",
    ") -> None:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f'PRAGMA table_info(\"{table}\")')\n",
    "    have = {r[1] for r in cur.fetchall()}\n",
    "    for name, typ in cols:\n",
    "        if name not in have:\n",
    "            cur.execute(f'ALTER TABLE \"{table}\" ADD COLUMN {name} {typ}')\n",
    "\n",
    "\n",
    "def iter_paths(conn: sqlite3.Connection, table: str, where_clause: str = \"\"):\n",
    "    cur = conn.cursor()\n",
    "    sql = f'SELECT id, pca_embedding, embedding_path FROM \"{table}\" {where_clause}'.strip()\n",
    "    cur.execute(sql)\n",
    "    while True:\n",
    "        rows = cur.fetchmany(BATCH)\n",
    "        if not rows:\n",
    "            break\n",
    "        for rid, pca_path, emb_path in rows:\n",
    "            yield int(rid), str(pca_path), str(emb_path)\n",
    "\n",
    "\n",
    "def count_rows(\n",
    "    conn: sqlite3.Connection, table: str, where_clause: Optional[str] = None\n",
    ") -> int:\n",
    "    cur = conn.cursor()\n",
    "    sql = f'SELECT COUNT(*) FROM \"{table}\" {where_clause or \"\"}'.strip()\n",
    "    cur.execute(sql)\n",
    "    v = cur.fetchone()[0]\n",
    "    return int(v or 0)\n",
    "\n",
    "\n",
    "def reservoir_sample_paths(\n",
    "    conn: sqlite3.Connection, tables: List[str], k: int, which: str\n",
    ") -> List[str]:\n",
    "    if k <= 0:\n",
    "        return []\n",
    "    reservoir: List[str] = []\n",
    "    seen = 0\n",
    "    total = None\n",
    "    if TQDM_ENABLE and COUNT_FOR_SAMPLING_TOTAL:\n",
    "        total = sum(count_rows(conn, t) for t in tables)\n",
    "    pbar = tqdm(\n",
    "        total=total,\n",
    "        disable=not TQDM_ENABLE,\n",
    "        mininterval=TQDM_MININTERVAL,\n",
    "        desc=f\"Reservoir-Sampling {which}\",\n",
    "        unit=\"row\",\n",
    "    )\n",
    "    try:\n",
    "        for t in tables:\n",
    "            for _, pca_path, emb_path in iter_paths(conn, t):\n",
    "                p = pca_path if which == \"32\" else emb_path\n",
    "                seen += 1\n",
    "                if len(reservoir) < k:\n",
    "                    reservoir.append(p)\n",
    "                else:\n",
    "                    j = random.randint(0, seen - 1)\n",
    "                    if j < k:\n",
    "                        reservoir[j] = p\n",
    "                if TQDM_ENABLE:\n",
    "                    pbar.update(1)\n",
    "    finally:\n",
    "        pbar.close()\n",
    "    return reservoir\n",
    "\n",
    "\n",
    "def load_vectors_from_paths(paths: List[str], dim: int, l2: bool) -> np.ndarray:\n",
    "    vecs = []\n",
    "    pbar = tqdm(\n",
    "        total=len(paths),\n",
    "        disable=not TQDM_ENABLE,\n",
    "        mininterval=TQDM_MININTERVAL,\n",
    "        desc=f\"Lade Vektoren {dim}D\",\n",
    "        unit=\"vec\",\n",
    "    )\n",
    "    try:\n",
    "        for p in paths:\n",
    "            try:\n",
    "                v = safe_load_vec(p)\n",
    "                if dim == 32:\n",
    "                    if v.shape[0] < 32:\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    v = v[:32]\n",
    "                elif dim == 512:\n",
    "                    if v.shape[0] < 128:\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                v = l2n(v) if l2 else v\n",
    "                vecs.append(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "    finally:\n",
    "        pbar.close()\n",
    "    if not vecs:\n",
    "        return np.empty((0, dim), dtype=np.float32)\n",
    "    return np.vstack(vecs).astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# -------- 64D-Params laden --------\n",
    "SAFE_KEYS = [\n",
    "    \"n_neighbors\",\n",
    "    \"min_dist\",\n",
    "    \"metric\",\n",
    "    \"metric_kwds\",\n",
    "    \"n_epochs\",\n",
    "    \"learning_rate\",\n",
    "    \"init\",\n",
    "    \"spread\",\n",
    "    \"low_memory\",\n",
    "    \"set_op_mix_ratio\",\n",
    "    \"local_connectivity\",\n",
    "    \"repulsion_strength\",\n",
    "    \"negative_sample_rate\",\n",
    "    \"transform_queue_size\",\n",
    "    \"random_state\",\n",
    "    \"angular_rp_forest\",\n",
    "    \"target_n_neighbors\",\n",
    "    \"target_metric\",\n",
    "    \"target_metric_kwds\",\n",
    "    \"target_weight\",\n",
    "    \"transform_seed\",\n",
    "    \"transform_mode\",\n",
    "    \"force_approximation_algorithm\",\n",
    "    \"verbose\",\n",
    "    \"unique\",\n",
    "    \"densmap\",\n",
    "    \"dens_lambda\",\n",
    "    \"dens_frac\",\n",
    "    \"dens_var_shift\",\n",
    "    \"disconnection_distance\",\n",
    "    \"output_metric\",\n",
    "    \"output_metric_kwds\",\n",
    "]\n",
    "FALLBACK_FROM_YOUR_64 = {\n",
    "    \"n_neighbors\": 15,\n",
    "    \"min_dist\": 0.1,\n",
    "    \"metric\": \"euclidean\",\n",
    "    \"learning_rate\": 1.0,\n",
    "    \"local_connectivity\": 1.0,\n",
    "    \"low_memory\": True,\n",
    "    \"init\": \"spectral\",\n",
    "    \"n_epochs\": None,\n",
    "    \"negative_sample_rate\": 5,\n",
    "    \"random_state\": 42,\n",
    "    \"repulsion_strength\": 1.0,\n",
    "    \"set_op_mix_ratio\": 1.0,\n",
    "    \"spread\": 1.0,\n",
    "    \"transform_queue_size\": 4.0,\n",
    "    \"transform_mode\": \"embedding\",\n",
    "    \"transform_seed\": 42,\n",
    "    \"unique\": False,\n",
    "    \"verbose\": False,\n",
    "    \"densmap\": False,\n",
    "    \"dens_lambda\": 2.0,\n",
    "    \"dens_frac\": 0.3,\n",
    "    \"dens_var_shift\": 0.1,\n",
    "    \"output_metric\": \"euclidean\",\n",
    "    \"metric_kwds\": None,\n",
    "    \"output_metric_kwds\": None,\n",
    "    \"target_metric\": \"categorical\",\n",
    "    \"target_metric_kwds\": None,\n",
    "    \"target_n_neighbors\": -1,\n",
    "    \"target_weight\": 0.5,\n",
    "    \"angular_rp_forest\": False,\n",
    "    \"force_approximation_algorithm\": False,\n",
    "    \"disconnection_distance\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def load_base_params_from_64(path: Path) -> Dict[str, Any]:\n",
    "    try:\n",
    "        obj = load(path)\n",
    "    except Exception:\n",
    "        return FALLBACK_FROM_YOUR_64.copy()\n",
    "    if isinstance(obj, dict):\n",
    "        reducer = (\n",
    "            obj.get(PREFER_KEY_64)\n",
    "            or obj.get(str(PREFER_KEY_64))\n",
    "            or next(iter(obj.values()))\n",
    "        )\n",
    "    else:\n",
    "        reducer = obj\n",
    "    params = reducer.get_params(deep=False)\n",
    "    base = {k: params.get(k) for k in SAFE_KEYS if k in params}\n",
    "    for k, v in FALLBACK_FROM_YOUR_64.items():\n",
    "        base.setdefault(k, v)\n",
    "    return base\n",
    "\n",
    "\n",
    "def train_umap_with(base_params: Dict[str, Any], X: np.ndarray, n_components: int = 2):\n",
    "    cfg = base_params.copy()\n",
    "    cfg[\"n_components\"] = n_components\n",
    "    reducer = umap.UMAP(**cfg)\n",
    "    reducer.fit(X)\n",
    "    return reducer\n",
    "\n",
    "\n",
    "def save_models(models: Dict[int, Any]):\n",
    "    if SAVE_MODE == \"separate\":\n",
    "        if 32 in models:\n",
    "            UMAP32_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "            dump(models[32], UMAP32_PATH)\n",
    "        if 512 in models:\n",
    "            UMAP512_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "            dump(models[512], UMAP512_PATH)\n",
    "    else:\n",
    "        if UMAP_SINGLE_PATH.exists():\n",
    "            obj = load(UMAP_SINGLE_PATH)\n",
    "            d = (\n",
    "                obj\n",
    "                if isinstance(obj, dict)\n",
    "                else {getattr(obj, \"n_features_in_\", 64): obj}\n",
    "            )\n",
    "            d.update(models)\n",
    "            dump(d, UMAP_SINGLE_PATH)\n",
    "        else:\n",
    "            dump(models, UMAP_SINGLE_PATH)\n",
    "\n",
    "\n",
    "def load_models_for_populate() -> Dict[int, Any]:\n",
    "    models: Dict[int, Any] = {}\n",
    "    if SAVE_MODE == \"separate\":\n",
    "        if UMAP32_PATH.exists():\n",
    "            models[32] = load(UMAP32_PATH)\n",
    "        if UMAP512_PATH.exists():\n",
    "            models[512] = load(UMAP512_PATH)\n",
    "    else:\n",
    "        if UMAP_SINGLE_PATH.exists():\n",
    "            obj = load(UMAP_SINGLE_PATH)\n",
    "            models = (\n",
    "                obj\n",
    "                if isinstance(obj, dict)\n",
    "                else {getattr(obj, \"n_features_in_\", 2): obj}\n",
    "            )\n",
    "    return models\n",
    "\n",
    "\n",
    "def _fetch_rows_missing(conn: sqlite3.Connection, table: str, where_clause: str):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        f'SELECT id, pca_embedding, embedding_path FROM \"{table}\" {where_clause}'\n",
    "    )\n",
    "    while True:\n",
    "        rows = cur.fetchmany(BATCH)\n",
    "        if not rows:\n",
    "            break\n",
    "        yield rows\n",
    "\n",
    "\n",
    "# ===== NEU: 32D .npy erzeugen & Pfad in pca_32 speichern =====\n",
    "def _pca32_target_path(table: str, rid: int) -> Path:\n",
    "    return PCA32_DIR / table / f\"{rid}_pca32.npy\"\n",
    "\n",
    "\n",
    "def populate_pca32_column(conn: sqlite3.Connection, tables: List[str]) -> None:\n",
    "    if not WRITE_PCA32_FILES:\n",
    "        print(\"[pca_32] Übersprungen (WRITE_PCA32_FILES=False).\")\n",
    "        return\n",
    "\n",
    "    PCA32_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for t in tables:\n",
    "        # Spalte pca_32 (TEXT) anlegen, falls fehlt\n",
    "        ensure_cols(conn, t, [(\"pca_32\", \"TEXT\")])\n",
    "\n",
    "        # Nur fehlende Einträge\n",
    "        where = \"WHERE pca_32 IS NULL\"\n",
    "        total_missing = (\n",
    "            count_rows(conn, t, where_clause=where)\n",
    "            if (TQDM_ENABLE and COUNT_FOR_POPULATE_TOTAL)\n",
    "            else None\n",
    "        )\n",
    "        pbar = tqdm(\n",
    "            total=total_missing,\n",
    "            disable=not TQDM_ENABLE,\n",
    "            mininterval=TQDM_MININTERVAL,\n",
    "            desc=f\"Write pca_32 {t}\",\n",
    "            unit=\"row\",\n",
    "        )\n",
    "\n",
    "        updates = []\n",
    "        written = 0\n",
    "        try:\n",
    "            for batch in _fetch_rows_missing(conn, t, where):\n",
    "                for rid, pca_path, _ in batch:\n",
    "                    try:\n",
    "                        v64 = safe_load_vec(pca_path)\n",
    "                        if v64.shape[0] < 32:\n",
    "                            if TQDM_ENABLE:\n",
    "                                pbar.update(1)\n",
    "                            continue\n",
    "                        v32 = v64[:32].astype(PCA32_DTYPE, copy=False)\n",
    "                        out_path = _pca32_target_path(t, int(rid))\n",
    "                        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        if not out_path.exists():\n",
    "                            np.save(out_path, v32, allow_pickle=False)\n",
    "                            written += 1\n",
    "                        updates.append((str(out_path), int(rid)))\n",
    "                    except Exception:\n",
    "                        # defekte Quelle -> skip\n",
    "                        pass\n",
    "                    finally:\n",
    "                        if TQDM_ENABLE:\n",
    "                            pbar.update(1)\n",
    "                if updates:\n",
    "                    conn.executemany(f'UPDATE \"{t}\" SET pca_32=? WHERE id=?', updates)\n",
    "                    conn.commit()\n",
    "                    updates = []\n",
    "        finally:\n",
    "            pbar.close()\n",
    "        print(f\"[pca_32] {t}: neue Dateien geschrieben: {written}\")\n",
    "\n",
    "\n",
    "# ===== UMAP-Koordinaten befüllen =====\n",
    "def populate_umap_coords(\n",
    "    conn: sqlite3.Connection, tables: List[str], dim: int, reducer: Any, l2: bool\n",
    ") -> None:\n",
    "    if dim == 32:\n",
    "        colx, coly = \"umap32_x\", \"umap32_y\"\n",
    "        loader = lambda pca_path, emb_path: safe_load_vec(pca_path)[:32]\n",
    "    else:\n",
    "        colx, coly = \"umap512_x\", \"umap512_y\"\n",
    "        loader = lambda pca_path, emb_path: safe_load_vec(emb_path)\n",
    "\n",
    "    need_cols = [(colx, \"REAL\"), (coly, \"REAL\")]\n",
    "    where = f\"WHERE {colx} IS NULL OR {coly} IS NULL\"\n",
    "\n",
    "    conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "    conn.execute(\"PRAGMA synchronous=NORMAL\")\n",
    "    conn.execute(\"PRAGMA temp_store=MEMORY\")\n",
    "\n",
    "    for t in tables:\n",
    "        ensure_cols(conn, t, need_cols)\n",
    "        total_missing = (\n",
    "            count_rows(conn, t, where_clause=where)\n",
    "            if (TQDM_ENABLE and COUNT_FOR_POPULATE_TOTAL)\n",
    "            else None\n",
    "        )\n",
    "        pbar = tqdm(\n",
    "            total=total_missing,\n",
    "            disable=not TQDM_ENABLE,\n",
    "            mininterval=TQDM_MININTERVAL,\n",
    "            desc=f\"Populate {t} → {colx}/{coly}\",\n",
    "            unit=\"row\",\n",
    "        )\n",
    "\n",
    "        total_upd = 0\n",
    "        try:\n",
    "            for batch in _fetch_rows_missing(conn, t, where):\n",
    "                ids, mats = [], []\n",
    "                for rid, pca_path, emb_path in batch:\n",
    "                    try:\n",
    "                        v = loader(pca_path, emb_path)\n",
    "                        if v.ndim == 2 and v.shape[0] == 1:\n",
    "                            v = v[0]\n",
    "                        v = l2n(v) if l2 else v.astype(np.float32, copy=False)\n",
    "                        mats.append(v)\n",
    "                        ids.append(int(rid))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if not ids:\n",
    "                    continue\n",
    "                X = np.vstack(mats).astype(np.float32, copy=False)\n",
    "                XY = reducer.transform(X)\n",
    "                rows = [\n",
    "                    (float(x), float(y), rid) for (x, y), rid in zip(XY[:, :2], ids)\n",
    "                ]\n",
    "                conn.executemany(\n",
    "                    f'UPDATE \"{t}\" SET {colx}=?, {coly}=? WHERE id=?', rows\n",
    "                )\n",
    "                total_upd += len(rows)\n",
    "                if TQDM_ENABLE:\n",
    "                    pbar.update(len(rows))\n",
    "            conn.commit()\n",
    "        finally:\n",
    "            pbar.close()\n",
    "        print(f\"[{t}] aktualisiert: {total_upd} Zeilen.\")\n",
    "\n",
    "\n",
    "# =================== PIPELINE ===================\n",
    "def main():\n",
    "    conn = sqlite3.connect(str(DB_PATH))\n",
    "    tables = list_tables(conn)\n",
    "    if not tables:\n",
    "        raise RuntimeError(f\"Keine Tabellen mit Präfix '{TABLE_PREFIX}' gefunden.\")\n",
    "\n",
    "    # 1) pca_32-Spalte befüllen (Dateien + Pfade)\n",
    "    populate_pca32_column(conn, tables)\n",
    "\n",
    "    # 2) 64D-Params laden\n",
    "    base = load_base_params_from_64(UMAP64_PATH)\n",
    "    print(\n",
    "        \"[Base64] Params:\",\n",
    "        {\n",
    "            k: base[k]\n",
    "            for k in (\n",
    "                \"metric\",\n",
    "                \"n_neighbors\",\n",
    "                \"min_dist\",\n",
    "                \"init\",\n",
    "                \"low_memory\",\n",
    "                \"negative_sample_rate\",\n",
    "                \"random_state\",\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    models: Dict[int, Any] = {}\n",
    "\n",
    "    # 3) TRAINING (nur wenn nicht ONLY_POPULATE)\n",
    "    if not ONLY_POPULATE:\n",
    "        # 32D\n",
    "        paths32 = reservoir_sample_paths(conn, tables, SAMPLE_32, which=\"32\")\n",
    "        X32 = load_vectors_from_paths(paths32, dim=32, l2=PREPROC_L2_32)\n",
    "        if X32.shape[0] > 0:\n",
    "            print(f\"[Train] 32D-UMAP fit auf {X32.shape[0]} Vektoren …\")\n",
    "            models[32] = train_umap_with(base, X32, n_components=2)\n",
    "        else:\n",
    "            print(\"[Warn] Kein 32D-Trainingssample gefunden.\")\n",
    "\n",
    "        # 512D\n",
    "        paths512 = reservoir_sample_paths(conn, tables, SAMPLE_512, which=\"512\")\n",
    "        X512 = load_vectors_from_paths(paths512, dim=512, l2=PREPROC_L2_512)\n",
    "        if X512.shape[0] > 0:\n",
    "            print(f\"[Train] 512D-UMAP fit auf {X512.shape[0]} Vektoren …\")\n",
    "            models[512] = train_umap_with(base, X512, n_components=2)\n",
    "        else:\n",
    "            print(\"[Warn] Kein 512D-Trainingssample gefunden.\")\n",
    "\n",
    "        if models:\n",
    "            save_models(models)\n",
    "            if SAVE_MODE == \"separate\":\n",
    "                print(\n",
    "                    f\"[Save] 32D -> {UMAP32_PATH if 32 in models else 'skip'} ; 512D -> {UMAP512_PATH if 512 in models else 'skip'}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"[Save] Dict -> {UMAP_SINGLE_PATH}\")\n",
    "\n",
    "    # 4) POPULATE (UMAP-Koordinaten)\n",
    "    if not ONLY_TRAIN:\n",
    "        if not models:\n",
    "            models = load_models_for_populate()\n",
    "        if 32 in models:\n",
    "            populate_umap_coords(\n",
    "                conn, tables, dim=32, reducer=models[32], l2=PREPROC_L2_32\n",
    "            )\n",
    "        else:\n",
    "            print(\"[Skip] Kein 32D-Reducer verfügbar.\")\n",
    "        if 512 in models:\n",
    "            populate_umap_coords(\n",
    "                conn, tables, dim=512, reducer=models[512], l2=PREPROC_L2_512\n",
    "            )\n",
    "        else:\n",
    "            print(\"[Skip] Kein 512D-Reducer verfügbar.\")\n",
    "\n",
    "    conn.close()\n",
    "    print(\"Fertig.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb08a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== UMAP(64D) get_params() ===\n",
      "{\n",
      "  \"a\": null,\n",
      "  \"angular_rp_forest\": false,\n",
      "  \"b\": null,\n",
      "  \"dens_frac\": 0.3,\n",
      "  \"dens_lambda\": 2.0,\n",
      "  \"dens_var_shift\": 0.1,\n",
      "  \"densmap\": false,\n",
      "  \"disconnection_distance\": null,\n",
      "  \"force_approximation_algorithm\": false,\n",
      "  \"init\": \"spectral\",\n",
      "  \"learning_rate\": 1.0,\n",
      "  \"local_connectivity\": 1.0,\n",
      "  \"low_memory\": true,\n",
      "  \"metric\": \"euclidean\",\n",
      "  \"metric_kwds\": null,\n",
      "  \"min_dist\": 0.1,\n",
      "  \"n_components\": 2,\n",
      "  \"n_epochs\": null,\n",
      "  \"n_jobs\": 1,\n",
      "  \"n_neighbors\": 15,\n",
      "  \"negative_sample_rate\": 5,\n",
      "  \"output_dens\": false,\n",
      "  \"output_metric\": \"euclidean\",\n",
      "  \"output_metric_kwds\": null,\n",
      "  \"precomputed_knn\": [\n",
      "    null,\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"random_state\": 42,\n",
      "  \"repulsion_strength\": 1.0,\n",
      "  \"set_op_mix_ratio\": 1.0,\n",
      "  \"spread\": 1.0,\n",
      "  \"target_metric\": \"categorical\",\n",
      "  \"target_metric_kwds\": null,\n",
      "  \"target_n_neighbors\": -1,\n",
      "  \"target_weight\": 0.5,\n",
      "  \"tqdm_kwds\": {\n",
      "    \"desc\": \"Epochs completed\",\n",
      "    \"bar_format\": \"{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]\",\n",
      "    \"disable\": true\n",
      "  },\n",
      "  \"transform_mode\": \"embedding\",\n",
      "  \"transform_queue_size\": 4.0,\n",
      "  \"transform_seed\": 42,\n",
      "  \"unique\": false,\n",
      "  \"verbose\": false\n",
      "}\n",
      "\n",
      "=== Extra ===\n",
      "{\n",
      "  \"n_features_in_\": null,\n",
      "  \"n_components_\": 2,\n",
      "  \"has_transform\": true,\n",
      "  \"learned_ab\": {\n",
      "    \"a_\": null,\n",
      "    \"b_\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# print_umap64_params.py\n",
    "from joblib import load\n",
    "import json\n",
    "\n",
    "UMAP_PATH = r\"C:\\BIG_DATA\\models\\umap_reducer.joblib\"  # ggf. anpassen\n",
    "\n",
    "\n",
    "def pick_64(obj):\n",
    "    # joblib kann entweder ein einzelnes UMAP-Objekt oder ein Dict {32:...,64:...,512:...} enthalten\n",
    "    if isinstance(obj, dict):\n",
    "        # bevorzugt Key 64 (int oder str)\n",
    "        if 64 in obj:\n",
    "            return obj[64]\n",
    "        if \"64\" in obj:\n",
    "            return obj[\"64\"]\n",
    "        # sonst: bestes Matching nehmen\n",
    "        for k in obj:\n",
    "            try:\n",
    "                if int(k) == 64:\n",
    "                    return obj[k]\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Fallback: irgendein Eintrag\n",
    "        return next(iter(obj.values()))\n",
    "    return obj\n",
    "\n",
    "\n",
    "def main():\n",
    "    reducer = pick_64(load(UMAP_PATH))\n",
    "    # Konstruktor-Parameter\n",
    "    params = reducer.get_params(deep=False)\n",
    "\n",
    "    # Nützliche Zusatzinfos\n",
    "    extra = {\n",
    "        \"n_features_in_\": getattr(reducer, \"n_features_in_\", None),\n",
    "        \"n_components_\": (\n",
    "            getattr(reducer, \"embedding_\", None).shape[1]\n",
    "            if getattr(reducer, \"embedding_\", None) is not None\n",
    "            else None\n",
    "        ),\n",
    "        \"has_transform\": hasattr(reducer, \"transform\"),\n",
    "        \"learned_ab\": {\n",
    "            \"a_\": getattr(reducer, \"a_\", None),\n",
    "            \"b_\": getattr(reducer, \"b_\", None),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== UMAP(64D) get_params() ===\")\n",
    "    print(json.dumps(params, indent=2, default=str))\n",
    "    print(\"\\n=== Extra ===\")\n",
    "    print(json.dumps(extra, indent=2, default=str))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada47e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos512: 0.2743372619152069\n",
      "cos64 : -0.13994933664798737\n",
      "cos32 : -0.15374481678009033\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, sqlite3\n",
    "import cv2\n",
    "from joblib import load\n",
    "\n",
    "IMG = r\"D:\\data\\image_data\\pixabay_dataset_v1\\images_01\\adult-background-business-computer-15700.jpg\"\n",
    "DB = r\"C:\\BIG_DATA\\data\\database.db\"\n",
    "MODELS_DIR = Path(r\"C:\\BIG_DATA\\models\")\n",
    "IPCA_PATH = MODELS_DIR / \"ipca.joblib\"\n",
    "\n",
    "\n",
    "class PCATransformer:\n",
    "    def __init__(self, ipca_path: Path):\n",
    "        self.ipca = load(ipca_path)  # IncrementalPCA\n",
    "\n",
    "    def transform64(self, vec512_raw: np.ndarray) -> np.ndarray:\n",
    "        # Erwartet den ROHEN 512D-Vektor (nicht normiert), um exakt wie im DB-Build zu sein\n",
    "        return self.ipca.transform(vec512_raw[None, :]).astype(np.float32)[0]\n",
    "\n",
    "\n",
    "def _read_rgb(p: Path) -> np.ndarray:\n",
    "    data = np.fromfile(str(p), dtype=np.uint8)\n",
    "    bgr = cv2.imdecode(data, cv2.IMREAD_COLOR)  # BGR\n",
    "    if bgr is None:\n",
    "        raise ValueError(f\"cv2.imdecode konnte Bild nicht lesen: {p}\")\n",
    "    return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)  # -> RGB\n",
    "\n",
    "\n",
    "# 1) Query-Embeddings (512→64→32) so wie im Script\n",
    "q512_raw = extract_embeddings([_read_rgb(IMG)])[0].astype(np.float32).ravel()\n",
    "q64_raw = PCATransformer(IPCA_PATH).transform64(q512_raw)\n",
    "q32_raw = q64_raw[:32]\n",
    "\n",
    "\n",
    "def l2n(x):\n",
    "    x = x.astype(np.float32).ravel()\n",
    "    n = np.linalg.norm(x) + 1e-12\n",
    "    return x / n\n",
    "\n",
    "\n",
    "# 2) Die passenden DB-Dateien holen\n",
    "conn = sqlite3.connect(DB)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\n",
    "    \"SELECT embedding_path, pca_embedding, pca_32 FROM image_features_part_1 WHERE path=? LIMIT 1\",\n",
    "    (IMG,),\n",
    ")\n",
    "row = cur.fetchone()\n",
    "conn.close()\n",
    "emb512_path, emb64_path, emb32_path = row\n",
    "\n",
    "db512 = np.load(emb512_path).astype(np.float32).ravel()\n",
    "db64 = np.load(emb64_path).astype(np.float32).ravel()[:64]\n",
    "db32 = np.load(emb32_path).astype(np.float32).ravel()[:32]\n",
    "\n",
    "print(\"cos512:\", float(l2n(q512_raw) @ l2n(db512)))\n",
    "print(\"cos64 :\", float(l2n(q64_raw) @ l2n(db64)))\n",
    "print(\"cos32 :\", float(l2n(q32_raw) @ l2n(db32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e4e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[best] 0.2993033528327942 {'weights': 'DEFAULT', 'channel': 'rgb', 'resize': 'pil', 'norm': 'imagenet'}\n"
     ]
    }
   ],
   "source": [
    "# calibrate_embed_match.py\n",
    "import os, numpy as np, cv2, torch\n",
    "from pathlib import Path\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# --- Eingaben: pass hier 1) Pfad zum DB-Bild  2) Pfad zum dazugehörigen DB-Embedding (512D .npy)\n",
    "IMG = r\"D:\\data\\image_data\\pixabay_dataset_v1\\images_01\\adult-background-business-computer-15700.jpg\"\n",
    "VEC_DB = r\"C:\\BIG_DATA\\embeddings\\adult-background-business-computer-15700.jpg.npy\"\n",
    "\n",
    "# --- Lade DB-Vector\n",
    "v_db = np.load(VEC_DB).astype(np.float32).ravel()\n",
    "v_db /= np.linalg.norm(v_db) + 1e-12\n",
    "\n",
    "# --- Kandidaten: Weights / Kanal / ResizeLib / Norm\n",
    "weight_opts = []\n",
    "for name in [\"DEFAULT\", \"IMAGENET1K_V1\", \"IMAGENET1K_V2\"]:\n",
    "    if hasattr(ResNet18_Weights, name):\n",
    "        weight_opts.append(name)\n",
    "if not weight_opts:\n",
    "    weight_opts = [\"DEFAULT\"]\n",
    "\n",
    "channel_modes = [\"rgb\", \"bgr\"]\n",
    "resize_libs = [\"cv2\", \"pil\"]\n",
    "norm_modes = [\"imagenet\", \"none\"]  # einige Pipelines speichern evtl. ohne Mean/Std\n",
    "\n",
    "IM_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IM_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "\n",
    "def read_img(path, mode):\n",
    "    data = np.fromfile(path, dtype=np.uint8)\n",
    "    bgr = cv2.imdecode(data, cv2.IMREAD_COLOR)\n",
    "    if bgr is None:\n",
    "        raise ValueError(\"cv2.imdecode failed\")\n",
    "    if mode == \"rgb\":\n",
    "        return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    return bgr  # \"bgr\"\n",
    "\n",
    "\n",
    "def resize_224(img, lib):\n",
    "    if lib == \"cv2\":\n",
    "        return cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    else:\n",
    "        from PIL import Image\n",
    "\n",
    "        im = Image.fromarray(img if img.dtype == np.uint8 else img.astype(np.uint8))\n",
    "        return np.asarray(im.resize((224, 224), Image.BILINEAR), dtype=np.uint8)\n",
    "\n",
    "\n",
    "def to_chw_float01(arr):\n",
    "    x = arr.astype(np.float32)\n",
    "    if x.max() > 1.5:\n",
    "        x = x / 255.0\n",
    "    x = np.transpose(x, (2, 0, 1))\n",
    "    return x\n",
    "\n",
    "\n",
    "def norm_imageNet(chw):\n",
    "    return (chw - IM_MEAN[:, None, None]) / IM_STD[:, None, None]\n",
    "\n",
    "\n",
    "def get_model(weights_name):\n",
    "    weights = getattr(ResNet18_Weights, weights_name)\n",
    "    net = resnet18(weights=weights)\n",
    "    net = torch.nn.Sequential(*list(net.children())[:-1]).eval()\n",
    "    return net\n",
    "\n",
    "\n",
    "def to_vec(net, chw):\n",
    "    with torch.no_grad():\n",
    "        t = torch.from_numpy(chw[None, ...])\n",
    "        v = net(t).squeeze(-1).squeeze(-1).cpu().numpy().astype(np.float32).ravel()\n",
    "    v /= np.linalg.norm(v) + 1e-12\n",
    "    return v\n",
    "\n",
    "\n",
    "best = (-1.0, None)\n",
    "img_cache = {}\n",
    "for ch in channel_modes:\n",
    "    img_cache[ch] = read_img(IMG, ch)\n",
    "\n",
    "for w in weight_opts:\n",
    "    net = get_model(w)\n",
    "    for ch in channel_modes:\n",
    "        for rl in resize_libs:\n",
    "            for nm in norm_modes:\n",
    "                img = resize_224(img_cache[ch], rl)\n",
    "                x = to_chw_float01(img)\n",
    "                if nm == \"imagenet\":\n",
    "                    x = norm_imageNet(x)\n",
    "                vq = to_vec(net, x)\n",
    "                cos = float(vq @ v_db)\n",
    "                if cos > best[0]:\n",
    "                    best = (\n",
    "                        cos,\n",
    "                        {\"weights\": w, \"channel\": ch, \"resize\": rl, \"norm\": nm},\n",
    "                    )\n",
    "\n",
    "print(\"[best]\", best[0], best[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20900147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelle: image_features_part_1\n",
      "  konvertiere 100000 Einträge …\n",
      "Tabelle: image_features_part_2\n",
      "  konvertiere 51500 Einträge …\n",
      "Tabelle: image_features_part_3\n",
      "  konvertiere 50000 Einträge …\n",
      "Tabelle: image_features_part_4\n",
      "  konvertiere 50000 Einträge …\n",
      "Tabelle: image_features_part_5\n",
      "  konvertiere 50004 Einträge …\n",
      "Tabelle: image_features_part_6\n",
      "  konvertiere 50000 Einträge …\n",
      "Tabelle: image_features_part_7\n",
      "  konvertiere 8250 Einträge …\n",
      "Fertig.\n"
     ]
    }
   ],
   "source": [
    "# run_once_migrate_color_hist_to_blob.py\n",
    "import sqlite3, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DB = Path(r\"C:\\BIG_DATA\\data\\database.db\")\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "TEXT_COL = \"color_hist\"  # dein Name\n",
    "BLOB_COL = \"color_hist_blob\"  # neu\n",
    "DIM = 96  # 3 * 32 bins\n",
    "\n",
    "with sqlite3.connect(str(DB)) as conn:\n",
    "    cur = conn.cursor()\n",
    "    # Tabellen finden\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    tables = [r[0] for r in cur.fetchall()]\n",
    "    for t in tables:\n",
    "        print(\"Tabelle:\", t)\n",
    "\n",
    "        # Spalte hinzufügen (falls nicht vorhanden)\n",
    "        cur.execute(f'PRAGMA table_info(\"{t}\")')\n",
    "        cols = {r[1] for r in cur.fetchall()}\n",
    "        if BLOB_COL not in cols:\n",
    "            cur.execute(f'ALTER TABLE \"{t}\" ADD COLUMN {BLOB_COL} BLOB')\n",
    "            conn.commit()\n",
    "\n",
    "        # Alle TEXT-Hists holen und einmalig konvertieren\n",
    "        cur.execute(f'SELECT id, \"{TEXT_COL}\" FROM \"{t}\" WHERE \"{BLOB_COL}\" IS NULL')\n",
    "        rows = cur.fetchall()\n",
    "        print(f\"  konvertiere {len(rows)} Einträge …\")\n",
    "        for rid, txt in rows:\n",
    "            if not txt:\n",
    "                cur.execute(f'UPDATE \"{t}\" SET \"{BLOB_COL}\"=? WHERE id=?', (None, rid))\n",
    "                continue\n",
    "            s = txt if isinstance(txt, str) else txt.decode(\"utf-8\", \"ignore\")\n",
    "            arr = np.fromstring(\n",
    "                s.strip().lstrip(\"[\").rstrip(\"]\"), sep=\",\", dtype=np.float32\n",
    "            )\n",
    "            if arr.size != DIM:\n",
    "                cur.execute(f'UPDATE \"{t}\" SET \"{BLOB_COL}\"=? WHERE id=?', (None, rid))\n",
    "                continue\n",
    "            ssum = float(arr.sum())\n",
    "            if ssum > 0:\n",
    "                arr /= ssum\n",
    "            cur.execute(\n",
    "                f'UPDATE \"{t}\" SET \"{BLOB_COL}\"=? WHERE id=?', (arr.tobytes(), rid)\n",
    "            )\n",
    "        conn.commit()\n",
    "print(\"Fertig.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4440a868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabelle: image_features_part_1\n",
      "  total=100,000 | blob NULL=0 | text leer/NULL=0\n",
      "\n",
      "Tabelle: image_features_part_2\n",
      "  total=51,500 | blob NULL=0 | text leer/NULL=0\n",
      "\n",
      "Tabelle: image_features_part_3\n",
      "  total=50,000 | blob NULL=0 | text leer/NULL=0\n",
      "\n",
      "Tabelle: image_features_part_4\n",
      "  total=50,000 | blob NULL=0 | text leer/NULL=0\n",
      "\n",
      "Tabelle: image_features_part_5\n",
      "  total=50,004 | blob NULL=0 | text leer/NULL=0\n",
      "\n",
      "Tabelle: image_features_part_6\n",
      "  total=50,000 | blob NULL=0 | text leer/NULL=0\n",
      "\n",
      "Tabelle: image_features_part_7\n",
      "  total=8,250 | blob NULL=0 | text leer/NULL=0\n"
     ]
    }
   ],
   "source": [
    "import sqlite3, re\n",
    "from pathlib import Path\n",
    "\n",
    "DB = Path(r\"C:\\BIG_DATA\\data\\database.db\")\n",
    "TABLE_PREFIX = \"image_features_part_\"\n",
    "TEXT_COL = \"color_hist\"\n",
    "BLOB_COL = \"color_hist_blob\"\n",
    "\n",
    "with sqlite3.connect(str(DB)) as conn:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ? ORDER BY name ASC\",\n",
    "        (f\"{TABLE_PREFIX}%\",),\n",
    "    )\n",
    "    tables = [r[0] for r in cur.fetchall()]\n",
    "\n",
    "    for t in tables:\n",
    "        print(f\"\\nTabelle: {t}\")\n",
    "        cur.execute(f'SELECT COUNT(*) FROM \"{t}\"')\n",
    "        total = cur.fetchone()[0]\n",
    "        cur.execute(f'SELECT COUNT(*) FROM \"{t}\" WHERE \"{BLOB_COL}\" IS NULL')\n",
    "        null_blob = cur.fetchone()[0]\n",
    "        cur.execute(\n",
    "            f'SELECT COUNT(*) FROM \"{t}\" WHERE \"{TEXT_COL}\" IS NULL OR TRIM(\"{TEXT_COL}\")=\"\"'\n",
    "        )\n",
    "        empty_txt = cur.fetchone()[0]\n",
    "        print(\n",
    "            f\"  total={total:,} | blob NULL={null_blob:,} | text leer/NULL={empty_txt:,}\"\n",
    "        )\n",
    "\n",
    "        # Beispiele: TEXT vorhanden aber blob NULL -> Parsing/Dimension/Summe-Problem\n",
    "        cur.execute(\n",
    "            f\"\"\"\n",
    "            SELECT id, substr(\"{TEXT_COL}\",1,120)\n",
    "            FROM \"{t}\"\n",
    "            WHERE \"{BLOB_COL}\" IS NULL AND \"{TEXT_COL}\" IS NOT NULL AND TRIM(\"{TEXT_COL}\")<>\"\"\n",
    "            LIMIT 5\n",
    "        \"\"\"\n",
    "        )\n",
    "        rows = cur.fetchall()\n",
    "        if rows:\n",
    "            print(\"  Beispiele problematische TEXT-Hists:\")\n",
    "            for rid, snippet in rows:\n",
    "                print(f\"    id={rid}  text='{snippet}...'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
